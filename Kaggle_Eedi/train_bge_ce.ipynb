{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1576b8cb-0b6f-44ad-a734-a098d8dadb27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 12:26:56.008126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 12:26:56.008237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 12:26:56.009451: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 12:26:56.016333: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 12:26:56.812940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Seed set to 5656\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from src import CustomEncoder,HardNegativesCrossEntropy, SoftNegativesCrossEntropy, NoDublicateSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import rankdata\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup, AutoModel, AutoTokenizer, AutoModelForMultipleChoice, AutoConfig\n",
    "import wandb\n",
    "pl.seed_everything(5656)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d33294d-e603-4765-96dc-8d15a5e9c50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    class data:\n",
    "        train_path = 'candidates_bge_large.parquet'\n",
    "        misconcepts = 'eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv'\n",
    "        tokenizer = 'BAAI/bge-large-en-v1.5'\n",
    "        num_workers = 8\n",
    "        nfolds = 5\n",
    "        batch_size = 4\n",
    "        use_prefix = False\n",
    "        max_length = 256 \n",
    "        num_negs = 10\n",
    "        seed = 56\n",
    "    class model:\n",
    "        model = 'BAAI/bge-large-en-v1.5'\n",
    "        optim = torch.optim.AdamW\n",
    "        grad_acum_steps = 2\n",
    "        torch_dtype = None\n",
    "        scheduler= 'cosine'\n",
    "        warnap_steps = 0.0 #0.25\n",
    "        label_smoothing = 0.0\n",
    "        lr = 1e-5\n",
    "        pool = 'mean'\n",
    "        max_epoches = 5\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        turn_off_drop = True\n",
    "        num_cycles = 0.5\n",
    "        eps = 1e-7\n",
    "        weight_decay = 0.0\n",
    "        weight_decay_fn = 0.0\n",
    "        betas = (0.9, 0.999)\n",
    "        use_lora = False\n",
    "    seed = 56\n",
    "    fold_number = 0\n",
    "\n",
    "def set_wandb_cfg():\n",
    "    config = {}\n",
    "    for k,v in CFG.model.__dict__.items():\n",
    "        if '__' not in k:\n",
    "            config[k] = v\n",
    "    for k,v in CFG.data.__dict__.items():\n",
    "        if '__' not in k:\n",
    "            config[k] = v\n",
    "    config['fold_number'] = CFG.fold_number\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295514ee-4a94-4a74-ab6d-8990c3f9df62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_anchor(x):\n",
    "    x = x.fillna('')\n",
    "    return f\"{x.answer_value}  {x.ConstructName} {x.QuestionText} {x.answer_value}\"\n",
    "\n",
    "def parse_negs(x,c=CFG.data.num_negs):\n",
    "    return [i for i in x['candidates'] if i != x['label']][:c]\n",
    "\n",
    "def make_df(train_path):\n",
    "    data = pd.read_parquet(train_path)\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    df['anchor'] = data.apply(parse_anchor,axis=1)\n",
    "    df['candidates'] = data['top25_candidates']\n",
    "    df['candidates_scores'] = data['top25_scores']\n",
    "    df['candidates_ranks'] = (-data['top25_scores']).apply(rankdata).map(lambda x:[int(i) for i in x])\n",
    "    df['misc_in_condidates'] = data['id_in_candidates']\n",
    "    df['label'] = data['misconcpts_id'].astype(int)\n",
    "    df['negs'] = df.apply(parse_negs,axis=1)\n",
    "    df['question_id'] = data['QuestionId']\n",
    "    df['answer_id'] = data['answer_id'].apply(lambda x: {'AnswerAText':0,'AnswerBText':1,'AnswerCText':2,'AnswerDText':3}[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e503de-01b8-492f-831b-d9d5e0fd1aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLDataset(Dataset):\n",
    "    def __init__(self, df, miscocepts_maper, tokenizer):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG.data\n",
    "        self.data = df\n",
    "        self.miscocepts_maper = miscocepts_maper\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def simple_negative_sample(self,row):\n",
    "        if row['misc_in_condidates']:\n",
    "            return [x for x in row['candidates'] if x != row['label']][:self.cfg.num_negs]\n",
    "        else:\n",
    "            return row['candidates'][:self.cfg.num_negs].tolist()\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        neatives = [row['label']] + self.simple_negative_sample(row)\n",
    "        anchor = row['anchor']\n",
    "        \n",
    "        encodes = [\n",
    "            self.tokenizer.encode_plus(\n",
    "                query,\n",
    "                max_length=self.cfg.max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            for query in [anchor] + [self.miscocepts_maper[x] for x in neatives]\n",
    "        ]\n",
    "        return {\n",
    "            'input_ids':torch.stack([x.input_ids.squeeze(0) for x in encodes]),\n",
    "            'attention_mask':torch.stack([x.attention_mask.squeeze(0) for x in encodes]),\n",
    "            'token_type_ids':torch.stack([x.token_type_ids.squeeze(0) for x in encodes])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e178cce5-f61d-4ed6-910c-c95d3f877f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG.data\n",
    "        self.is_setup = False\n",
    "        self.is_prepared = False\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        if self.is_prepared: return None\n",
    "        self.df = make_df(self.cfg.train_path)\n",
    "        self.misconcepts_maper = pd.read_csv(self.cfg.misconcepts).set_index('MisconceptionId')['MisconceptionName'].to_dict()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer)\n",
    "        self.is_prepared = True\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        if self.is_setup: return None\n",
    "        kf = GroupKFold(n_splits=self.cfg.nfolds)\n",
    "        splits = [(x,y) for x,y in  kf.split(self.df,groups=self.df['question_id'])][CFG.fold_number]\n",
    "        self.train_df, self.val_df = self.df.iloc[splits[0]], self.df.iloc[splits[1]]\n",
    "        \n",
    "        self.train_dataset = PLDataset(self.train_df, self.misconcepts_maper, self.tokenizer)\n",
    "        self.val_dataset = PLDataset(self.val_df, self.misconcepts_maper, self.tokenizer)\n",
    "        #self.predict_dataset = PLDataset(self.test_df, self.misconcepts_maper, self.tokenizer)\n",
    "        self.is_setup = True\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          #batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          batch_sampler = NoDublicateSampler(\n",
    "                             self.train_dataset,\n",
    "                             self.train_df['negs'].tolist(),\n",
    "                             self.train_df['label'].tolist(),\n",
    "                             self.cfg.batch_size\n",
    "                         ),\n",
    "                         #shuffle=False\n",
    "                         )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset,\n",
    "                          batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b16259-cfa9-47a9-826c-c54b62f25c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "        self.history = []\n",
    "    \n",
    "    def update(self,y_t,y_p):\n",
    "        self.labels += y_t\n",
    "        self.preds += y_p\n",
    "        \n",
    "    def clean(self):\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "\n",
    "    def calc_metrics(self):\n",
    "        metrics = {}\n",
    "        metrics['accuracy'] = accuracy_score(self.labels, self.preds)\n",
    "        self.history.append(metrics)\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0313374a-5def-41d2-88b8-47b3194deed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG.model\n",
    "        self.model = CustomEncoder(self.cfg)\n",
    "        self.avg_meter = AverageMeter()\n",
    "        self.criterion = SoftNegativesCrossEntropy()\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        bs, num_negs, seq_ln = batch['attention_mask'].shape\n",
    "        batch['attention_mask'] = batch['attention_mask'].view((bs * num_negs),seq_ln)\n",
    "        batch['input_ids'] = batch['input_ids'].view((bs * num_negs),seq_ln)\n",
    "        batch['token_type_ids'] = batch['token_type_ids'].view((bs * num_negs),seq_ln)\n",
    "\n",
    "        output = self.model(**batch)\n",
    "        output['pooler_output'] = output['pooler_output'].view(bs, num_negs, -1)\n",
    "        output['last_hidden_state'] = output['last_hidden_state'].view(bs, num_negs, seq_ln, -1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, i):\n",
    "        out = self(batch)\n",
    "        loss = self.criterion(out['pooler_output'])\n",
    "        self.log('train_loss',loss.item())\n",
    "        return loss\n",
    "            \n",
    "    def validation_step(self, batch, i):\n",
    "        out = self(batch)\n",
    "        loss = self.criterion(out['pooler_output'])\n",
    "        self.log('val_loss',loss.item())\n",
    "        logits = F.cosine_similarity(out['pooler_output'][:,0:1,:],out['pooler_output'][:,1:,:],dim=2).argmax(dim=-1).tolist()\n",
    "        self.avg_meter.update([0] * len(logits),logits)\n",
    "                \n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.avg_meter.calc_metrics()\n",
    "        self.log_dict(metrics)\n",
    "        self.avg_meter.clean()\n",
    "            \n",
    "    def configure_optimizers(self):        \n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.model.model.named_parameters() if not any(nd in n for nd in self.cfg.no_decay)],\n",
    "             'lr': self.cfg.lr, 'weight_decay': self.cfg.weight_decay},\n",
    "            {'params': [p for n, p in self.model.model.named_parameters() if any(nd in n for nd in self.cfg.no_decay)],\n",
    "             'lr': self.cfg.lr, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        \n",
    "        optim = self.cfg.optim(\n",
    "            optimizer_parameters,\n",
    "            lr=self.cfg.lr,\n",
    "            betas=self.cfg.betas,\n",
    "            weight_decay=self.cfg.weight_decay,\n",
    "            eps=self.cfg.eps\n",
    "        )\n",
    "        \n",
    "        if self.cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(optim,\n",
    "                                                        num_training_steps=self.cfg.num_training_steps,\n",
    "                                                        num_warmup_steps=self.cfg.num_training_steps * self.cfg.warnap_steps,\n",
    "                                                        num_cycles=self.cfg.num_cycles)\n",
    "        elif self.cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                                        num_training_steps=self.cfg.num_training_steps,\n",
    "                                                        num_warmup_steps=self.cfg.num_training_steps * self.cfg.warnap_steps)\n",
    "        else:\n",
    "            return optim\n",
    "        \n",
    "        scheduler = {'scheduler': scheduler,'interval': 'step', 'frequency': 1}\n",
    "\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad0c8eb-faa8-4120-8ea3-08bdfdf9a7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm = PLDataModule()\n",
    "dm.prepare_data()\n",
    "dm.setup(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513a27d0-cd1a-429c-b444-312d62025667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG.model.num_training_steps = len(dm.train_dataloader()) * CFG.model.max_epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9255715d-e963-4947-9f2a-fce5a33d9d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PLModule().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "577a49a0-21af-4a65-a4f2-0727df9cc07e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrewkhl\u001b[0m (\u001b[33mandlh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240926_122700-qw1n6h16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andlh/Kaggle_Eedi/runs/qw1n6h16' target=\"_blank\">bge_soft_cross_enropy </a></strong> to <a href='https://wandb.ai/andlh/Kaggle_Eedi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andlh/Kaggle_Eedi' target=\"_blank\">https://wandb.ai/andlh/Kaggle_Eedi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andlh/Kaggle_Eedi/runs/qw1n6h16' target=\"_blank\">https://wandb.ai/andlh/Kaggle_Eedi/runs/qw1n6h16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/andlh/Kaggle_Eedi/runs/qw1n6h16?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8f00498c90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"31520b01739d418e5d77a11fd8a79a70b189b8bc\")\n",
    "os.environ['WANDB_API_KEY'] = \"31520b01739d418e5d77a11fd8a79a70b189b8bc\"\n",
    "wandb.init(project='Kaggle_Eedi',name='bge_soft_cross_enropy ',config=set_wandb_cfg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d0c800-73df-4260-a74c-88eb96a937c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='./outputs/',\n",
    "    filename='model_{epoch:02d}-{accuracy:.4f}',\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    precision=32,\n",
    "    callbacks = [lr_monitor,checkpoint_cb],\n",
    "    logger = pl.loggers.WandbLogger(save_code=True),\n",
    "    log_every_n_steps=1,\n",
    "    accumulate_grad_batches=CFG.model.grad_acum_steps,\n",
    "    enable_checkpointing=True,\n",
    "    min_epochs=1,\n",
    "    devices=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    max_epochs=CFG.model.max_epoches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0219ffba-4b11-46ca-982c-b17836746197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /notebooks/outputs exists and is not empty.\n",
      "Restoring states from the checkpoint path at outputs/model_epoch=01-accuracy=0.5412.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                      | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | model     | CustomEncoder             | 335 M  | train\n",
      "1 | criterion | SoftNegativesCrossEntropy | 0      | train\n",
      "----------------------------------------------------------------\n",
      "335 M     Trainable params\n",
      "0         Non-trainable params\n",
      "335 M     Total params\n",
      "1,340.568 Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "444       Modules in eval mode\n",
      "Restored all states from the checkpoint at outputs/model_epoch=01-accuracy=0.5412.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iters 20352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a07ff6dede491eb0023082123817cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iters 53128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iters 64211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iters 82095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm, ckpt_path=\"outputs/model_epoch=01-accuracy=0.5412.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72444d-8097-4b90-aa7f-a4c811ef3481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
