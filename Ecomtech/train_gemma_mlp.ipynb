{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "756740a8-b71a-42a3-88ec-442e2b30e9c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 56\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from src import CustomClassifierEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import rankdata\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup, AutoModel, AutoTokenizer, AutoModelForMultipleChoice, AutoConfig\n",
    "import wandb\n",
    "pl.seed_everything(56)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d94cc50a-c29b-4368-8a7a-d40bf38a83c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e5bd5d517047ae9b623aaada0a9e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for gemma 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\n",
    "# for llama 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\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731f98cf-68d7-441b-bac4-267a8bf2d1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    class data:\n",
    "        train_path = 'train.csv'\n",
    "        test_path = 'test.csv'\n",
    "        tokenizer = 'IlyaGusev/gemma-2-9b-it-abliterated'#'microsoft/mdeberta-v3-base'\n",
    "        num_workers = 8\n",
    "        nfolds = 5\n",
    "        batch_size = 4\n",
    "        use_prefix = False\n",
    "        max_length = 105 \n",
    "        seed = 56\n",
    "    class model:\n",
    "        model = 'IlyaGusev/gemma-2-9b-it-abliterated'#'microsoft/mdeberta-v3-base'\n",
    "        optim = torch.optim.AdamW\n",
    "        use_only_encoder=False\n",
    "        grad_acum_steps = 1\n",
    "        torch_dtype = torch.bfloat16\n",
    "        scheduler= 'cosine'\n",
    "        warnap_steps = 0.0 #0.25\n",
    "        num_labels = 50\n",
    "        label_smoothing = 0.0\n",
    "        lr = lr_fn = 1e-5 #1e-4\n",
    "        cls_drop_type = None\n",
    "        cls_drop = 0.0\n",
    "        pool = 'attention'\n",
    "        max_epoches = 10\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        turn_off_drop = True\n",
    "        num_cycles = 0.5\n",
    "        eps = 1e-7\n",
    "        weight_decay = 0.0\n",
    "        weight_decay_fn = 0.0\n",
    "        betas = (0.9, 0.999)\n",
    "        use_lora = True\n",
    "        class lora:\n",
    "            r = 64\n",
    "            lora_alpha = 128\n",
    "            lora_dropout = 0.05\n",
    "            bias = 'none'\n",
    "            use_dora = False\n",
    "            target_modules = ['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj']\n",
    "            layers_to_transform = None#list(range(42))\n",
    "\n",
    "    seed = 56\n",
    "    fold_number = 0\n",
    "\n",
    "def set_wandb_cfg():\n",
    "    config = {}\n",
    "    for k,v in CFG.model.__dict__.items():\n",
    "        if '__' not in k:\n",
    "            config[k] = v\n",
    "    for k,v in CFG.data.__dict__.items():\n",
    "        if '__' not in k:\n",
    "            config[k] = v\n",
    "    config['fold_number'] = CFG.fold_number\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52a4057d-e5b9-4a3c-a997-ffd24219ba3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_df(path,is_test=False):\n",
    "    data = pd.read_csv(path)\n",
    "    df = pd.DataFrame()\n",
    "    if is_test:\n",
    "        df['label'] = [[0] * 50] * len(df)\n",
    "    else:\n",
    "        df['label'] = data.apply(lambda x: [x[f'trend_id_res{i}'] for i in range(50)],axis=1)\n",
    "    df['text'] = data['text']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea959bbb-4502-4489-b1b7-e29dc320e827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG.data\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]        \n",
    "        \n",
    "        encodes = self.tokenizer.encode_plus(\n",
    "            row['text'],\n",
    "            max_length=self.cfg.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodes.input_ids.squeeze(0),\n",
    "            'attention_mask': encodes.attention_mask.squeeze(0),\n",
    "            #'token_type_ids': encodes.token_type_ids.squeeze(0),\n",
    "            'labels': torch.tensor(row['label'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa988de6-c9e1-4deb-b61e-ef6f0c78f7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG.data\n",
    "        self.is_setup = False\n",
    "        self.is_prepared = False\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        if self.is_prepared: return None\n",
    "        self.df = make_df(self.cfg.train_path)\n",
    "        self.test_df = make_df(self.cfg.test_path,is_test=True)\n",
    "        self.test_df['text'] = self.test_df['text'].fillna('')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer)\n",
    "        self.is_prepared = True\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        if self.is_setup: return None\n",
    "        kf = MultilabelStratifiedKFold(n_splits=self.cfg.nfolds, shuffle=True, random_state=self.cfg.seed)\n",
    "        splits = [(x,y) for x,y in  kf.split(self.df.values,np.stack(dm.df['label'].values))][CFG.fold_number]\n",
    "        self.train_df, self.val_df = self.df.iloc[splits[0]], self.df.iloc[splits[1]]\n",
    "        self.train_dataset = PLDataset(self.train_df,self.tokenizer)\n",
    "        self.val_dataset = PLDataset(self.val_df,self.tokenizer)\n",
    "        self.predict_dataset = PLDataset(self.test_df,self.tokenizer)\n",
    "        self.is_setup = True\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                         batch_size=self.cfg.batch_size,\n",
    "                         num_workers=self.cfg.num_workers,\n",
    "                         pin_memory=True,\n",
    "                         shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset,\n",
    "                          batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "699f4a09-f9d3-4f3e-b062-2d1487142ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "        self.history = []\n",
    "    \n",
    "    def update(self,y_t,y_p):\n",
    "        self.labels += y_t\n",
    "        self.preds += y_p\n",
    "        \n",
    "    def clean(self):\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "\n",
    "    def calc_metrics(self):\n",
    "        metrics = {}\n",
    "\n",
    "        preds = [list(map(lambda y: str(round(y)),x)) for x in self.preds]\n",
    "        labels = [''.join(map(str,x)) for x in self.labels]\n",
    "        for i in range(len(self.preds)):\n",
    "            preds[i][np.argmax(self.preds[i])] = '1'\n",
    "        preds = [''.join(x) for x in preds]\n",
    "        \n",
    "        metrics['accuracy'] = accuracy_score(labels, preds)\n",
    "        self.history.append(metrics)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f08e127-e05a-408d-9fa6-7fe4ab7e157e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PLModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG.model\n",
    "        self.model = CustomClassifierEncoder(self.cfg)\n",
    "        self.avg_meter = AverageMeter()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        output = self.model(**batch)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, i):\n",
    "        logits = self(batch).logits\n",
    "        loss = self.criterion(logits, batch['labels'].float())\n",
    "        self.log('train_loss', loss.item())\n",
    "        return loss\n",
    "            \n",
    "    def validation_step(self, batch, i):\n",
    "        logits = self(batch).logits\n",
    "        loss = self.criterion(logits, batch['labels'].float())\n",
    "        self.log('val_loss',loss.item())\n",
    "        \n",
    "        preds = logits.sigmoid().tolist()\n",
    "        labels = batch['labels'].tolist()\n",
    "        \n",
    "        self.avg_meter.update(labels,preds)\n",
    "    \n",
    "    def predict_step(self, batch, i):\n",
    "        logits = self(batch).logits\n",
    "        return logits.sigmoid().tolist()\n",
    "                \n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.avg_meter.calc_metrics()\n",
    "        self.log_dict(metrics)\n",
    "        self.avg_meter.clean()\n",
    "            \n",
    "    def configure_optimizers(self):        \n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.model.model.named_parameters() if not any(nd in n for nd in self.cfg.no_decay)],\n",
    "             'lr': self.cfg.lr, 'weight_decay': self.cfg.weight_decay},\n",
    "            {'params': [p for n, p in self.model.model.named_parameters() if any(nd in n for nd in self.cfg.no_decay)],\n",
    "             'lr': self.cfg.lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if \"model\" not in n],\n",
    "             'lr': self.cfg.lr_fn, 'weight_decay': self.cfg.weight_decay_fn}\n",
    "        ]\n",
    "        \n",
    "        optim = self.cfg.optim(\n",
    "            optimizer_parameters,\n",
    "            lr=self.cfg.lr,\n",
    "            betas=self.cfg.betas,\n",
    "            weight_decay=self.cfg.weight_decay,\n",
    "            eps=self.cfg.eps\n",
    "        )\n",
    "        \n",
    "        if self.cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(optim,\n",
    "                                                        num_training_steps=self.cfg.num_training_steps,\n",
    "                                                        num_warmup_steps=self.cfg.num_training_steps * self.cfg.warnap_steps,\n",
    "                                                        num_cycles=self.cfg.num_cycles)\n",
    "        elif self.cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                                        num_training_steps=self.cfg.num_training_steps,\n",
    "                                                        num_warmup_steps=self.cfg.num_training_steps * self.cfg.warnap_steps)\n",
    "        else:\n",
    "            return optim\n",
    "        \n",
    "        scheduler = {'scheduler': scheduler,'interval': 'step', 'frequency': 1}\n",
    "\n",
    "        return [optim], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c56345f-a659-4ccd-bb1b-f716b4f8129a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm = PLDataModule()\n",
    "dm.prepare_data()\n",
    "dm.setup(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971ca6d2-7982-4728-b2f8-f2d742f06afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dm.tokenizer.pad_token = dm.tokenizer.eos_token\n",
    "dm.tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c155de9-3dd1-4f46-8e17-cdc753fe33de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG.model.num_training_steps = len(dm.train_dataloader()) * CFG.model.max_epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e5f61db-2177-4602-b86c-2172ac96721b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4046d46fd54c62b7073fc22f353fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = PLModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b884ea60-f5b0-44c0-b26c-cc7fb480b0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.model.fc = nn.Sequential(\n",
    "#    nn.Linear(3584,1792),\n",
    "#    nn.GELU(),\n",
    "#    nn.Linear(1792,50)\n",
    "#)\n",
    "\n",
    "model.model.fc = nn.Sequential(\n",
    "    nn.Linear(3584,3584 * 3 // 2),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(3584 * 3 // 2,50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38db5475-ff4b-4d4d-9433-22005ad8b89b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:nbxqgha5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▅▆▅█▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>lr-AdamW/pg1</td><td>█████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr-AdamW/pg2</td><td>█████████▇▇▇▇▇▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr-AdamW/pg3</td><td>██████▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▄█▆▇▆▃▃█▆▃▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>▃▁▂▃▅▆█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.63309</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>lr-AdamW/pg1</td><td>0.0</td></tr><tr><td>lr-AdamW/pg2</td><td>0.0</td></tr><tr><td>lr-AdamW/pg3</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.0001</td></tr><tr><td>trainer/global_step</td><td>9199</td></tr><tr><td>val_loss</td><td>0.05386</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">IlyaGusev/gemma-2-9b-it-abliterated_mlp</strong> at: <a href='https://wandb.ai/andlh/DLS/runs/nbxqgha5' target=\"_blank\">https://wandb.ai/andlh/DLS/runs/nbxqgha5</a><br/> View project at: <a href='https://wandb.ai/andlh/DLS' target=\"_blank\">https://wandb.ai/andlh/DLS</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241007_193746-nbxqgha5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:nbxqgha5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20241007_210122-7x58ytq3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andlh/DLS/runs/7x58ytq3' target=\"_blank\">IlyaGusev/gemma-2-9b-it-abliterated_mlp_large</a></strong> to <a href='https://wandb.ai/andlh/DLS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andlh/DLS' target=\"_blank\">https://wandb.ai/andlh/DLS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andlh/DLS/runs/7x58ytq3' target=\"_blank\">https://wandb.ai/andlh/DLS/runs/7x58ytq3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/andlh/DLS/runs/7x58ytq3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f91189d1b50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"31520b01739d418e5d77a11fd8a79a70b189b8bc\")\n",
    "os.environ['WANDB_API_KEY'] = \"31520b01739d418e5d77a11fd8a79a70b189b8bc\"\n",
    "wandb.init(project='DLS',name='IlyaGusev/gemma-2-9b-it-abliterated_mlp_large',config=set_wandb_cfg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "475c92bd-c501-4f74-b67f-207edf15cff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightning_fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='./outputs/',\n",
    "    filename='model_{epoch:02d}-{accuracy:.4f}',\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    precision='bf16',\n",
    "    callbacks = [lr_monitor],\n",
    "    logger = pl.loggers.WandbLogger(save_code=True),\n",
    "    log_every_n_steps=1,\n",
    "    accumulate_grad_batches=CFG.model.grad_acum_steps,\n",
    "    enable_checkpointing=False,\n",
    "    min_epochs=1,\n",
    "    devices=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    max_epochs=CFG.model.max_epoches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66647b8c-fc77-46ce-92f7-e535a54ca8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                    | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model     | CustomClassifierEncoder | 9.5 B  | train\n",
      "1 | criterion | BCEWithLogitsLoss       | 0      | train\n",
      "--------------------------------------------------------------\n",
      "237 M     Trainable params\n",
      "9.2 B     Non-trainable params\n",
      "9.5 B     Total params\n",
      "37,916.624Total estimated model params size (MB)\n",
      "2955      Modules in train mode\n",
      "676       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad13f97af95467ca6102d612a9444ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1db3d428-4d3d-40bb-98e5-e376492da049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c322e365d00043b5a5da28817df3787b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_val = trainer.predict(model,dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e1fbf3b-4cbb-4812-a755-11bf4a7cd9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e156c44116c34e7d927836c7b3bfdf06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_test = trainer.predict(model,dm.predict_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78642b43-9845-46af-a55e-e8f2dbb9d716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.save(\"IlyaGusev-gemma-2-9b-it-abliterated_mlp_large_val-fold-0.npy\",np.concatenate(preds_val))\n",
    "np.save(\"IlyaGusev-gemma-2-9b-it-abliterated_mlp_large_test_fold-0.npy\",np.concatenate(preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "763cefbd-0be9-488a-99e7-6ce8eaab5184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T14:27:50.961632Z",
     "iopub.status.busy": "2024-10-06T14:27:50.961011Z",
     "iopub.status.idle": "2024-10-06T14:27:50.971261Z",
     "shell.execute_reply": "2024-10-06T14:27:50.969257Z",
     "shell.execute_reply.started": "2024-10-06T14:27:50.961608Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_predict(pred):\n",
    "    preds_r = list(map(lambda y: str(round(y)),pred))\n",
    "    preds_r[np.argmax(pred)] = '1'\n",
    "    return ' '.join([str(i) for i,x in enumerate(preds_r) if x == '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e1854f0-29c6-43ee-b0de-5a6180c4752e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_pr = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe078051-2555-4e7f-904f-dbdd7cc07a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35973096e-07, 9.92187500e-01, 1.00000000e+00, 1.47223473e-05,\n",
       "       4.50015068e-06, 3.46451998e-07, 9.53674316e-06, 1.04773790e-08,\n",
       "       3.12328339e-05, 2.14576721e-05, 4.04357910e-04, 3.93018126e-07,\n",
       "       9.60937500e-01, 1.52504072e-08, 5.14984131e-05, 3.52859497e-05,\n",
       "       6.34463504e-09, 6.89178705e-07, 4.94765118e-09, 1.04773790e-08,\n",
       "       1.18743628e-08, 2.11596489e-06, 5.69969416e-07, 3.25962901e-07,\n",
       "       3.84170562e-09, 6.00703061e-08, 3.25962901e-07, 2.72691250e-06,\n",
       "       3.25962901e-07, 1.20699406e-06, 1.29938126e-05, 2.23517418e-07,\n",
       "       4.50015068e-06, 1.44354999e-07, 6.89178705e-07, 3.29315662e-06,\n",
       "       1.66992188e-01, 5.78165054e-06, 4.45172191e-07, 1.44354999e-07,\n",
       "       5.36441803e-07, 8.58562998e-10, 9.53674316e-06, 6.84522092e-08,\n",
       "       2.99769454e-09, 7.72997737e-08, 3.25962901e-07, 2.11596489e-06,\n",
       "       1.52504072e-08, 6.48200512e-07])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_pr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2369914a-0442-4f3f-815e-e07b6e5417b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_pr = [prepare_predict(x) for x in preds_pr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c214df-0653-4b0c-ba77-0227e37582e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_submission_formated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3453a74-37ad-4d88-8cbd-a431be97f94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['target'] = preds_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77a1cf41-d8bd-48a0-a75f-6a35a7deaa24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('simple_subv2_nlp.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa0824-6375-40a0-9a5d-78741a0c1186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
