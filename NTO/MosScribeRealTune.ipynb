{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ced5ceec-bd74-46cc-80bb-d60a4f3d1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.optim import Adam, AdamW, SGD\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from molscribe.dataset import TrainDataset, AuxTrainDataset, bms_collate\n",
    "from molscribe.model import Encoder, Decoder\n",
    "from molscribe.loss import Criterion\n",
    "from molscribe.utils import seed_torch, save_args, init_summary_writer, LossMeter, AverageMeter, asMinutes, timeSince, \\\n",
    "    print_rank_0, format_df\n",
    "from molscribe.chemistry import convert_graph_to_smiles, postprocess_smiles, keep_main_molecule\n",
    "from molscribe.tokenizer import get_tokenizer\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05d03e4-f5b7-48c2-b0ca-fa7c0f76e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48541fd1-49d3-4afa-8864-5dc3bf8ab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    parser.add_argument('--do_valid', action='store_true')\n",
    "    parser.add_argument('--do_test', action='store_true')\n",
    "    parser.add_argument('--fp16', action='store_true')\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--print_freq', type=int, default=200)\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "    parser.add_argument('--backend', type=str, default='gloo', choices=['gloo', 'nccl'])\n",
    "    # Model\n",
    "    parser.add_argument('--encoder', type=str, default='resnet34')\n",
    "    parser.add_argument('--decoder', type=str, default='lstm')\n",
    "    parser.add_argument('--no_pretrained', action='store_true')\n",
    "    parser.add_argument('--use_checkpoint', action='store_true')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5)\n",
    "    parser.add_argument('--embed_dim', type=int, default=256)\n",
    "    parser.add_argument('--enc_pos_emb', action='store_true')\n",
    "    group = parser.add_argument_group(\"lstm_options\")\n",
    "    group.add_argument('--decoder_dim', type=int, default=512)\n",
    "    group.add_argument('--decoder_layer', type=int, default=1)\n",
    "    group.add_argument('--attention_dim', type=int, default=256)\n",
    "    group = parser.add_argument_group(\"transformer_options\")\n",
    "    group.add_argument(\"--dec_num_layers\", help=\"No. of layers in transformer decoder\", type=int, default=6)\n",
    "    group.add_argument(\"--dec_hidden_size\", help=\"Decoder hidden size\", type=int, default=256)\n",
    "    group.add_argument(\"--dec_attn_heads\", help=\"Decoder no. of attention heads\", type=int, default=8)\n",
    "    group.add_argument(\"--dec_num_queries\", type=int, default=128)\n",
    "    group.add_argument(\"--hidden_dropout\", help=\"Hidden dropout\", type=float, default=0.1)\n",
    "    group.add_argument(\"--attn_dropout\", help=\"Attention dropout\", type=float, default=0.1)\n",
    "    group.add_argument(\"--max_relative_positions\", help=\"Max relative positions\", type=int, default=0)\n",
    "    # Data\n",
    "    parser.add_argument('--data_path', type=str, default=None)\n",
    "    parser.add_argument('--train_file', type=str, default=None)\n",
    "    parser.add_argument('--valid_file', type=str, default=None)\n",
    "    parser.add_argument('--test_file', type=str, default=None)\n",
    "    parser.add_argument('--aux_file', type=str, default=None)\n",
    "    parser.add_argument('--coords_file', type=str, default=None)\n",
    "    parser.add_argument('--vocab_file', type=str, default=None)\n",
    "    parser.add_argument('--dynamic_indigo', action='store_true')\n",
    "    parser.add_argument('--default_option', action='store_true')\n",
    "    parser.add_argument('--pseudo_coords', action='store_true')\n",
    "    parser.add_argument('--include_condensed', action='store_true')\n",
    "    parser.add_argument('--formats', type=str, default=None)\n",
    "    parser.add_argument('--num_workers', type=int, default=8)\n",
    "    parser.add_argument('--input_size', type=int, default=384)\n",
    "    parser.add_argument('--multiscale', action='store_true')\n",
    "    parser.add_argument('--augment', action='store_true')\n",
    "    parser.add_argument('--mol_augment', action='store_true')\n",
    "    parser.add_argument('--coord_bins', type=int, default=100)\n",
    "    parser.add_argument('--sep_xy', action='store_true')\n",
    "    parser.add_argument('--mask_ratio', type=float, default=0)\n",
    "    parser.add_argument('--continuous_coords', action='store_true')\n",
    "    # Training\n",
    "    parser.add_argument('--epochs', type=int, default=8)\n",
    "    parser.add_argument('--batch_size', type=int, default=256)\n",
    "    parser.add_argument('--encoder_lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--decoder_lr', type=float, default=4e-4)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-6)\n",
    "    parser.add_argument('--max_grad_norm', type=float, default=5.)\n",
    "    parser.add_argument('--scheduler', type=str, choices=['cosine', 'constant'], default='cosine')\n",
    "    parser.add_argument('--warmup_ratio', type=float, default=0)\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1)\n",
    "    parser.add_argument('--load_path', type=str, default=None)\n",
    "    parser.add_argument('--load_encoder_only', action='store_true')\n",
    "    parser.add_argument('--train_steps_per_epoch', type=int, default=-1)\n",
    "    parser.add_argument('--save_path', type=str, default='output/')\n",
    "    parser.add_argument('--save_mode', type=str, default='best', choices=['best', 'all', 'last'])\n",
    "    parser.add_argument('--load_ckpt', type=str, default='best')\n",
    "    parser.add_argument('--resume', action='store_true')\n",
    "    parser.add_argument('--all_data', action='store_true', help='Use both train and valid data for training.')\n",
    "    parser.add_argument('--init_scheduler', action='store_true')\n",
    "    parser.add_argument('--label_smoothing', type=float, default=0.0)\n",
    "    parser.add_argument('--shuffle_nodes', action='store_true')\n",
    "    parser.add_argument('--save_image', action='store_true')\n",
    "    # Inference\n",
    "    parser.add_argument('--beam_size', type=int, default=1)\n",
    "    parser.add_argument('--n_best', type=int, default=1)\n",
    "    parser.add_argument('--predict_coords', action='store_true')\n",
    "    parser.add_argument('--save_attns', action='store_true')\n",
    "    parser.add_argument('--molblock', action='store_true')\n",
    "    parser.add_argument('--compute_confidence', action='store_true')\n",
    "    parser.add_argument('--keep_main_molecule', action='store_true')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0b3779f-3350-4c78-bd2a-cd66769b69d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_states(args, load_path):\n",
    "    if load_path.endswith('.pth'):\n",
    "        path = load_path\n",
    "    elif args.load_ckpt == 'best':\n",
    "        path = os.path.join(load_path, f'{args.encoder}_{args.decoder}_best.pth')\n",
    "    else:\n",
    "        path = os.path.join(load_path, f'{args.encoder}_{args.decoder}_{args.load_ckpt}.pth')\n",
    "    print_rank_0('Load ' + path)\n",
    "    states = torch.load(path, map_location=torch.device('cpu'))\n",
    "    return states\n",
    "\n",
    "def safe_load(module, module_states):\n",
    "    def remove_prefix(state_dict):\n",
    "        return {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    missing_keys, unexpected_keys = module.load_state_dict(remove_prefix(module_states), strict=False)\n",
    "    if missing_keys:\n",
    "        print_rank_0('Missing keys: ' + str(missing_keys))\n",
    "    if unexpected_keys:\n",
    "        print_rank_0('Unexpected keys: ' + str(unexpected_keys))\n",
    "    return\n",
    "\n",
    "def get_model(args, tokenizer, device,load_path):\n",
    "    encoder = Encoder(args, pretrained=(not args.no_pretrained and load_path is None))\n",
    "    args.encoder_dim = encoder.n_features\n",
    "    print_rank_0(f'encoder_dim: {args.encoder_dim}')\n",
    "\n",
    "    decoder = Decoder(args, tokenizer)\n",
    "    if load_path:\n",
    "        states = load_states(args, load_path)\n",
    "        safe_load(encoder, states['encoder'])\n",
    "        safe_load(decoder, states['decoder'])\n",
    "        # print_rank_0(f\"Model loaded from {load_path}\")\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6159f58a-d808-451e-a8e2-efb2847e387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(args, encoder, decoder, load_path=None):\n",
    "    encoder_optimizer = AdamW(encoder.parameters(), lr=args.encoder_lr, weight_decay=args.weight_decay, amsgrad=False)\n",
    "    encoder_scheduler = get_scheduler(args.scheduler, encoder_optimizer, args.num_warmup_steps, args.num_training_steps)\n",
    "\n",
    "    decoder_optimizer = AdamW(decoder.parameters(), lr=args.decoder_lr, weight_decay=args.weight_decay, amsgrad=False)\n",
    "    decoder_scheduler = get_scheduler(args.scheduler, decoder_optimizer, args.num_warmup_steps, args.num_training_steps)\n",
    "\n",
    "    if load_path and args.resume:\n",
    "        states = load_states(args, load_path)\n",
    "        encoder_optimizer.load_state_dict(states['encoder_optimizer'])\n",
    "        decoder_optimizer.load_state_dict(states['decoder_optimizer'])\n",
    "        if args.init_scheduler:\n",
    "            for group in encoder_optimizer.param_groups:\n",
    "                group['lr'] = args.encoder_lr\n",
    "            for group in decoder_optimizer.param_groups:\n",
    "                group['lr'] = args.decoder_lr\n",
    "        else:\n",
    "            encoder_scheduler.load_state_dict(states['encoder_scheduler'])\n",
    "            decoder_scheduler.load_state_dict(states['decoder_scheduler'])\n",
    "        print_rank_0(f\"Optimizer loaded from {load_path}\")\n",
    "\n",
    "    return encoder_optimizer, encoder_scheduler, decoder_optimizer, decoder_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460e8ac6-5563-4154-902c-90d5284dfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch,\n",
    "             encoder_scheduler, decoder_scheduler, scaler, device, global_step, SUMMARY, args):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    loss_meter = LossMeter()\n",
    "    # switch to train mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    start = end = time.time()\n",
    "    encoder_grad_norm = decoder_grad_norm = 0\n",
    "\n",
    "    for step, (indices, images, refs) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=args.fp16):\n",
    "            features, hiddens = encoder(images, refs)\n",
    "            results = decoder(features, hiddens, refs)\n",
    "            losses = criterion(results, refs)\n",
    "            loss = sum(losses.values())\n",
    "        # record loss\n",
    "        loss_meter.update(loss, losses, batch_size)\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(encoder_optimizer)\n",
    "            scaler.unscale_(decoder_optimizer)\n",
    "            encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), args.max_grad_norm)\n",
    "            decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), args.max_grad_norm)\n",
    "            scaler.step(encoder_optimizer)\n",
    "            scaler.step(decoder_optimizer)\n",
    "            scaler.update()\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_scheduler.step()\n",
    "            decoder_scheduler.step()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % args.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            loss_str = ' '.join([f'{k}:{v.avg:.4f}' for k, v in loss_meter.subs.items()])\n",
    "            print_rank_0('Epoch: [{0}][{1}/{2}] '\n",
    "                         'Data {data_time.avg:.3f}s ({sum_data_time}) '\n",
    "                         'Run {remain:s} '\n",
    "                         'Loss: {loss.avg:.4f} ({loss_str}) '\n",
    "                         'Grad: {encoder_grad_norm:.3f}/{decoder_grad_norm:.3f} '\n",
    "                         'LR: {encoder_lr:.6f} {decoder_lr:.6f}'\n",
    "            .format(\n",
    "                epoch + 1, step, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=loss_meter, loss_str=loss_str,\n",
    "                sum_data_time=asMinutes(data_time.sum),\n",
    "                remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                encoder_grad_norm=encoder_grad_norm,\n",
    "                decoder_grad_norm=decoder_grad_norm,\n",
    "                encoder_lr=encoder_scheduler.get_lr()[0],\n",
    "                decoder_lr=decoder_scheduler.get_lr()[0]))\n",
    "            loss_meter.reset()\n",
    "        if args.train_steps_per_epoch != -1 and (\n",
    "                step + 1) // args.gradient_accumulation_steps == args.train_steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    return loss_meter.epoch.avg, global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0d4755d-41a1-445c-9b68-d16549cf1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b2ac1db-7ebf-42af-96ba-208f3f379a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "args.train_file = 'train_df2.csv'\n",
    "args.vocab_file = 'vocab_chars.json'\n",
    "args.formats = ['chartok_coords']\n",
    "args.coord_bins = 64\n",
    "args.input_size = 384\n",
    "args.encoder = 'swin_base'\n",
    "args.decoder = 'transformer'\n",
    "args.encoder_lr = 5e-5\n",
    "args.decoder_lr = 5e-5\n",
    "args.label_smoothing = 2e-2\n",
    "args.data_path= './train/'\n",
    "args.encoder_dim = 1024\n",
    "args.dropout = 0.5\n",
    "args.embed_dim = 256\n",
    "args.decoder_dim = 512\n",
    "args.decoder_layer = 1\n",
    "args.attention_dim = 256\n",
    "args.dec_num_layers = 6\n",
    "args.dec_hidden_size = 256\n",
    "args.dec_attn_heads = 8\n",
    "args.dec_num_queries = 128\n",
    "args.hidden_dropout = 0.1\n",
    "args.attn_dropout = 0.1\n",
    "args.epochs = 1\n",
    "args.batch_size = 64\n",
    "args.num_workers = 12\n",
    "args.gradient_accumulation_steps = 1\n",
    "args.warmup = 0.02\n",
    "args.print_freq = 25\n",
    "args.device = 'cuda'\n",
    "args.load_path = args.save_path\n",
    "args.local_rank = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e4e8145-2fa8-4679-a1f8-ecb73bc4f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainARGS:\n",
    "    formats = ['chartok_coords','edges']\n",
    "    input_size = 384\n",
    "    save_image = False\n",
    "    mol_augment = None\n",
    "    default_option = None\n",
    "    shuffle_nodes = True\n",
    "    include_condensed = None\n",
    "    vocab_file = 'vocab_chars.json'\n",
    "    coord_bins = 64\n",
    "    sep_xy = True\n",
    "    continuous_coords = None\n",
    "    data_path = './train/'\n",
    "    augment = None\n",
    "    coords_file = None\n",
    "    pseudo_coords = None\n",
    "    predict_coords = None\n",
    "    encoder = 'swin_base'\n",
    "    decoder = 'transformer'\n",
    "    use_checkpoint = False\n",
    "    encoder_dim = 1024\n",
    "    dropout = 0.5\n",
    "    embed_dim = 256\n",
    "    decoder_dim = 512\n",
    "    decoder_layer = 1\n",
    "    attention_dim = 256\n",
    "    dec_num_layers = 6\n",
    "    dec_hidden_size = 256\n",
    "    dec_attn_heads = 8\n",
    "    dec_num_queries = 128\n",
    "    hidden_dropout = 0.1\n",
    "    attn_dropout = 0.1\n",
    "    max_relative_positions = 0\n",
    "    no_pretrained = True\n",
    "    label_smoothing = 0.0\n",
    "    compute_confidence = None\n",
    "    enc_pos_emb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c239c9eb-9ea1-4e3f-bc59-94a82c910a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f83a2256-cfcc-4e11-b455-7ac32b268743",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(args, train_df, tokenizer, split='train', dynamic_indigo=args.dynamic_indigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c589139-e623-4735-8a3c-c68146762f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=args.batch_size,\n",
    "                          num_workers=args.num_workers,\n",
    "                          prefetch_factor=4,\n",
    "                          persistent_workers=True,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True,\n",
    "                          collate_fn=bms_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acd7a099-0acf-488e-b1d4-c6bd4ef612c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_dim: 1024\n",
      "Load /root/.cache/huggingface/hub/models--yujieq--MolScribe/snapshots/601bb0f491fb9598fa40227ae9759f800661cd74/swin_base_char_aux_1m680k.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Decoder:\n\tsize mismatch for decoder.chartok_coords.output_layer.weight: copying a param with shape torch.Size([229, 256]) from checkpoint, the shape in current model is torch.Size([165, 256]).\n\tsize mismatch for decoder.chartok_coords.output_layer.bias: copying a param with shape torch.Size([229]) from checkpoint, the shape in current model is torch.Size([165]).\n\tsize mismatch for decoder.chartok_coords.embeddings.make_embedding.emb_luts.0.weight: copying a param with shape torch.Size([229, 256]) from checkpoint, the shape in current model is torch.Size([165, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder, decoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainARGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [47], line 32\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(args, tokenizer, device, load_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m     states \u001b[38;5;241m=\u001b[39m load_states(args, load_path)\n\u001b[1;32m     31\u001b[0m     safe_load(encoder, states[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 32\u001b[0m     \u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# print_rank_0(f\"Model loaded from {load_path}\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m encoder\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn [47], line 16\u001b[0m, in \u001b[0;36msafe_load\u001b[0;34m(module, module_states)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_prefix\u001b[39m(state_dict):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 16\u001b[0m missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[1;32m     18\u001b[0m     print_rank_0(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing keys: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(missing_keys))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Decoder:\n\tsize mismatch for decoder.chartok_coords.output_layer.weight: copying a param with shape torch.Size([229, 256]) from checkpoint, the shape in current model is torch.Size([165, 256]).\n\tsize mismatch for decoder.chartok_coords.output_layer.bias: copying a param with shape torch.Size([229]) from checkpoint, the shape in current model is torch.Size([165]).\n\tsize mismatch for decoder.chartok_coords.embeddings.make_embedding.emb_luts.0.weight: copying a param with shape torch.Size([229, 256]) from checkpoint, the shape in current model is torch.Size([165, 256])."
     ]
    }
   ],
   "source": [
    "encoder, decoder = get_model(TrainARGS, tokenizer,'cuda',ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b29fc142-a807-44a5-abfa-ae8b798e4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = hf_hub_download('yujieq/MolScribe', 'swin_base_char_aux_1m680k.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48db29eb-cd7b-432c-a902-b81dff5a2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(args, train_df, valid_df, aux_df, tokenizer, save_path):\n",
    "    SUMMARY = None\n",
    "\n",
    "    if args.local_rank == 0 and not args.debug:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        save_args(args)\n",
    "        SUMMARY = init_summary_writer(save_path)\n",
    "\n",
    "    print_rank_0(\"========== training ==========\")\n",
    "\n",
    "    device = args.device\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "\n",
    "    if aux_df is None:\n",
    "        train_dataset = TrainDataset(args, train_df, tokenizer, split='train', dynamic_indigo=args.dynamic_indigo)\n",
    "        print_rank_0(train_dataset.transform)\n",
    "    else:\n",
    "        train_dataset = AuxTrainDataset(args, train_df, aux_df, tokenizer)\n",
    "    if args.local_rank != -1:\n",
    "        train_sampler = DistributedSampler(train_dataset, shuffle=True)\n",
    "    else:\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "    # TODO: may need to set timeout, as sometimes train_loader unexpectedly stucks\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=args.batch_size,\n",
    "                              sampler=train_sampler,\n",
    "                              num_workers=args.num_workers,\n",
    "                              prefetch_factor=4,\n",
    "                              persistent_workers=True,\n",
    "                              pin_memory=True,\n",
    "                              drop_last=True,\n",
    "                              collate_fn=bms_collate)\n",
    "\n",
    "    if args.train_steps_per_epoch == -1:\n",
    "        args.train_steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps\n",
    "    args.num_training_steps = args.epochs * args.train_steps_per_epoch\n",
    "    args.num_warmup_steps = int(args.num_training_steps * args.warmup_ratio)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    if args.resume and args.load_path is None:\n",
    "        args.load_path = args.save_path\n",
    "    encoder, decoder = get_model(args, tokenizer, device, load_path=args.load_path)\n",
    "    encoder_optimizer, encoder_scheduler, decoder_optimizer, decoder_scheduler = \\\n",
    "        get_optimizer_and_scheduler(args, encoder, decoder, load_path=args.load_path)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=args.fp16)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = Criterion(args, tokenizer).to(device)\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_loss = np.inf\n",
    "\n",
    "    global_step = encoder_scheduler.last_epoch\n",
    "    start_epoch = global_step // args.train_steps_per_epoch\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "        if args.local_rank != -1:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            dist.barrier()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss, global_step = train_fn(\n",
    "            train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch,\n",
    "            encoder_scheduler, decoder_scheduler, scaler, device, global_step, SUMMARY, args)\n",
    "\n",
    "        # eval\n",
    "        scores = inference(args, valid_df, tokenizer, encoder, decoder, save_path, split='valid')\n",
    "\n",
    "        if args.local_rank != 0:\n",
    "            continue\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print_rank_0(f'Epoch {epoch + 1} - Time: {elapsed:.0f}s')\n",
    "        print_rank_0(f'Epoch {epoch + 1} - Score: ' + json.dumps(scores))\n",
    "\n",
    "        save_obj = {\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "            'encoder_scheduler': encoder_scheduler.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'decoder_optimizer': decoder_optimizer.state_dict(),\n",
    "            'decoder_scheduler': decoder_scheduler.state_dict(),\n",
    "            'global_step': global_step,\n",
    "            'args': {key: args.__dict__[key] for key in ['formats', 'input_size', 'coord_bins', 'sep_xy']}\n",
    "        }\n",
    "\n",
    "        for name in ['post_smiles', 'graph_smiles', 'canon_smiles']:\n",
    "            if name in scores:\n",
    "                score = scores[name]\n",
    "                break\n",
    "\n",
    "        if SUMMARY:\n",
    "            SUMMARY.add_scalar('train/loss', avg_loss, global_step)\n",
    "            encoder_lr = encoder_scheduler.get_lr()[0]\n",
    "            decoder_lr = decoder_scheduler.get_lr()[0]\n",
    "            SUMMARY.add_scalar('train/encoder_lr', encoder_lr, global_step)\n",
    "            SUMMARY.add_scalar('train/decoder_lr', decoder_lr, global_step)\n",
    "            for key in scores:\n",
    "                SUMMARY.add_scalar(f'valid/{key}', scores[key], global_step)\n",
    "\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            print_rank_0(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(save_obj, os.path.join(save_path, f'{args.encoder}_{args.decoder}_best.pth'))\n",
    "            with open(os.path.join(save_path, 'best_valid.json'), 'w') as f:\n",
    "                json.dump(scores, f)\n",
    "\n",
    "        if args.save_mode == 'all':\n",
    "            torch.save(save_obj, os.path.join(save_path, f'{args.encoder}_{args.decoder}_ep{epoch}.pth'))\n",
    "        if args.save_mode == 'last':\n",
    "            torch.save(save_obj, os.path.join(save_path, f'{args.encoder}_{args.decoder}_last.pth'))\n",
    "\n",
    "    if args.local_rank != -1:\n",
    "        dist.barrier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e2655-e55a-454d-88c7-b932d5654500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
