{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d4a383-89b1-4e47-bcfd-b7eca736aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/MolScribe\n"
     ]
    }
   ],
   "source": [
    "%cd MolScribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681c9ff6-5678-427f-962a-663c3562e84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "from rdkit import RDLogger,Chem\n",
    "from rdkit.Chem import AllChem,DataStructs\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import argparse\n",
    "from rdkit.DataStructs import TanimotoSimilarity\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from molscribe.dataset import TrainDataset, AuxTrainDataset, bms_collate\n",
    "from molscribe.model import Encoder, Decoder\n",
    "from molscribe.loss import Criterion\n",
    "from molscribe.chemistry import convert_graph_to_smiles, postprocess_smiles, keep_main_molecule\n",
    "from molscribe.tokenizer import get_tokenizer\n",
    "pl.seed_everything(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09dd8a09-135c-44a8-a3f1-0b9c52a56d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babd71d3-a079-47b8-a6f8-561eb5dfd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    wandb=True\n",
    "    ckpt_path='zinc_2-3m.csv'\n",
    "    train_path = 'zinc_6-7m.csv'\n",
    "    aux_path = './uspto_mol/train_680k.csv'\n",
    "    val_df = 'train.csv'\n",
    "    betas=(0.9, 0.999)\n",
    "    img_size = 384\n",
    "    max_pred_len = 128\n",
    "    val_split_size = 0.2\n",
    "    scheduler = None\n",
    "    emb_dim = 512  \n",
    "    attention_dim = 512\n",
    "    freq_threshold = 2\n",
    "    decoder_dim = 512\n",
    "    img_size=512\n",
    "    dropout = 0.4\n",
    "    eps=1e-6\n",
    "    num_workers = 12\n",
    "    batch_size = 64\n",
    "    lr=2e-5\n",
    "    encoder_lr = 2e-5\n",
    "    decoder_lr = 3e-5\n",
    "    weight_decay = 0.01\n",
    "    fine_tune_encoder = False\n",
    "    max_epoches=6\n",
    "    seed=56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4170f49e-f554-43bb-87d4-88c7b93bd947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainARGS:\n",
    "    formats = ['chartok_coords','edges']\n",
    "    input_size = 384\n",
    "    save_image = False\n",
    "    mol_augment = None\n",
    "    default_option = None\n",
    "    shuffle_nodes = False\n",
    "    include_condensed = None\n",
    "    vocab_file = 'vocab_chars.json'\n",
    "    save_path='./saved_images'\n",
    "    coord_bins = 128\n",
    "    dynamic_indigo = True\n",
    "    sep_xy = True\n",
    "    continuous_coords = None\n",
    "    data_path = ''\n",
    "    augment = None\n",
    "    coords_file = None\n",
    "    pseudo_coords = None\n",
    "    predict_coords = None\n",
    "    encoder = 'swin_base'\n",
    "    decoder = 'transformer'\n",
    "    use_checkpoint = False\n",
    "    encoder_dim = 1024\n",
    "    dropout = 0.4\n",
    "    embed_dim = 256\n",
    "    decoder_dim = 512\n",
    "    decoder_layer = 1\n",
    "    attention_dim = 256\n",
    "    dec_num_layers = 6\n",
    "    dec_hidden_size = 256\n",
    "    dec_attn_heads = 8\n",
    "    dec_num_queries = 128\n",
    "    hidden_dropout = 0.1\n",
    "    attn_dropout = 0.1\n",
    "    max_relative_positions = 0\n",
    "    save_path = './saved_images'\n",
    "    mask_ratio = 0.0\n",
    "    label_smoothing = 0.0\n",
    "    compute_confidence = None\n",
    "    enc_pos_emb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c7c75bd-a17b-4da6-a64b-b7688a7af7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(args_states=None):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        # Model\n",
    "        parser.add_argument('--encoder', type=str, default='swin_base')\n",
    "        parser.add_argument('--decoder', type=str, default='transformer')\n",
    "        parser.add_argument('--trunc_encoder', action='store_true')  # use the hidden states before downsample\n",
    "        parser.add_argument('--no_pretrained', action='store_true')\n",
    "        parser.add_argument('--use_checkpoint', action='store_true', default=True)\n",
    "        parser.add_argument('--dropout', type=float, default=0.5)\n",
    "        parser.add_argument('--embed_dim', type=int, default=256)\n",
    "        parser.add_argument('--enc_pos_emb', action='store_true')\n",
    "        group = parser.add_argument_group(\"transformer_options\")\n",
    "        group.add_argument(\"--dec_num_layers\", help=\"No. of layers in transformer decoder\", type=int, default=6)\n",
    "        group.add_argument(\"--dec_hidden_size\", help=\"Decoder hidden size\", type=int, default=256)\n",
    "        group.add_argument(\"--dec_attn_heads\", help=\"Decoder no. of attention heads\", type=int, default=8)\n",
    "        group.add_argument(\"--dec_num_queries\", type=int, default=128)\n",
    "        group.add_argument(\"--hidden_dropout\", help=\"Hidden dropout\", type=float, default=0.1)\n",
    "        group.add_argument(\"--attn_dropout\", help=\"Attention dropout\", type=float, default=0.1)\n",
    "        group.add_argument(\"--max_relative_positions\", help=\"Max relative positions\", type=int, default=0)\n",
    "        parser.add_argument('--continuous_coords', action='store_true')\n",
    "        parser.add_argument('--compute_confidence', action='store_true')\n",
    "        # Data\n",
    "        parser.add_argument('--input_size', type=int, default=384)\n",
    "        parser.add_argument('--vocab_file', type=str, default=None)\n",
    "        parser.add_argument('--coord_bins', type=int, default=64)\n",
    "        parser.add_argument('--sep_xy', action='store_true', default=True)\n",
    "\n",
    "        args = parser.parse_args([])\n",
    "        if args_states:\n",
    "            for key, value in args_states.items():\n",
    "                args.__dict__[key] = value\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e12ea7a-9347-496e-b624-aa81a150f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(Dataset):\n",
    "    def __init__(self,df,transforms,img_dir=\"./train/\"):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        smiles = self.df.iloc[idx]['smiles']\n",
    "        path = self.img_dir + str(self.df.iloc[idx]['id']) +'.png'\n",
    "        img = self._read_image(path)\n",
    "        img = self.transforms(image=img,keypoints=[])['image']\n",
    "        return img,smiles\n",
    "    \n",
    "    def _read_image(self,path):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3ba7e26-99a0-4986-a532-bce09ac71261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,tokenizer):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG()\n",
    "        self.is_setup = False\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.train_df = pd.read_csv(CFG.train_path).reset_index(drop=True)\n",
    "        self.train_df['SMILES'] = self.train_df['smiles']\n",
    "        self.aux_df = pd.read_csv(CFG.aux_path)[:300_000].reset_index(drop=True)\n",
    "        self.val_df = pd.read_csv(CFG.val_df)\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        #self.train_df, self.val_df = train_test_split(self.train_data, test_size=self.cfg.val_split_size,random_state=self.cfg.seed)\n",
    "        #self.train_df = self.train_df.reset_index(drop=True)\n",
    "        #self.val_df = self.val_df.reset_index(drop=True)\n",
    "        self.train_dataset = AuxTrainDataset(TrainARGS,self.train_df,self.aux_df,self.tokenizer)\n",
    "        self.val_dataset = ValDataset(self.val_df,self.train_dataset.train_dataset.transform)\n",
    "        self.is_setup = True\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          collate_fn=bms_collate,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.cfg.batch_size,\n",
    "                          num_workers=self.cfg.num_workers,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e24cd843-c028-464c-a346-4685c3de70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_accuracy(y_p,y):\n",
    "    y_p,y = list(y_p),list(y)\n",
    "    ln = min(len(y_p),len(y))\n",
    "    score = 0\n",
    "    for i in range(ln):\n",
    "        if y_p[i] == y[i]:\n",
    "            score += 1\n",
    "    return score / max(len(y_p),len(y))\n",
    "\n",
    "def accuracy(y_p,y):\n",
    "    if y_p == y:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def correct_part(y_p):\n",
    "    if Chem.MolFromSmiles(y_p) is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def tanimoto(y_p,y):\n",
    "    try:\n",
    "        mol1 = Chem.MolFromSmiles(y_p)\n",
    "        mol2 = Chem.MolFromSmiles(y)\n",
    "    \n",
    "        vec_1 = AllChem.RDKFingerprint(mol1)\n",
    "        vec_2 = AllChem.RDKFingerprint(mol2)\n",
    "        return DataStructs.TanimotoSimilarity(vec_1,vec_2)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb00dc5c-63d0-47b7-a555-9da45aaf7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "\n",
    "    def update(self,preds,targets):\n",
    "        self.preds += preds\n",
    "        self.targets += targets\n",
    "    \n",
    "    def calc_metrics(self):\n",
    "        f = dict()\n",
    "        \n",
    "        f['char_acc'] = np.mean([char_accuracy(x,y) for x,y in zip(self.preds,self.targets)])\n",
    "        f['acc'] = np.mean([accuracy(x,y) for x,y in zip(self.preds,self.targets)])\n",
    "        f['corrent_part'] = np.mean([correct_part(x) for x in self.preds])\n",
    "        f['tanimoto'] = np.mean([tanimoto(x,y) for x,y in zip(self.preds,self.targets)])\n",
    "        \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42037834-0ab7-4118-8e40-fe4999c3b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "\n",
    "    def update(self,preds,targets):\n",
    "        self.preds += preds\n",
    "        self.targets += targets\n",
    "    \n",
    "    def calc_metrics(self):\n",
    "        f = dict()\n",
    "        \n",
    "        f['char_acc'] = np.mean([char_accuracy(x,y) for x,y in zip(self.preds,self.targets)])\n",
    "        f['acc'] = np.mean([accuracy(x,y) for x,y in zip(self.preds,self.targets)])\n",
    "        f['corrent_part'] = np.mean([correct_part(x) for x in self.preds])\n",
    "        f['tanimoto'] = np.mean([tanimoto(x,y) for x,y in zip(self.preds,self.targets)])\n",
    "        \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e520764-0f7a-4329-85c8-76410bacc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLModule(pl.LightningModule):\n",
    "    def __init__(self,encoder,decoder,tokenizer):\n",
    "        super().__init__()\n",
    "        self.cfg = CFG()\n",
    "        self.avg_meter = AverageMeter()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.criterion = Criterion(TrainARGS,tokenizer)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def forward(self,images,refs):\n",
    "        features, hiddens = encoder(images, refs)\n",
    "        results = decoder(features, hiddens, refs)\n",
    "        return sum(self.criterion(results, refs).values())\n",
    "   \n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        _,images,refs = batch\n",
    "        loss = self(images,refs)\n",
    "        self.log_dict({'train_loss':loss.item()})\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def fvalidation_step(self, batch, _):\n",
    "        image,labels = batch\n",
    "        \n",
    "        features, hiddens = self.encoder(image)\n",
    "        preds = self.decoder.decode(features, hiddens)\n",
    "        smiles = [pred['chartok_coords']['smiles'] for pred in preds]\n",
    "        node_coords = [pred['chartok_coords']['coords'] for pred in preds]\n",
    "        node_symbols = [pred['chartok_coords']['symbols'] for pred in preds]\n",
    "        edges = [pred['edges'] for pred in preds]#\n",
    "    \n",
    "        smiles_list, molblock_list, r_success = convert_graph_to_smiles(\n",
    "            node_coords, node_symbols, edges, images=image.cpu().detach().numpy())\n",
    "        \n",
    "        self.avg_meter.update(smiles_list,labels)\n",
    "    \n",
    "    def predict_step(self,batch,_):\n",
    "        \n",
    "        logits = self.model.generate(\n",
    "            batch['input_ids'],\n",
    "            num_beams=1,\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        logits = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in logits]\n",
    "        return logits\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        f = self.avg_meter.calc_metrics()\n",
    "        self.log_dict(f)\n",
    "        print(f)\n",
    "        self.avg_meter.reset()\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.encoder.named_parameters()],\n",
    "                \"lr\":self.cfg.lr\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.decoder.named_parameters()],\n",
    "                \"lr\":self.cfg.lr\n",
    "            },\n",
    "        ]\n",
    "        return torch.optim.AdamW(optimizer_grouped_parameters,\n",
    "                                 betas=self.cfg.betas,\n",
    "                                 weight_decay=self.cfg.weight_decay,\n",
    "                                 eps=self.cfg.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80775f1-60cc-4f29-a5ed-02c6add3ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(TrainARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d576176-379c-4562-8809-74eedc5a1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = PLDataModule(tokenizer)\n",
    "dm.prepare_data()\n",
    "dm.setup(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a59636-967f-4db9-b059-029a510f38e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6a79b844304b0baa958c0f5ceebb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "swin_base_char_aux_1m.pth:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_path = hf_hub_download('yujieq/MolScribe', 'swin_base_char_aux_1m.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680a3604-ae13-469d-b8c6-53367f2075ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chekpoint = torch.load('kagglev2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aa78c62-65ba-4e2c-b5a2-ee3feb092f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(chekpoint['args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3da32cd4-7e93-48f5-87da-46b0f31c9788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(args)\n",
    "args.encoder_dim = encoder.n_features\n",
    "decoder = Decoder(args,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b329b1d0-457d-493f-8649-b743a5ab9f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(args)\n",
    "args.encoder_dim = encoder.n_features\n",
    "decoder = Decoder(args,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6076701-2745-417b-93e6-088ffeb44520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load(module, module_states):\n",
    "    def remove_prefix(state_dict):\n",
    "        return {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    module.load_state_dict(remove_prefix(module_states))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39f350ef-f6a8-4793-bc6d-8bf276067a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "safe_load(encoder, chekpoint['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e324184-9f8c-4089-8892-469e77098e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_load(decoder, chekpoint['decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e83887-5ab1-40d3-8975-8843e9eec270",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pl = PLModule(encoder,decoder,tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aa242a6-0d36-4aa1-a478-c0f4cac8cefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey20007\u001b[0m (\u001b[33mandrey2007\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240120_153621-he4nh6h3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/andrey2007/MOLECULA/runs/he4nh6h3\" target=\"_blank\">Molscribe_Small_ZinC_5-6m</a></strong> to <a href=\"https://wandb.ai/andrey2007/MOLECULA\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/andrey2007/MOLECULA/runs/he4nh6h3?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f466c0d3ca0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"673ae6e9b51cc896110db5327738b993795fffad\")\n",
    "os.environ['WANDB_API_KEY'] = \"673ae6e9b51cc896110db5327738b993795fffad\"\n",
    "wandb.init(project='MOLECULA',name='Molscribe_Small_ZinC_5-6m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47884cfd-bdf3-4378-8a0a-4cf34ce620f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='./outputs/',\n",
    "    filename='base_model{epoch:02d}',\n",
    "    monitor='tanimoto',\n",
    "    mode='max',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    precision=32,\n",
    "    callbacks = [lr_monitor,checkpoint_cb],\n",
    "    logger = pl.loggers.WandbLogger(),\n",
    "    min_epochs=1,\n",
    "    devices=[0],\n",
    "    check_val_every_n_epoch=1,\n",
    "    max_epochs=CFG.max_epoches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96135f1b-df3c-4bad-ae92-d732a7139927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /notebooks/outputs exists and is not empty.\n",
      "Restoring states from the checkpoint path at outputs/last-v5.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | encoder   | Encoder   | 86.9 M\n",
      "1 | decoder   | Decoder   | 6.9 M \n",
      "2 | criterion | Criterion | 0     \n",
      "----------------------------------------\n",
      "93.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.8 M    Total params\n",
      "375.114   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at outputs/last-v5.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276717fe151b4f6bb591c4b0500faf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model_pl,datamodule=dm,ckpt_path=\"outputs/last-v5.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98636baa-357a-41e1-8e62-e94046f677dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pl.state_dict(),'molsc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d821b9e-ce96-40c0-a91d-3748d37f9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = dict()\n",
    "state_dict['encoder'] = encoder.state_dict()\n",
    "state_dict['decoder'] = decoder.state_dict()\n",
    "state_dict['args'] = chekpoint['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "173fddd6-a7cd-47dd-b398-6e9257ef096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict,'kagglev2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e66f4b-c824-490b-a2a3-858ae396fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pl.state_dict(),'create.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ba4bfe3-5d47-4c42-9276-59025376e29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pl.load_state_dict(torch.load('create.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e4019dc-927b-4b65-8a20-f468b954b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan  7 21:44:44 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   32C    P0    80W / 400W |   3409MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ac2737b-092e-4193-b318-e29bc249a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Received SIGTERM: 15\n"
     ]
    }
   ],
   "source": [
    "trainer.save_checkpoint('chep.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14ffbe-7365-4c55-b331-504414f520a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Trained\n",
    "500_000 - 900_000 ChEMBL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
