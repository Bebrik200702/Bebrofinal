{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers -U","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:20:35.775860Z","iopub.execute_input":"2024-01-08T05:20:35.776216Z","iopub.status.idle":"2024-01-08T05:20:58.561867Z","shell.execute_reply.started":"2024-01-08T05:20:35.776187Z","shell.execute_reply":"2024-01-08T05:20:58.560552Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nCollecting transformers\n  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.0\n    Uninstalling transformers-4.36.0:\n      Successfully uninstalled transformers-4.36.0\nSuccessfully installed transformers-4.36.2\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!wget https://storage.yandexcloud.net/ds-ods/files/content/2023/12/21/03e203ba/train_data.zip","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:20:58.564960Z","iopub.execute_input":"2024-01-08T05:20:58.565349Z","iopub.status.idle":"2024-01-08T05:21:55.459349Z","shell.execute_reply.started":"2024-01-08T05:20:58.565315Z","shell.execute_reply":"2024-01-08T05:21:55.458134Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%capture\n!unzip /kaggle/working/train_data.zip\n!pip install rdkit","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:21:55.460907Z","iopub.execute_input":"2024-01-08T05:21:55.461270Z","iopub.status.idle":"2024-01-08T05:22:27.821639Z","shell.execute_reply.started":"2024-01-08T05:21:55.461238Z","shell.execute_reply":"2024-01-08T05:22:27.820300Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:22:27.825190Z","iopub.execute_input":"2024-01-08T05:22:27.825943Z","iopub.status.idle":"2024-01-08T05:22:42.278547Z","shell.execute_reply.started":"2024-01-08T05:22:27.825901Z","shell.execute_reply":"2024-01-08T05:22:42.277565Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting einops\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m121.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport timm\nimport torch\nfrom torch import nn\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.backends.cudnn as cudnn\nimport wandb\nimport pytorch_lightning as pl\nfrom torch.nn.utils.rnn import pad_sequence,pack_padded_sequence\nfrom transformers import (AutoProcessor,\n                          AutoTokenizer,\n                          VisionEncoderDecoderModel,\n                          RobertaTokenizerFast,\n                          TrOCRForCausalLM,\n                          AutoModel,\n                          TrOCRConfig,\n                          ViTImageProcessor,\n                          Swinv2Model,\n                          Swinv2Config,\n                          GPT2TokenizerFast\n                         )\nfrom sklearn.model_selection import train_test_split\nfrom rdkit import RDLogger,Chem\nfrom rdkit.Chem import AllChem,DataStructs\nimport torch.nn.functional as F\nimport os\nfrom rdkit.DataStructs import TanimotoSimilarity\nfrom Levenshtein import distance as levenshtein_distance\npl.seed_everything(56)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:22:42.279844Z","iopub.execute_input":"2024-01-08T05:22:42.280150Z","iopub.status.idle":"2024-01-08T05:23:00.532943Z","shell.execute_reply.started":"2024-01-08T05:22:42.280123Z","shell.execute_reply":"2024-01-08T05:23:00.532061Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"56"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import ViTConfig,VisionEncoderDecoderModel,AutoModelWithLMHead,AutoTokenizer\nfrom transformers.modeling_outputs import BaseModelOutput\n\nimport timm\nfrom einops import rearrange","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.534374Z","iopub.execute_input":"2024-01-08T05:23:00.535136Z","iopub.status.idle":"2024-01-08T05:23:00.544755Z","shell.execute_reply.started":"2024-01-08T05:23:00.535082Z","shell.execute_reply":"2024-01-08T05:23:00.543812Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"RDLogger.DisableLog('rdApp.*')\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.545978Z","iopub.execute_input":"2024-01-08T05:23:00.546384Z","iopub.status.idle":"2024-01-08T05:23:00.667320Z","shell.execute_reply.started":"2024-01-08T05:23:00.546348Z","shell.execute_reply":"2024-01-08T05:23:00.666373Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    wandb=False\n    encoder=\"google/vit-base-patch16-384\"\n    decoder=\"microsoft/resnet50\"\n    train_path = './train.csv'\n    train_folder = './train/'\n    betas=(0.9, 0.999)\n    img_size = 512\n    max_pred_len = 128\n    val_split_size = 0.2\n    scheduler = None\n    emb_dim = 512  \n    attention_dim = 512\n    freq_threshold = 2\n    decoder_dim = 512\n    img_size=512\n    dropout = 0.4\n    eps=1e-6\n    num_workers = 2\n    batch_size = 8\n    encoder_lr = 1e-4 \n    decoder_lr = 2e-4\n    weight_decay = 0.01\n    fine_tune_encoder = False\n    max_epoches=6\n    seed=56","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.668479Z","iopub.execute_input":"2024-01-08T05:23:00.668761Z","iopub.status.idle":"2024-01-08T05:23:00.679441Z","shell.execute_reply.started":"2024-01-08T05:23:00.668737Z","shell.execute_reply":"2024-01-08T05:23:00.678540Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class PLDataset(Dataset):\n    def __init__(self, df, tokenizer,processor):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.processor = processor\n        self.photo_dir = CFG.train_folder\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.photo_dir+str(self.df.iloc[idx]['id'])+'.png').convert('RGB')\n        label = self.df.iloc[idx]['smiles']\n        image = self.processor(image,return_tensors='pt').pixel_values\n        label_enc = self.tokenizer.encode_plus(label, padding='max_length',max_length=128, truncation=True, return_tensors='pt')\n        return {'image':image.squeeze(0),\n                'input_ids':label_enc.input_ids.squeeze(0),\n                'attention_mask':label_enc.attention_mask.squeeze(0)}\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.680556Z","iopub.execute_input":"2024-01-08T05:23:00.680822Z","iopub.status.idle":"2024-01-08T05:23:00.693873Z","shell.execute_reply.started":"2024-01-08T05:23:00.680800Z","shell.execute_reply":"2024-01-08T05:23:00.693157Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class PLDataModule(pl.LightningDataModule):\n    def __init__(self,tokenizer,processor):\n        super().__init__()\n        self.cfg = CFG()\n        self.is_setup = False\n        self.tokenizer = tokenizer\n        self.processor = processor\n        \n    def prepare_data(self):\n        self.train_data = pd.read_csv(CFG.train_path)\n        \n    def setup(self, stage: str):\n        self.train_df, self.val_df = train_test_split(self.train_data, test_size=self.cfg.val_split_size,random_state=self.cfg.seed)\n        self.train_df = self.train_df.reset_index(drop=True)\n        self.val_df = self.val_df.reset_index(drop=True)\n        self.train_dataset = PLDataset(self.train_df,self.tokenizer,self.processor)\n        self.val_dataset = PLDataset(self.val_df,self.tokenizer,self.processor)\n        self.is_setup = True\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,\n                          batch_size=self.cfg.batch_size,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True,\n                          shuffle=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,\n                          batch_size=self.cfg.batch_size,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True,\n                          shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.697993Z","iopub.execute_input":"2024-01-08T05:23:00.698845Z","iopub.status.idle":"2024-01-08T05:23:00.708244Z","shell.execute_reply.started":"2024-01-08T05:23:00.698809Z","shell.execute_reply":"2024-01-08T05:23:00.707394Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def char_accuracy(y_p,y):\n    y_p,y = list(y_p),list(y)\n    ln = min(len(y_p),len(y))\n    score = 0\n    for i in range(ln):\n        if y_p[i] == y[i]:\n            score += 1\n    return score / max(len(y_p),len(y))\n\ndef correct_part(y_p):\n    if Chem.MolFromSmiles(y_p) is None:\n        return 0\n    else:\n        return 1\n\ndef tanimoto(y_p,y):\n    try:\n        mol1 = Chem.MolFromSmiles(y_p)\n        mol2 = Chem.MolFromSmiles(y)\n    \n        vec_1 = AllChem.RDKFingerprint(mol1)\n        vec_2 = AllChem.RDKFingerprint(mol2)\n        return DataStructs.TanimotoSimilarity(vec_1,vec_2)\n    except:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.709499Z","iopub.execute_input":"2024-01-08T05:23:00.709765Z","iopub.status.idle":"2024-01-08T05:23:00.722861Z","shell.execute_reply.started":"2024-01-08T05:23:00.709741Z","shell.execute_reply":"2024-01-08T05:23:00.721951Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.preds = []\n        self.targets = []\n\n    def update(self,preds,targets):\n        self.preds += preds\n        self.targets += targets\n    \n    def calc_metrics(self):\n        f = dict()\n        \n        f['char_acc'] = np.mean([char_accuracy(x,y) for x,y in zip(self.preds,self.targets)])\n        f['corrent_part'] = np.mean([correct_part(x) for x in self.preds])\n        f['tanimoto'] = np.mean([tanimoto(x,y) for x,y in zip(self.preds,self.targets)])\n        \n        return f","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.723987Z","iopub.execute_input":"2024-01-08T05:23:00.724319Z","iopub.status.idle":"2024-01-08T05:23:00.734590Z","shell.execute_reply.started":"2024-01-08T05:23:00.724293Z","shell.execute_reply":"2024-01-08T05:23:00.733759Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class PLModule(pl.LightningModule):\n    def __init__(self,model,tokenizer):\n        super().__init__()\n        self.cfg = CFG()\n        self.avg_meter = AverageMeter()\n        self.model = model\n        self.tokenizer = tokenizer\n        \n    def forward(self,image,input_ids=None,attention_mask=None):\n        return self.model(pixel_values=image,labels=input_ids,decoder_attention_mask=attention_mask)   \n\n    def training_step(self, batch, _):\n        loss = self(**batch).loss\n        self.log_dict({'train_loss':loss.item()})\n        return loss\n        \n    \n    def predict_step(self,batch,_):\n        \n        logits = self.model.generate(\n            batch['image'],\n            num_beams=1,\n            max_length=128\n        )\n        \n        logits = logits.detach().cpu().numpy()\n        logits = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in logits]\n        return logits\n    \n    def on_validation_epoch_end(self):\n        f = self.avg_meter.calc_metrics()\n        self.log_dict(f)\n        print(f)\n        self.avg_meter.reset()\n            \n    def configure_optimizers(self):\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.encoder.named_parameters()],\n                \"lr\":self.cfg.encoder_lr\n            },\n            {\n                \"params\": [p for n, p in self.model.decoder.named_parameters()],\n                \"lr\": self.cfg.decoder_lr\n            },\n        ]\n        return torch.optim.AdamW(optimizer_grouped_parameters,\n                                 betas=self.cfg.betas,\n                                 weight_decay=self.cfg.weight_decay,\n                                 eps=self.cfg.eps)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:00.735769Z","iopub.execute_input":"2024-01-08T05:23:00.736050Z","iopub.status.idle":"2024-01-08T05:23:00.748997Z","shell.execute_reply.started":"2024-01-08T05:23:00.736026Z","shell.execute_reply":"2024-01-08T05:23:00.748274Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('entropy/roberta_zinc_480m')\nprocessor = AutoProcessor.from_pretrained(CFG.encoder)\n#processor = ViTImageProcessor.from_pretrained('proc_swin')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-08T05:23:00.750245Z","iopub.execute_input":"2024-01-08T05:23:00.750527Z","iopub.status.idle":"2024-01-08T05:23:03.107204Z","shell.execute_reply.started":"2024-01-08T05:23:00.750503Z","shell.execute_reply":"2024-01-08T05:23:03.106170Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01f17971764f4afdaa8ea7f10cdf2d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/40.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1b196e12d34c739c95f322e2f8ab1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/24.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5f5fd208fe44434950467316dfb41e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2702e89b79034ae8b917bdb3522fda1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3340b99f74d04990be55182aba49b3fa"}},"metadata":{}}]},{"cell_type":"code","source":"processor.size = {\"height\": 384,\"width\": 384}","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:03.108921Z","iopub.execute_input":"2024-01-08T05:23:03.109281Z","iopub.status.idle":"2024-01-08T05:23:03.114033Z","shell.execute_reply.started":"2024-01-08T05:23:03.109252Z","shell.execute_reply":"2024-01-08T05:23:03.112748Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class TimmFeatureEncoder(nn.Module):\n  def __init__(self,backbone='resnet50',hidden_size=2048,pool_dims=(8,8)):\n    super().__init__()\n    self.main_input_name = backbone\n    self.model = timm.create_model(backbone,pretrained=True)\n    self.config = ViTConfig(hidden_size=hidden_size)\n    self.adaptive_pool = nn.AdaptiveAvgPool2d(pool_dims)\n\n  def forward(self,pixel_values,**kwargs):\n    features = self.model.forward_features(pixel_values)\n    features = self.adaptive_pool(features)\n    features = rearrange(features,'b x c h -> b (c h) x')\n    return BaseModelOutput(last_hidden_state=features,\n                           hidden_states=None,\n                           attentions=None)\n\n  def get_output_embeddings(self,):\n    return None\n\nencoder = TimmFeatureEncoder(backbone='maxvit_tiny_tf_384.in1k',\n                         hidden_size=512)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:03.115689Z","iopub.execute_input":"2024-01-08T05:23:03.116073Z","iopub.status.idle":"2024-01-08T05:23:04.998254Z","shell.execute_reply.started":"2024-01-08T05:23:03.116040Z","shell.execute_reply":"2024-01-08T05:23:04.997417Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/124M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e5c278b169474d9ce3bb58b1f28532"}},"metadata":{}}]},{"cell_type":"code","source":"decoder = AutoModelWithLMHead.from_pretrained(\"entropy/roberta_zinc_decoder\")","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:04.999751Z","iopub.execute_input":"2024-01-08T05:23:05.000053Z","iopub.status.idle":"2024-01-08T05:23:06.931054Z","shell.execute_reply.started":"2024-01-08T05:23:05.000028Z","shell.execute_reply":"2024-01-08T05:23:06.930184Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1564: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e25e4dc1aa4c08804048e7cec2e008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/237M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca16ddfe41d49018a34bf314573a9d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33c8b2d2151240c5a85452c47ab3a2c2"}},"metadata":{}}]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel(encoder=encoder,decoder=decoder)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-08T05:23:06.932484Z","iopub.execute_input":"2024-01-08T05:23:06.933169Z","iopub.status.idle":"2024-01-08T05:23:06.947641Z","shell.execute_reply.started":"2024-01-08T05:23:06.933108Z","shell.execute_reply":"2024-01-08T05:23:06.946786Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:06.948939Z","iopub.execute_input":"2024-01-08T05:23:06.949267Z","iopub.status.idle":"2024-01-08T05:23:06.959788Z","shell.execute_reply.started":"2024-01-08T05:23:06.949241Z","shell.execute_reply":"2024-01-08T05:23:06.958816Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dm = PLDataModule(tokenizer,processor)\ndm.prepare_data()\ndm.setup(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:06.961163Z","iopub.execute_input":"2024-01-08T05:23:06.961531Z","iopub.status.idle":"2024-01-08T05:23:07.090458Z","shell.execute_reply.started":"2024-01-08T05:23:06.961500Z","shell.execute_reply":"2024-01-08T05:23:07.089684Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_pl = PLModule(model,tokenizer).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:07.091679Z","iopub.execute_input":"2024-01-08T05:23:07.092040Z","iopub.status.idle":"2024-01-08T05:23:07.430791Z","shell.execute_reply.started":"2024-01-08T05:23:07.092008Z","shell.execute_reply":"2024-01-08T05:23:07.429991Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"673ae6e9b51cc896110db5327738b993795fffad\")\nos.environ['WANDB_API_KEY'] = \"673ae6e9b51cc896110db5327738b993795fffad\"\nwandb.init(project='MOLECULA',name='Maxvit_GPT')","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:07.431983Z","iopub.execute_input":"2024-01-08T05:23:07.432365Z","iopub.status.idle":"2024-01-08T05:23:40.232983Z","shell.execute_reply.started":"2024-01-08T05:23:07.432331Z","shell.execute_reply":"2024-01-08T05:23:40.232076Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey20007\u001b[0m (\u001b[33mandrey2007\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240108_052309-8px9cuvf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/andrey2007/MOLECULA/runs/8px9cuvf' target=\"_blank\">Maxvit_GPT</a></strong> to <a href='https://wandb.ai/andrey2007/MOLECULA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/andrey2007/MOLECULA' target=\"_blank\">https://wandb.ai/andrey2007/MOLECULA</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/andrey2007/MOLECULA/runs/8px9cuvf' target=\"_blank\">https://wandb.ai/andrey2007/MOLECULA/runs/8px9cuvf</a>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/andrey2007/MOLECULA/runs/8px9cuvf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7d1914a28910>"},"metadata":{}}]},{"cell_type":"code","source":"lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\ncheckpoint_cb = pl.callbacks.ModelCheckpoint(\n    dirpath='./outputs/',\n    filename='base_model{epoch:02d}',\n    monitor='tanimoto',\n    mode='max',\n    save_last=True\n)\n\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    precision=32,\n    callbacks = [lr_monitor,checkpoint_cb],\n    logger = pl.loggers.WandbLogger(),\n    min_epochs=1,\n    devices=[0],\n    check_val_every_n_epoch=1,\n    max_epochs=CFG.max_epoches\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:40.234424Z","iopub.execute_input":"2024-01-08T05:23:40.234785Z","iopub.status.idle":"2024-01-08T05:23:40.946479Z","shell.execute_reply.started":"2024-01-08T05:23:40.234751Z","shell.execute_reply":"2024-01-08T05:23:40.945557Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model_pl,datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T05:23:40.947896Z","iopub.execute_input":"2024-01-08T05:23:40.948373Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b8dc04f9c34ffb82fcbac04bdafe2f"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}