{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf3c266-ef71-4876-bdf9-66ef2739c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DefaultDataCollator\n",
    "from PIL import Image\n",
    "import requests\n",
    "from evaluate import load\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.models.vision_encoder_decoder.modeling_vision_encoder_decoder import shift_tokens_right\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from rdkit import Chem, RDLogger\n",
    "import os\n",
    "from rdkit.Chem import Draw\n",
    "from io import BytesIO\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "wandb.login(key=\"673ae6e9b51cc896110db5327738b993795fffad\")\n",
    "os.environ['WANDB_API_KEY'] = \"673ae6e9b51cc896110db5327738b993795fffad\"\n",
    "wandb.init(project='DoHACK',name='DONUT')\n",
    "cer = load('cer')\n",
    "\n",
    "def set_seed(seed: int = 56) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "set_seed()\n",
    "\n",
    "molecula_130m = load_dataset(\"parquet\", data_files=\"PubChem90m_canon.parquet.gzip\")['train']\n",
    "zinc20 = load_dataset('sagawa/ZINC-canonicalized')['train']\n",
    "zinc20 = zinc20.filter(lambda x: Chem.MolFromSmiles(x['smiles']) is not None and len(x['smiles']) < 128, num_proc=12)\n",
    "ds = molecula_130m.train_test_split(0.0002, seed=56)\n",
    "dataset_train = ds['train']\n",
    "dataset_val = load_dataset(\"csv\", data_files=\"new_val.csv\")['train'] #ds['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained('sagawa/PubChem-10m-t5-v2')\n",
    "\n",
    "processor = DonutProcessor.from_pretrained('naver-clova-ix/donut-base')\n",
    "processor.tokenizer= tokenizer\n",
    "processor.image_processor.size = {'height': 384, 'width': 384}#{'height': 512, 'width': 512}\n",
    "\n",
    "class VisionEncoderDecoderSmooth(VisionEncoderDecoderModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values = None,\n",
    "        decoder_input_ids = None,\n",
    "        decoder_attention_mask = None,\n",
    "        encoder_outputs = None,\n",
    "        past_key_values = None,\n",
    "        decoder_inputs_embeds = None,\n",
    "        labels=None,\n",
    "        use_cache = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = True,\n",
    "        return_dict = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n",
    "\n",
    "        kwargs_decoder = {\n",
    "            argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
    "        }\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "            encoder_outputs = self.encoder(\n",
    "                pixel_values,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs_encoder,\n",
    "            )\n",
    "        elif isinstance(encoder_outputs, tuple):\n",
    "            encoder_outputs = BaseModelOutput(*encoder_outputs)\n",
    "\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        # optionally project encoder_hidden_states\n",
    "        if (\n",
    "            self.encoder.config.hidden_size != self.decoder.config.hidden_size\n",
    "            and self.decoder.config.cross_attention_hidden_size is None\n",
    "        ):\n",
    "            encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n",
    "\n",
    "        # else:\n",
    "        encoder_attention_mask = None\n",
    "\n",
    "        if (labels is not None) and (decoder_input_ids is None and decoder_inputs_embeds is None):\n",
    "            decoder_input_ids = shift_tokens_right(\n",
    "                labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            use_cache=use_cache,\n",
    "            past_key_values=past_key_values,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs_decoder,\n",
    "        )\n",
    "\n",
    "        # Compute loss independent from decoder (as some shift the logits inside them)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "            loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.reshape(-1))\n",
    "            \n",
    "            \n",
    "\n",
    "        if not return_dict:\n",
    "            if loss is not None:\n",
    "                return (loss,) + decoder_outputs + encoder_outputs\n",
    "            else:\n",
    "                return decoder_outputs + encoder_outputs\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=decoder_outputs.logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "model = VisionEncoderDecoderSmooth.from_pretrained('naver-clova-ix/donut-base')\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "processor.tokenizer.cls_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.max_length = 256\n",
    "\n",
    "\n",
    "def draw_smiles(smiles):\n",
    "    try:\n",
    "        m = Chem.MolFromSmiles(smiles)\n",
    "        d2d = Draw.MolDraw2DCairo(512,512)\n",
    "        dopts = d2d.drawOptions()\n",
    "        dopts.useBWAtomPalette()\n",
    "        d2d.DrawMolecule(m)\n",
    "        d2d.FinishDrawing()\n",
    "        bio = BytesIO(d2d.GetDrawingText())\n",
    "        return Image.open(bio).convert('RGB')\n",
    "    except:\n",
    "        return draw_smiles('C')\n",
    "\n",
    "MERGE_PROB = 0.15\n",
    "merge_i = 10_300_000\n",
    "ORGANIC_SET = ['B', 'C', 'N', 'O', 'P', 'S', 'F', 'Cl', 'Br', 'I']\n",
    "ELEMENTS = [\n",
    "    \"H\", \"He\", \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\",\n",
    "    \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\", \"K\", \"Ca\",\n",
    "    \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\",\n",
    "    \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\", \"Rb\", \"Sr\", \"Y\", \"Zr\",\n",
    "    \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\",\n",
    "    \"Sb\", \"Te\", \"I\", \"Xe\", \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\",\n",
    "    \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\",\n",
    "    \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\",\n",
    "    \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\", \"Fr\", \"Ra\", \"Ac\", \"Th\",\n",
    "    \"Pa\", \"U\", \"Np\", \"Pu\", \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\",\n",
    "    \"Md\", \"No\", \"Lr\", \"Rf\", \"Db\", \"Sg\", \"Bh\", \"Hs\", \"Mt\", \"Ds\",\n",
    "    \"Rg\", \"Cn\", \"Nh\", \"Fl\", \"Mc\", \"Lv\", \"Ts\", \"Og\"\n",
    "]\n",
    "\n",
    "ATOMS = ORGANIC_SET + [f'[{e}]' for e in ELEMENTS]\n",
    "len_merge = len(zinc20)\n",
    "\n",
    "def aug_smiles(smiles):\n",
    "    global merge_i\n",
    "    \n",
    "    if random.random() < MERGE_PROB:\n",
    "        mode = random.choice(['long', 'short'])\n",
    "        if mode == 'long':\n",
    "            add_smiles_idx = merge_i % len_merge\n",
    "            merge_i += 1\n",
    "            smileses = [smiles, zinc20[add_smiles_idx]['smiles']]\n",
    "            smileses.sort(key=len)\n",
    "            smiles = '.'.join(smileses)\n",
    "        else:\n",
    "            count = random.randint(1, 3)\n",
    "            add_atoms = np.random.choice(ATOMS, count)\n",
    "            smileses = [smiles]\n",
    "            smileses.extend(list(add_atoms))\n",
    "            smileses.sort(key=len)\n",
    "            smiles = '.'.join(smileses)\n",
    "    #smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), kekuleSmiles=True)\n",
    "    return smiles\n",
    "\n",
    "def prepare_features(examples):\n",
    "    smileses = [aug_smiles(s) for s in examples['smiles']]\n",
    "    images = [draw_smiles(s) for s in smileses]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    target_encoding = processor.tokenizer(\n",
    "        [f'{i}' for i in smileses],\n",
    "        padding=\"longest\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    tokenized_examples = {'pixel_values': pixel_values}\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    tokenized_examples['labels'] = labels\n",
    "    return tokenized_examples\n",
    "\n",
    "def prepare_features_val(examples):\n",
    "    smileses = [s for s in examples['smiles']]\n",
    "    images = [draw_smiles(s) for s in smileses]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    target_encoding = processor.tokenizer(\n",
    "        [f'{i}' for i in smileses],\n",
    "        padding=\"longest\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    tokenized_examples = {'pixel_values': pixel_values}\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    tokenized_examples['labels'] = labels\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "tokenized_dataset_train = dataset_train.with_transform(prepare_features)\n",
    "tokenized_dataset_val = dataset_val.with_transform(prepare_features_val)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    'donut_modelv2',\n",
    "    remove_unused_columns=False,\n",
    "    save_safetensors = False,\n",
    "    evaluation_strategy = 'steps',\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    learning_rate = 5e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps = 5,\n",
    "    save_strategy = 'steps',\n",
    "    eval_steps = 750,\n",
    "    save_steps=750,\n",
    "    report_to = 'wandb',\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=12,\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    predict_with_generate = True,\n",
    "    save_total_limit = 3,\n",
    "    optim='adamw_torch',\n",
    "    adam_beta2 = 0.98,\n",
    ")\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    labels, predictions = preds.label_ids, preds.predictions\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    print(labels[-1], predictions[-1])\n",
    "    y_true = [x.strip() for x in labels]\n",
    "    y_pred = [x.strip() for x in predictions]\n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    return {f'cer': cer.compute(predictions=y_pred, references=y_true),'accuracy':accuracy}\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset = tokenized_dataset_train,\n",
    "    eval_dataset = tokenized_dataset_val,\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    data_collator=DefaultDataCollator()\n",
    ")\n",
    "\n",
    "trainer.train(\"donut_modelv2/checkpoint-174000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9abc76-4f89-4284-bf98-654cff141315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#dump = json.load(open('donut_modelv2/checkpoint-16000/trainer_state.json'))\n",
    "#dump['global_step'] = 16500\n",
    "#\n",
    "#with open('donut_modelv2/checkpoint-16000/trainer_state.json', 'w') as f:\n",
    "#    json.dump(dump, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6aa03a-6b8d-41ca-b9ba-179e42c384ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey20007\u001b[0m (\u001b[33mandrey2007\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u001b[34m\u001b[4mhttps://wandb.me/wandb-init\u001b[0m.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/notebooks/wandb/run-20240314_151946-fsrrjboo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDONUT\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/andrey2007/DoHACK\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/andrey2007/DoHACK/runs/fsrrjboo\u001b[0m\n",
      "Random seed set as 56\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "{'loss': 0.4552, 'learning_rate': 0.00020675019219961355, 'epoch': 0.56}        \n",
      "{'loss': 0.4549, 'learning_rate': 0.00020673777546758877, 'epoch': 0.56}        \n",
      " 56%|██████████████████▍              | 174012/311496 [00:43<01:30, 1526.12it/s]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --mixed_precision=fp16 ./run.py\n",
    "# Отруби ZINC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca143b65-662e-409d-b40f-b0d09252d53a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
