{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf3c266-ef71-4876-bdf9-66ef2739c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.py\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer, DefaultDataCollator\n",
    "from PIL import Image\n",
    "import requests\n",
    "from evaluate import load\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers.models.vision_encoder_decoder.modeling_vision_encoder_decoder import shift_tokens_right\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from rdkit import Chem, RDLogger\n",
    "import os\n",
    "from rdkit.Chem import Draw\n",
    "from io import BytesIO\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "wandb.login(key=\"673ae6e9b51cc896110db5327738b993795fffad\")\n",
    "os.environ['WANDB_API_KEY'] = \"673ae6e9b51cc896110db5327738b993795fffad\"\n",
    "wandb.init(project='DoHACK',name='TrOCR_Small')\n",
    "cer = load('cer')\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "set_seed()\n",
    "\n",
    "molecula_130m = load_dataset(\"parquet\", data_files=\"train_90m_ZINK_uniq.parquet.gzip\")['train']\n",
    "ds = molecula_130m.train_test_split(0.0001, seed=42)\n",
    "dataset_train = ds['train']\n",
    "dataset_val = ds['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained('sagawa/PubChem-10m-t5-v2')\n",
    "\n",
    "processor = DonutProcessor.from_pretrained('naver-clova-ix/donut-base')\n",
    "processor.tokenizer= tokenizer\n",
    "processor.image_processor.size = {'height': 384, 'width': 384}#{'height': 512, 'width': 512}\n",
    "\n",
    "class VisionEncoderDecoderSmooth(VisionEncoderDecoderModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values = None,\n",
    "        decoder_input_ids = None,\n",
    "        decoder_attention_mask = None,\n",
    "        encoder_outputs = None,\n",
    "        past_key_values = None,\n",
    "        decoder_inputs_embeds = None,\n",
    "        labels=None,\n",
    "        use_cache = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = True,\n",
    "        return_dict = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n",
    "\n",
    "        kwargs_decoder = {\n",
    "            argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
    "        }\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            if pixel_values is None:\n",
    "                raise ValueError(\"You have to specify pixel_values\")\n",
    "\n",
    "            encoder_outputs = self.encoder(\n",
    "                pixel_values,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **kwargs_encoder,\n",
    "            )\n",
    "        elif isinstance(encoder_outputs, tuple):\n",
    "            encoder_outputs = BaseModelOutput(*encoder_outputs)\n",
    "\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        # optionally project encoder_hidden_states\n",
    "        if (\n",
    "            self.encoder.config.hidden_size != self.decoder.config.hidden_size\n",
    "            and self.decoder.config.cross_attention_hidden_size is None\n",
    "        ):\n",
    "            encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n",
    "\n",
    "        # else:\n",
    "        encoder_attention_mask = None\n",
    "\n",
    "        if (labels is not None) and (decoder_input_ids is None and decoder_inputs_embeds is None):\n",
    "            decoder_input_ids = shift_tokens_right(\n",
    "                labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            use_cache=use_cache,\n",
    "            past_key_values=past_key_values,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs_decoder,\n",
    "        )\n",
    "\n",
    "        # Compute loss independent from decoder (as some shift the logits inside them)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.reshape(-1))\n",
    "            \n",
    "            \n",
    "\n",
    "        if not return_dict:\n",
    "            if loss is not None:\n",
    "                return (loss,) + decoder_outputs + encoder_outputs\n",
    "            else:\n",
    "                return decoder_outputs + encoder_outputs\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=decoder_outputs.logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "model = VisionEncoderDecoderSmooth.from_pretrained('naver-clova-ix/donut-base')\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "processor.tokenizer.cls_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "model.config.eos_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.max_length = 256\n",
    "\n",
    "\n",
    "def draw_smiles(smiles):\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    d2d = Draw.MolDraw2DCairo(512,512)\n",
    "    dopts = d2d.drawOptions()\n",
    "    dopts.useBWAtomPalette()\n",
    "    d2d.DrawMolecule(m)\n",
    "    d2d.FinishDrawing()\n",
    "    bio = BytesIO(d2d.GetDrawingText())\n",
    "    return Image.open(bio).convert('RGB')\n",
    "\n",
    "MERGE_PROB = 0.2\n",
    "merge_i = 0\n",
    "ORGANIC_SET = ['B', 'C', 'N', 'O', 'P', 'S', 'F', 'Cl', 'Br', 'I']\n",
    "ELEMENTS = [\n",
    "    \"H\", \"He\", \"Li\", \"Be\", \"B\", \"C\", \"N\", \"O\", \"F\", \"Ne\",\n",
    "    \"Na\", \"Mg\", \"Al\", \"Si\", \"P\", \"S\", \"Cl\", \"Ar\", \"K\", \"Ca\",\n",
    "    \"Sc\", \"Ti\", \"V\", \"Cr\", \"Mn\", \"Fe\", \"Co\", \"Ni\", \"Cu\", \"Zn\",\n",
    "    \"Ga\", \"Ge\", \"As\", \"Se\", \"Br\", \"Kr\", \"Rb\", \"Sr\", \"Y\", \"Zr\",\n",
    "    \"Nb\", \"Mo\", \"Tc\", \"Ru\", \"Rh\", \"Pd\", \"Ag\", \"Cd\", \"In\", \"Sn\",\n",
    "    \"Sb\", \"Te\", \"I\", \"Xe\", \"Cs\", \"Ba\", \"La\", \"Ce\", \"Pr\", \"Nd\",\n",
    "    \"Pm\", \"Sm\", \"Eu\", \"Gd\", \"Tb\", \"Dy\", \"Ho\", \"Er\", \"Tm\", \"Yb\",\n",
    "    \"Lu\", \"Hf\", \"Ta\", \"W\", \"Re\", \"Os\", \"Ir\", \"Pt\", \"Au\", \"Hg\",\n",
    "    \"Tl\", \"Pb\", \"Bi\", \"Po\", \"At\", \"Rn\", \"Fr\", \"Ra\", \"Ac\", \"Th\",\n",
    "    \"Pa\", \"U\", \"Np\", \"Pu\", \"Am\", \"Cm\", \"Bk\", \"Cf\", \"Es\", \"Fm\",\n",
    "    \"Md\", \"No\", \"Lr\", \"Rf\", \"Db\", \"Sg\", \"Bh\", \"Hs\", \"Mt\", \"Ds\",\n",
    "    \"Rg\", \"Cn\", \"Nh\", \"Fl\", \"Mc\", \"Lv\", \"Ts\", \"Og\"\n",
    "]\n",
    "\n",
    "ATOMS = ORGANIC_SET + [f'[{e}]' for e in ELEMENTS]\n",
    "\n",
    "def aug_smiles(smiles):\n",
    "    global merge_i\n",
    "    \n",
    "    if random.random() < MERGE_PROB:\n",
    "        mode = random.choice(['long', 'short'])\n",
    "        if mode == 'long':\n",
    "            pass\n",
    "            #add_smiles_idx = merge_i % len_merge\n",
    "            #merge_i += 1\n",
    "            #smileses = [smiles, zinc20[add_smiles_idx]['smiles']]\n",
    "            #smileses.sort(key=len)\n",
    "            #smiles = '.'.join(smileses)\n",
    "        else:\n",
    "            count = random.randint(1, 3)\n",
    "            add_atoms = np.random.choice(ATOMS, count)\n",
    "            smileses = [smiles]\n",
    "            smileses.extend(list(add_atoms))\n",
    "            smileses.sort(key=len)\n",
    "            smiles = '.'.join(smileses)\n",
    "    #smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles), kekuleSmiles=True)\n",
    "    return smiles\n",
    "\n",
    "def prepare_features(examples):\n",
    "    smileses = [aug_smiles(s) for s in examples['smiles']]\n",
    "    images = [draw_smiles(s) for s in smileses]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    target_encoding = processor.tokenizer(\n",
    "        [f'{i}' for i in smileses],\n",
    "        padding=\"longest\",\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    tokenized_examples = {'pixel_values': pixel_values}\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    tokenized_examples['labels'] = labels\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_dataset_train = dataset_train.with_transform(\n",
    "    prepare_features)\n",
    "tokenized_dataset_val = dataset_val.with_transform(\n",
    "    prepare_features)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    'donut_modelv2',\n",
    "    remove_unused_columns=False,\n",
    "    save_safetensors = False,\n",
    "    evaluation_strategy = 'steps',\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    learning_rate = 4e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps = 5,\n",
    "    save_strategy = 'steps',\n",
    "    eval_steps = 2_000,\n",
    "    save_steps=2_000,\n",
    "    report_to = 'wandb',\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=12,\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    predict_with_generate = True,\n",
    "    save_total_limit = 10,\n",
    "    optim='adamw_torch',\n",
    "    adam_beta2 = 0.98,\n",
    ")\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    labels, predictions = preds.label_ids, preds.predictions\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    print(labels[-1], predictions[-1])\n",
    "    y_true = [x.strip() for x in labels]\n",
    "    y_pred = [x.strip() for x in predictions]\n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    return {f'cer': cer.compute(predictions=y_pred, references=y_true),'accuracy':accuracy}\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset = tokenized_dataset_train,\n",
    "    eval_dataset = tokenized_dataset_val,\n",
    "    compute_metrics = compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    data_collator=DefaultDataCollator()\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6aa03a-6b8d-41ca-b9ba-179e42c384ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey20007\u001b[0m (\u001b[33mandrey2007\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u001b[34m\u001b[4mhttps://wandb.me/wandb-init\u001b[0m.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/notebooks/wandb/run-20240204_162305-2u46om50\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mTrOCR_Small\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/andrey2007/DoHACK\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/andrey2007/DoHACK/runs/2u46om50\u001b[0m\n",
      "Random seed set as 42\n",
      "Generating train split: 89907045 examples [00:17, 5060156.60 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.74k/4.74k [00:00<00:00, 3.67MB/s]\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 809M/809M [00:02<00:00, 351MB/s]\n",
      "/usr/local/lib/python3.9/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "{'loss': 14.7978, 'learning_rate': 0.00039999999999199655, 'epoch': 0.0}        \n",
      "{'loss': 8.5171, 'learning_rate': 0.00039999999971187416, 'epoch': 0.0}         \n",
      "{'loss': 5.019, 'learning_rate': 0.0003999999990315769, 'epoch': 0.0}           \n",
      "{'loss': 3.971, 'learning_rate': 0.00039999999795110484, 'epoch': 0.0}          \n",
      "{'loss': 3.6693, 'learning_rate': 0.00039999999647045796, 'epoch': 0.0}         \n",
      "{'loss': 3.2441, 'learning_rate': 0.00039999999458963624, 'epoch': 0.0}         \n",
      "{'loss': 2.8747, 'learning_rate': 0.0003999999923086398, 'epoch': 0.0}          \n",
      "{'loss': 2.7171, 'learning_rate': 0.0003999999896274684, 'epoch': 0.0}          \n",
      "{'loss': 2.5564, 'learning_rate': 0.0003999999865461222, 'epoch': 0.0}          \n",
      "{'loss': 2.441, 'learning_rate': 0.00039999998306460124, 'epoch': 0.0}          \n",
      "{'loss': 2.3386, 'learning_rate': 0.00039999997918290544, 'epoch': 0.0}         \n",
      "{'loss': 2.266, 'learning_rate': 0.0003999999749010349, 'epoch': 0.0}           \n",
      "{'loss': 2.1784, 'learning_rate': 0.00039999997021898946, 'epoch': 0.0}         \n",
      "{'loss': 2.1071, 'learning_rate': 0.00039999996513676934, 'epoch': 0.0}         \n",
      "{'loss': 2.0997, 'learning_rate': 0.0003999999596543744, 'epoch': 0.0}          \n",
      "{'loss': 2.0319, 'learning_rate': 0.0003999999537718048, 'epoch': 0.0}          \n",
      "{'loss': 1.9915, 'learning_rate': 0.0003999999474890604, 'epoch': 0.0}          \n",
      "{'loss': 1.9534, 'learning_rate': 0.0003999999408061413, 'epoch': 0.0}          \n",
      "{'loss': 1.9294, 'learning_rate': 0.00039999993372304745, 'epoch': 0.0}         \n",
      "{'loss': 1.8911, 'learning_rate': 0.0003999999262397789, 'epoch': 0.0}          \n",
      "{'loss': 1.8715, 'learning_rate': 0.00039999991835633564, 'epoch': 0.0}         \n",
      "{'loss': 1.9006, 'learning_rate': 0.0003999999100727177, 'epoch': 0.0}          \n",
      "{'loss': 1.8367, 'learning_rate': 0.00039999990138892515, 'epoch': 0.0}         \n",
      "{'loss': 1.8147, 'learning_rate': 0.00039999989230495795, 'epoch': 0.0}         \n",
      "{'loss': 1.8073, 'learning_rate': 0.00039999988282081613, 'epoch': 0.0}         \n",
      "{'loss': 1.7825, 'learning_rate': 0.00039999987293649967, 'epoch': 0.0}         \n",
      "{'loss': 1.7772, 'learning_rate': 0.0003999998626520087, 'epoch': 0.0}          \n",
      "{'loss': 1.7446, 'learning_rate': 0.00039999985196734314, 'epoch': 0.0}         \n",
      "{'loss': 1.7266, 'learning_rate': 0.000399999840882503, 'epoch': 0.0}           \n",
      "{'loss': 1.7259, 'learning_rate': 0.0003999998293974884, 'epoch': 0.0}          \n",
      "{'loss': 1.7074, 'learning_rate': 0.00039999981751229923, 'epoch': 0.0}         \n",
      "{'loss': 1.7209, 'learning_rate': 0.0003999998052269357, 'epoch': 0.0}          \n",
      "{'loss': 1.7016, 'learning_rate': 0.00039999979254139764, 'epoch': 0.0}         \n",
      "{'loss': 1.6966, 'learning_rate': 0.0003999997794556852, 'epoch': 0.0}          \n",
      "{'loss': 1.6782, 'learning_rate': 0.00039999976596979833, 'epoch': 0.0}         \n",
      "{'loss': 1.6617, 'learning_rate': 0.00039999975208373714, 'epoch': 0.0}         \n",
      "{'loss': 1.6381, 'learning_rate': 0.00039999973779750154, 'epoch': 0.0}         \n",
      "{'loss': 1.6445, 'learning_rate': 0.0003999997231110917, 'epoch': 0.0}          \n",
      "{'loss': 1.6312, 'learning_rate': 0.0003999997080245075, 'epoch': 0.0}          \n",
      "{'loss': 1.6301, 'learning_rate': 0.00039999969253774916, 'epoch': 0.0}         \n",
      "{'loss': 1.6319, 'learning_rate': 0.0003999996766508165, 'epoch': 0.0}          \n",
      "{'loss': 1.6355, 'learning_rate': 0.00039999966036370974, 'epoch': 0.0}         \n",
      "{'loss': 1.6047, 'learning_rate': 0.00039999964367642873, 'epoch': 0.0}         \n",
      "{'loss': 1.5923, 'learning_rate': 0.00039999962658897364, 'epoch': 0.0}         \n",
      "{'loss': 1.5872, 'learning_rate': 0.0003999996091013445, 'epoch': 0.0}          \n",
      "{'loss': 1.6181, 'learning_rate': 0.0003999995912135413, 'epoch': 0.0}          \n",
      "{'loss': 1.5944, 'learning_rate': 0.00039999957292556404, 'epoch': 0.0}         \n",
      "{'loss': 1.5969, 'learning_rate': 0.0003999995542374128, 'epoch': 0.0}          \n",
      "{'loss': 1.5883, 'learning_rate': 0.00039999953514908764, 'epoch': 0.0}         \n",
      "{'loss': 1.5763, 'learning_rate': 0.00039999951566058856, 'epoch': 0.0}         \n",
      "{'loss': 1.5641, 'learning_rate': 0.0003999994957719156, 'epoch': 0.0}          \n",
      "{'loss': 1.5806, 'learning_rate': 0.00039999947548306883, 'epoch': 0.0}         \n",
      "{'loss': 1.5598, 'learning_rate': 0.00039999945479404824, 'epoch': 0.0}         \n",
      "{'loss': 1.5756, 'learning_rate': 0.00039999943370485393, 'epoch': 0.0}         \n",
      "{'loss': 1.5587, 'learning_rate': 0.000399999412215486, 'epoch': 0.0}           \n",
      "{'loss': 1.55, 'learning_rate': 0.0003999993903259443, 'epoch': 0.0}            \n",
      "{'loss': 1.5478, 'learning_rate': 0.000399999368036229, 'epoch': 0.0}           \n",
      "{'loss': 1.5413, 'learning_rate': 0.0003999993453463402, 'epoch': 0.0}          \n",
      "{'loss': 1.524, 'learning_rate': 0.0003999993222562778, 'epoch': 0.0}           \n",
      "{'loss': 1.5377, 'learning_rate': 0.00039999929876604197, 'epoch': 0.0}         \n",
      "{'loss': 1.5385, 'learning_rate': 0.00039999927487563273, 'epoch': 0.0}         \n",
      "{'loss': 1.5205, 'learning_rate': 0.00039999925058505, 'epoch': 0.0}            \n",
      "{'loss': 1.5201, 'learning_rate': 0.000399999225894294, 'epoch': 0.0}           \n",
      "{'loss': 1.5043, 'learning_rate': 0.00039999920080336477, 'epoch': 0.0}         \n",
      "{'loss': 1.5188, 'learning_rate': 0.00039999917531226216, 'epoch': 0.0}         \n",
      "{'loss': 1.5158, 'learning_rate': 0.0003999991494209865, 'epoch': 0.0}          \n",
      "{'loss': 1.5068, 'learning_rate': 0.00039999912312953766, 'epoch': 0.0}         \n",
      "{'loss': 1.5034, 'learning_rate': 0.0003999990964379157, 'epoch': 0.0}          \n",
      "{'loss': 1.4957, 'learning_rate': 0.0003999990693461207, 'epoch': 0.0}          \n",
      "{'loss': 1.511, 'learning_rate': 0.0003999990418541528, 'epoch': 0.0}           \n",
      "{'loss': 1.511, 'learning_rate': 0.00039999901396201194, 'epoch': 0.0}          \n",
      "{'loss': 1.4913, 'learning_rate': 0.00039999898566969825, 'epoch': 0.0}         \n",
      "{'loss': 1.4912, 'learning_rate': 0.00039999895697721176, 'epoch': 0.0}         \n",
      "{'loss': 1.487, 'learning_rate': 0.0003999989278845525, 'epoch': 0.0}           \n",
      "{'loss': 1.5267, 'learning_rate': 0.0003999988983917205, 'epoch': 0.0}          \n",
      "{'loss': 1.5087, 'learning_rate': 0.0003999988684987159, 'epoch': 0.0}          \n",
      "{'loss': 1.4912, 'learning_rate': 0.00039999883820553877, 'epoch': 0.0}         \n",
      "{'loss': 1.4777, 'learning_rate': 0.0003999988075121891, 'epoch': 0.0}          \n",
      "{'loss': 1.4776, 'learning_rate': 0.000399998776418667, 'epoch': 0.0}           \n",
      "{'loss': 1.4707, 'learning_rate': 0.00039999874492497244, 'epoch': 0.0}         \n",
      "{'loss': 1.4727, 'learning_rate': 0.0003999987130311056, 'epoch': 0.0}          \n",
      "{'loss': 1.4651, 'learning_rate': 0.00039999868073706653, 'epoch': 0.0}         \n",
      "{'loss': 1.4626, 'learning_rate': 0.0003999986480428553, 'epoch': 0.0}          \n",
      "{'loss': 1.467, 'learning_rate': 0.0003999986149484719, 'epoch': 0.0}           \n",
      "{'loss': 1.457, 'learning_rate': 0.0003999985814539164, 'epoch': 0.0}           \n",
      "{'loss': 1.4593, 'learning_rate': 0.00039999854755918895, 'epoch': 0.0}         \n",
      "{'loss': 1.4489, 'learning_rate': 0.00039999851326428946, 'epoch': 0.0}         \n",
      "{'loss': 1.4589, 'learning_rate': 0.0003999984785692182, 'epoch': 0.0}          \n",
      "{'loss': 1.4605, 'learning_rate': 0.00039999844347397514, 'epoch': 0.0}         \n",
      "{'loss': 1.4467, 'learning_rate': 0.0003999984079785604, 'epoch': 0.0}          \n",
      "{'loss': 1.4412, 'learning_rate': 0.00039999837208297396, 'epoch': 0.0}         \n",
      "{'loss': 1.442, 'learning_rate': 0.00039999833578721595, 'epoch': 0.0}          \n",
      "{'loss': 1.4378, 'learning_rate': 0.0003999982990912864, 'epoch': 0.0}          \n",
      "{'loss': 1.4395, 'learning_rate': 0.00039999826199518546, 'epoch': 0.0}         \n",
      "{'loss': 1.4341, 'learning_rate': 0.0003999982244989132, 'epoch': 0.0}          \n",
      "{'loss': 1.4303, 'learning_rate': 0.0003999981866024696, 'epoch': 0.0}          \n",
      "{'loss': 1.4236, 'learning_rate': 0.0003999981483058548, 'epoch': 0.0}          \n",
      "{'loss': 1.4171, 'learning_rate': 0.0003999981096090689, 'epoch': 0.0}          \n",
      "{'loss': 1.4251, 'learning_rate': 0.00039999807051211187, 'epoch': 0.0}         \n",
      "{'loss': 1.4121, 'learning_rate': 0.00039999803101498397, 'epoch': 0.0}         \n",
      "{'loss': 1.4158, 'learning_rate': 0.00039999799111768513, 'epoch': 0.0}         \n",
      "{'loss': 1.4199, 'learning_rate': 0.00039999795082021545, 'epoch': 0.0}         \n",
      "{'loss': 1.4127, 'learning_rate': 0.00039999791012257504, 'epoch': 0.0}         \n",
      "{'loss': 1.4164, 'learning_rate': 0.000399997869024764, 'epoch': 0.0}           \n",
      "{'loss': 1.4191, 'learning_rate': 0.00039999782752678237, 'epoch': 0.0}         \n",
      "{'loss': 1.4128, 'learning_rate': 0.00039999778562863027, 'epoch': 0.0}         \n",
      "{'loss': 1.4162, 'learning_rate': 0.00039999774333030776, 'epoch': 0.0}         \n",
      "{'loss': 1.4128, 'learning_rate': 0.0003999977006318149, 'epoch': 0.0}          \n",
      "{'loss': 1.4154, 'learning_rate': 0.0003999976575331518, 'epoch': 0.0}          \n",
      "{'loss': 1.4188, 'learning_rate': 0.0003999976140343186, 'epoch': 0.0}          \n",
      "{'loss': 1.412, 'learning_rate': 0.0003999975701353153, 'epoch': 0.0}           \n",
      "{'loss': 1.396, 'learning_rate': 0.00039999752583614207, 'epoch': 0.0}          \n",
      "{'loss': 1.3949, 'learning_rate': 0.0003999974811367989, 'epoch': 0.0}          \n",
      "{'loss': 1.4068, 'learning_rate': 0.00039999743603728595, 'epoch': 0.0}         \n",
      "{'loss': 1.3988, 'learning_rate': 0.00039999739053760337, 'epoch': 0.0}         \n",
      "{'loss': 1.4073, 'learning_rate': 0.0003999973446377511, 'epoch': 0.0}          \n",
      "{'loss': 1.3966, 'learning_rate': 0.00039999729833772933, 'epoch': 0.0}         \n",
      "{'loss': 1.3988, 'learning_rate': 0.0003999972516375381, 'epoch': 0.0}          \n",
      "{'loss': 1.3922, 'learning_rate': 0.00039999720453717753, 'epoch': 0.0}         \n",
      "{'loss': 1.398, 'learning_rate': 0.0003999971570366477, 'epoch': 0.0}           \n",
      "{'loss': 1.3871, 'learning_rate': 0.00039999710913594884, 'epoch': 0.0}         \n",
      "{'loss': 1.3831, 'learning_rate': 0.0003999970608350808, 'epoch': 0.0}          \n",
      "{'loss': 1.3778, 'learning_rate': 0.0003999970121340439, 'epoch': 0.0}          \n",
      "{'loss': 1.3827, 'learning_rate': 0.0003999969630328381, 'epoch': 0.0}          \n",
      "{'loss': 1.38, 'learning_rate': 0.00039999691353146354, 'epoch': 0.0}           \n",
      "{'loss': 1.3811, 'learning_rate': 0.0003999968636299203, 'epoch': 0.0}          \n",
      "{'loss': 1.3745, 'learning_rate': 0.00039999681332820856, 'epoch': 0.0}         \n",
      "{'loss': 1.3883, 'learning_rate': 0.0003999967626263283, 'epoch': 0.0}          \n",
      "{'loss': 1.3812, 'learning_rate': 0.0003999967115242797, 'epoch': 0.0}          \n",
      "{'loss': 1.38, 'learning_rate': 0.00039999666002206284, 'epoch': 0.0}           \n",
      "{'loss': 1.3753, 'learning_rate': 0.0003999966081196779, 'epoch': 0.0}          \n",
      "{'loss': 1.3655, 'learning_rate': 0.00039999655581712487, 'epoch': 0.0}         \n",
      "{'loss': 1.3677, 'learning_rate': 0.0003999965031144038, 'epoch': 0.0}          \n",
      "{'loss': 1.3602, 'learning_rate': 0.000399996450011515, 'epoch': 0.0}           \n",
      "{'loss': 1.3603, 'learning_rate': 0.0003999963965084584, 'epoch': 0.0}          \n",
      "{'loss': 1.3587, 'learning_rate': 0.00039999634260523423, 'epoch': 0.0}         \n",
      "{'loss': 1.3474, 'learning_rate': 0.00039999628830184253, 'epoch': 0.0}         \n",
      "{'loss': 1.3463, 'learning_rate': 0.00039999623359828336, 'epoch': 0.0}         \n",
      "{'loss': 1.3521, 'learning_rate': 0.000399996178494557, 'epoch': 0.0}           \n",
      "{'loss': 1.3505, 'learning_rate': 0.0003999961229906634, 'epoch': 0.0}          \n",
      "{'loss': 1.3634, 'learning_rate': 0.0003999960670866027, 'epoch': 0.0}          \n",
      "{'loss': 1.3461, 'learning_rate': 0.00039999601078237506, 'epoch': 0.0}         \n",
      "{'loss': 1.3513, 'learning_rate': 0.00039999595407798057, 'epoch': 0.0}         \n",
      "{'loss': 1.3448, 'learning_rate': 0.0003999958969734193, 'epoch': 0.0}          \n",
      "{'loss': 1.3254, 'learning_rate': 0.00039999583946869144, 'epoch': 0.0}         \n",
      "{'loss': 1.3165, 'learning_rate': 0.000399995781563797, 'epoch': 0.0}           \n",
      "{'loss': 1.3149, 'learning_rate': 0.0003999957232587363, 'epoch': 0.0}          \n",
      "{'loss': 1.3223, 'learning_rate': 0.0003999956645535092, 'epoch': 0.0}          \n",
      "{'loss': 1.3072, 'learning_rate': 0.000399995605448116, 'epoch': 0.0}           \n",
      "{'loss': 1.3081, 'learning_rate': 0.00039999554594255673, 'epoch': 0.0}         \n",
      "{'loss': 1.2912, 'learning_rate': 0.00039999548603683154, 'epoch': 0.0}         \n",
      "{'loss': 1.289, 'learning_rate': 0.00039999542573094057, 'epoch': 0.0}          \n",
      "{'loss': 1.2784, 'learning_rate': 0.00039999536502488396, 'epoch': 0.0}         \n",
      "{'loss': 1.2617, 'learning_rate': 0.0003999953039186617, 'epoch': 0.0}          \n",
      "{'loss': 1.2627, 'learning_rate': 0.000399995242412274, 'epoch': 0.0}           \n",
      "{'loss': 1.2444, 'learning_rate': 0.00039999518050572104, 'epoch': 0.0}         \n",
      "{'loss': 1.2389, 'learning_rate': 0.0003999951181990029, 'epoch': 0.0}          \n",
      "{'loss': 1.2276, 'learning_rate': 0.00039999505549211963, 'epoch': 0.0}         \n",
      "{'loss': 1.216, 'learning_rate': 0.00039999499238507145, 'epoch': 0.0}          \n",
      "{'loss': 1.2241, 'learning_rate': 0.00039999492887785845, 'epoch': 0.0}         \n",
      "{'loss': 1.209, 'learning_rate': 0.00039999486497048074, 'epoch': 0.0}          \n",
      "{'loss': 1.1944, 'learning_rate': 0.0003999948006629385, 'epoch': 0.0}          \n",
      "{'loss': 1.1789, 'learning_rate': 0.0003999947359552318, 'epoch': 0.0}          \n",
      "{'loss': 1.1702, 'learning_rate': 0.0003999946708473608, 'epoch': 0.0}          \n",
      "{'loss': 1.1632, 'learning_rate': 0.0003999946053393257, 'epoch': 0.0}          \n",
      "{'loss': 1.1642, 'learning_rate': 0.0003999945394311265, 'epoch': 0.0}          \n",
      "{'loss': 1.1519, 'learning_rate': 0.00039999447312276333, 'epoch': 0.0}         \n",
      "{'loss': 1.1444, 'learning_rate': 0.00039999440641423644, 'epoch': 0.0}         \n",
      "{'loss': 1.129, 'learning_rate': 0.00039999433930554593, 'epoch': 0.0}          \n",
      "{'loss': 1.1248, 'learning_rate': 0.0003999942717966919, 'epoch': 0.0}          \n",
      "{'loss': 1.1056, 'learning_rate': 0.00039999420388767446, 'epoch': 0.0}         \n",
      "{'loss': 1.1026, 'learning_rate': 0.00039999413557849383, 'epoch': 0.0}         \n",
      "{'loss': 1.1013, 'learning_rate': 0.0003999940668691501, 'epoch': 0.0}          \n",
      "{'loss': 1.0907, 'learning_rate': 0.00039999399775964337, 'epoch': 0.0}         \n",
      "{'loss': 1.0801, 'learning_rate': 0.0003999939282499738, 'epoch': 0.0}          \n",
      "{'loss': 1.0723, 'learning_rate': 0.00039999385834014155, 'epoch': 0.0}         \n",
      "{'loss': 1.065, 'learning_rate': 0.00039999378803014685, 'epoch': 0.0}          \n",
      "{'loss': 1.0619, 'learning_rate': 0.00039999371731998967, 'epoch': 0.0}         \n",
      "{'loss': 1.058, 'learning_rate': 0.0003999936462096702, 'epoch': 0.0}           \n",
      "{'loss': 1.0415, 'learning_rate': 0.0003999935746991886, 'epoch': 0.0}          \n",
      "{'loss': 1.0405, 'learning_rate': 0.00039999350278854503, 'epoch': 0.0}         \n",
      "{'loss': 1.0325, 'learning_rate': 0.0003999934304777397, 'epoch': 0.0}          \n",
      "{'loss': 1.0249, 'learning_rate': 0.0003999933577667726, 'epoch': 0.0}          \n",
      "{'loss': 1.0224, 'learning_rate': 0.00039999328465564396, 'epoch': 0.0}         \n",
      "{'loss': 1.0167, 'learning_rate': 0.00039999321114435397, 'epoch': 0.0}         \n",
      "{'loss': 1.0136, 'learning_rate': 0.0003999931372329027, 'epoch': 0.0}          \n",
      "{'loss': 1.042, 'learning_rate': 0.0003999930629212903, 'epoch': 0.0}           \n",
      "{'loss': 1.0617, 'learning_rate': 0.000399992988209517, 'epoch': 0.0}           \n",
      "{'loss': 1.0395, 'learning_rate': 0.00039999291309758284, 'epoch': 0.0}         \n",
      "{'loss': 1.0153, 'learning_rate': 0.00039999283758548806, 'epoch': 0.0}         \n",
      "{'loss': 1.0103, 'learning_rate': 0.0003999927616732328, 'epoch': 0.0}          \n",
      "{'loss': 0.9918, 'learning_rate': 0.00039999268536081715, 'epoch': 0.0}         \n",
      "{'loss': 0.9871, 'learning_rate': 0.0003999926086482413, 'epoch': 0.0}          \n",
      "{'loss': 0.9862, 'learning_rate': 0.00039999253153550534, 'epoch': 0.0}         \n",
      "{'loss': 0.9759, 'learning_rate': 0.0003999924540226096, 'epoch': 0.0}          \n",
      "{'loss': 0.9842, 'learning_rate': 0.00039999237610955404, 'epoch': 0.0}         \n",
      "{'loss': 0.9849, 'learning_rate': 0.0003999922977963389, 'epoch': 0.0}          \n",
      "{'loss': 0.9828, 'learning_rate': 0.00039999221908296445, 'epoch': 0.0}         \n",
      "{'loss': 0.9734, 'learning_rate': 0.00039999213996943063, 'epoch': 0.0}         \n",
      "{'loss': 0.9642, 'learning_rate': 0.0003999920604557377, 'epoch': 0.0}          \n",
      "{'loss': 0.9658, 'learning_rate': 0.0003999919805418859, 'epoch': 0.0}          \n",
      "{'loss': 0.9683, 'learning_rate': 0.00039999190022787525, 'epoch': 0.0}         \n",
      "{'loss': 0.9642, 'learning_rate': 0.00039999181951370596, 'epoch': 0.0}         \n",
      "{'loss': 0.9553, 'learning_rate': 0.00039999173839937825, 'epoch': 0.0}         \n",
      "{'loss': 0.9632, 'learning_rate': 0.00039999165688489217, 'epoch': 0.0}         \n",
      "{'loss': 0.9627, 'learning_rate': 0.000399991574970248, 'epoch': 0.0}           \n",
      "{'loss': 0.9569, 'learning_rate': 0.0003999914926554459, 'epoch': 0.0}          \n",
      "{'loss': 0.9609, 'learning_rate': 0.00039999140994048584, 'epoch': 0.0}         \n",
      "{'loss': 0.9516, 'learning_rate': 0.00039999132682536824, 'epoch': 0.0}         \n",
      "{'loss': 0.9487, 'learning_rate': 0.00039999124331009314, 'epoch': 0.0}         \n",
      "{'loss': 0.949, 'learning_rate': 0.00039999115939466065, 'epoch': 0.0}          \n",
      "{'loss': 0.9487, 'learning_rate': 0.00039999107507907115, 'epoch': 0.0}         \n",
      "{'loss': 0.9472, 'learning_rate': 0.0003999909903633246, 'epoch': 0.0}          \n",
      "{'loss': 0.9475, 'learning_rate': 0.00039999056078224315, 'epoch': 0.0}         \n",
      "{'loss': 0.9426, 'learning_rate': 0.00039999047366555835, 'epoch': 0.0}         \n",
      "{'loss': 0.9381, 'learning_rate': 0.00039999038614871774, 'epoch': 0.0}         \n",
      "{'loss': 0.9291, 'learning_rate': 0.0003999902982317215, 'epoch': 0.0}          \n",
      "{'loss': 0.9377, 'learning_rate': 0.0003999902099145699, 'epoch': 0.0}          \n",
      "{'loss': 0.936, 'learning_rate': 0.000399990121197263, 'epoch': 0.0}            \n",
      "{'loss': 0.9322, 'learning_rate': 0.0003999900320798011, 'epoch': 0.0}          \n",
      "{'loss': 0.9277, 'learning_rate': 0.0003999899425621843, 'epoch': 0.0}          \n",
      "{'loss': 0.9303, 'learning_rate': 0.00039998985264441277, 'epoch': 0.0}         \n",
      "{'loss': 0.9325, 'learning_rate': 0.0003999897623264866, 'epoch': 0.0}          \n",
      "{'loss': 0.9349, 'learning_rate': 0.0003999896716084062, 'epoch': 0.0}          \n",
      "{'loss': 0.9264, 'learning_rate': 0.0003999895804901717, 'epoch': 0.0}          \n",
      "{'loss': 0.926, 'learning_rate': 0.00039998948897178305, 'epoch': 0.0}          \n",
      "{'loss': 0.9233, 'learning_rate': 0.00039998939705324066, 'epoch': 0.0}         \n",
      "{'loss': 0.9238, 'learning_rate': 0.0003999893047345447, 'epoch': 0.0}          \n",
      "{'loss': 0.9269, 'learning_rate': 0.0003999892120156952, 'epoch': 0.0}          \n",
      "{'loss': 0.92, 'learning_rate': 0.0003999891188966925, 'epoch': 0.0}            \n",
      "{'loss': 0.9289, 'learning_rate': 0.0003999890253775368, 'epoch': 0.0}          \n",
      "{'loss': 0.9293, 'learning_rate': 0.00039998893145822814, 'epoch': 0.0}         \n",
      "{'loss': 0.9484, 'learning_rate': 0.00039998883713876683, 'epoch': 0.0}         \n",
      "{'loss': 0.9536, 'learning_rate': 0.000399988742419153, 'epoch': 0.0}           \n",
      "{'loss': 0.9266, 'learning_rate': 0.0003999886472993869, 'epoch': 0.0}          \n",
      "{'loss': 0.9205, 'learning_rate': 0.0003999885517794686, 'epoch': 0.0}          \n",
      "{'loss': 0.9142, 'learning_rate': 0.00039998845585939845, 'epoch': 0.0}         \n",
      "{'loss': 0.9177, 'learning_rate': 0.00039998835953917653, 'epoch': 0.0}         \n",
      "{'loss': 0.9128, 'learning_rate': 0.00039998826281880306, 'epoch': 0.0}         \n",
      "{'loss': 0.9165, 'learning_rate': 0.0003999881656982782, 'epoch': 0.0}          \n",
      "{'loss': 0.9171, 'learning_rate': 0.0003999880681776023, 'epoch': 0.0}          \n",
      "{'loss': 0.9166, 'learning_rate': 0.00039998797025677535, 'epoch': 0.0}         \n",
      "{'loss': 0.9124, 'learning_rate': 0.0003999878719357977, 'epoch': 0.0}          \n",
      "{'loss': 0.9174, 'learning_rate': 0.0003999877732146694, 'epoch': 0.0}          \n",
      "{'loss': 0.9218, 'learning_rate': 0.0003999876740933908, 'epoch': 0.0}          \n",
      "{'loss': 0.9211, 'learning_rate': 0.0003999875745719619, 'epoch': 0.0}          \n",
      "{'loss': 0.9146, 'learning_rate': 0.00039998747465038316, 'epoch': 0.0}         \n",
      "{'loss': 0.9122, 'learning_rate': 0.00039998737432865464, 'epoch': 0.0}         \n",
      "{'loss': 0.9072, 'learning_rate': 0.0003999872736067765, 'epoch': 0.0}          \n",
      "{'loss': 0.9115, 'learning_rate': 0.00039998717248474895, 'epoch': 0.0}         \n",
      "{'loss': 0.9095, 'learning_rate': 0.00039998707096257233, 'epoch': 0.0}         \n",
      "{'loss': 0.9087, 'learning_rate': 0.00039998696904024666, 'epoch': 0.0}         \n",
      "{'loss': 0.9122, 'learning_rate': 0.0003999868667177722, 'epoch': 0.0}          \n",
      "{'loss': 0.9104, 'learning_rate': 0.0003999867639951493, 'epoch': 0.0}          \n",
      "{'loss': 0.9137, 'learning_rate': 0.000399986660872378, 'epoch': 0.0}           \n",
      "{'loss': 0.9196, 'learning_rate': 0.00039998655734945854, 'epoch': 0.0}         \n",
      "{'loss': 0.9108, 'learning_rate': 0.0003999864534263912, 'epoch': 0.0}          \n",
      "{'loss': 0.9126, 'learning_rate': 0.000399986349103176, 'epoch': 0.0}           \n",
      "{'loss': 0.9139, 'learning_rate': 0.0003999862443798134, 'epoch': 0.0}          \n",
      "{'loss': 0.9094, 'learning_rate': 0.00039998613925630344, 'epoch': 0.0}         \n",
      "{'loss': 0.9062, 'learning_rate': 0.00039998603373264636, 'epoch': 0.0}         \n",
      "{'loss': 0.917, 'learning_rate': 0.00039998592780884245, 'epoch': 0.0}          \n",
      "{'loss': 0.9113, 'learning_rate': 0.00039998582148489183, 'epoch': 0.0}         \n",
      "{'loss': 0.9076, 'learning_rate': 0.0003999857147607947, 'epoch': 0.0}          \n",
      "{'loss': 0.9028, 'learning_rate': 0.00039998560763655137, 'epoch': 0.0}         \n",
      "{'loss': 0.9037, 'learning_rate': 0.00039998550011216197, 'epoch': 0.0}         \n",
      "{'loss': 0.908, 'learning_rate': 0.0003999853921876268, 'epoch': 0.0}           \n",
      "{'loss': 0.9051, 'learning_rate': 0.00039998528386294595, 'epoch': 0.0}         \n",
      "{'loss': 0.8987, 'learning_rate': 0.00039998517513811977, 'epoch': 0.0}         \n",
      "{'loss': 0.8994, 'learning_rate': 0.0003999850660131484, 'epoch': 0.0}          \n",
      "{'loss': 0.8998, 'learning_rate': 0.00039998495648803205, 'epoch': 0.0}         \n",
      "{'loss': 0.8982, 'learning_rate': 0.000399984846562771, 'epoch': 0.0}           \n",
      "{'loss': 0.9031, 'learning_rate': 0.0003999847362373654, 'epoch': 0.0}          \n",
      "{'loss': 0.9014, 'learning_rate': 0.0003999846255118155, 'epoch': 0.0}          \n",
      "{'loss': 0.9023, 'learning_rate': 0.00039998451438612157, 'epoch': 0.0}         \n",
      "{'loss': 0.9039, 'learning_rate': 0.00039998440286028375, 'epoch': 0.0}         \n",
      "{'loss': 0.903, 'learning_rate': 0.00039998429093430235, 'epoch': 0.0}          \n",
      "{'loss': 0.9012, 'learning_rate': 0.0003999841786081775, 'epoch': 0.0}          \n",
      "{'loss': 0.9019, 'learning_rate': 0.0003999840658819095, 'epoch': 0.0}          \n",
      "{'loss': 0.8965, 'learning_rate': 0.00039998395275549854, 'epoch': 0.0}         \n",
      "{'loss': 0.8963, 'learning_rate': 0.0003999838392289448, 'epoch': 0.0}          \n",
      "{'loss': 0.8983, 'learning_rate': 0.0003999837253022486, 'epoch': 0.0}          \n",
      "{'loss': 0.8964, 'learning_rate': 0.00039998361097541015, 'epoch': 0.0}         \n",
      "{'loss': 0.8934, 'learning_rate': 0.0003999834962484297, 'epoch': 0.0}          \n",
      "{'loss': 0.8961, 'learning_rate': 0.00039998338112130733, 'epoch': 0.0}         \n",
      "{'loss': 0.8965, 'learning_rate': 0.0003999832655940434, 'epoch': 0.0}          \n",
      "{'loss': 0.893, 'learning_rate': 0.0003999831496666382, 'epoch': 0.0}           \n",
      "{'loss': 0.8945, 'learning_rate': 0.0003999830333390918, 'epoch': 0.0}          \n",
      "{'loss': 0.8926, 'learning_rate': 0.00039998291661140457, 'epoch': 0.0}         \n",
      "{'loss': 0.8934, 'learning_rate': 0.0003999827994835767, 'epoch': 0.0}          \n",
      "{'loss': 0.9001, 'learning_rate': 0.0003999826819556083, 'epoch': 0.0}          \n",
      "{'loss': 0.924, 'learning_rate': 0.0003999825640274998, 'epoch': 0.0}           \n",
      "{'loss': 0.9141, 'learning_rate': 0.00039998244569925133, 'epoch': 0.0}         \n",
      "{'loss': 0.899, 'learning_rate': 0.00039998232697086315, 'epoch': 0.0}          \n",
      "{'loss': 0.8967, 'learning_rate': 0.0003999822078423355, 'epoch': 0.0}          \n",
      "{'loss': 0.9021, 'learning_rate': 0.0003999820883136686, 'epoch': 0.0}          \n",
      "{'loss': 0.8988, 'learning_rate': 0.00039998196838486274, 'epoch': 0.0}         \n",
      "{'loss': 0.8941, 'learning_rate': 0.00039998184805591807, 'epoch': 0.0}         \n",
      "{'loss': 0.8926, 'learning_rate': 0.00039998172732683487, 'epoch': 0.0}         \n",
      "{'loss': 0.8886, 'learning_rate': 0.0003999816061976135, 'epoch': 0.0}          \n",
      "{'loss': 0.8894, 'learning_rate': 0.000399981484668254, 'epoch': 0.0}           \n",
      "{'loss': 0.8869, 'learning_rate': 0.0003999813627387568, 'epoch': 0.0}          \n",
      "{'loss': 0.8871, 'learning_rate': 0.00039998124040912196, 'epoch': 0.0}         \n",
      "{'loss': 0.8903, 'learning_rate': 0.00039998111767934987, 'epoch': 0.0}         \n",
      "{'loss': 0.8906, 'learning_rate': 0.0003999809945494407, 'epoch': 0.0}          \n",
      "{'loss': 0.8901, 'learning_rate': 0.0003999808710193947, 'epoch': 0.0}          \n",
      "{'loss': 0.8888, 'learning_rate': 0.00039998074708921225, 'epoch': 0.0}         \n",
      "{'loss': 0.8871, 'learning_rate': 0.0003999806227588934, 'epoch': 0.0}          \n",
      "{'loss': 0.8905, 'learning_rate': 0.00039998049802843845, 'epoch': 0.0}         \n",
      "{'loss': 0.8871, 'learning_rate': 0.0003999803728978478, 'epoch': 0.0}          \n",
      "{'loss': 0.8866, 'learning_rate': 0.0003999802473671215, 'epoch': 0.0}          \n",
      "{'loss': 0.8887, 'learning_rate': 0.0003999801214362599, 'epoch': 0.0}          \n",
      "{'loss': 0.8911, 'learning_rate': 0.0003999799951052633, 'epoch': 0.0}          \n",
      "{'loss': 0.8876, 'learning_rate': 0.00039997986837413184, 'epoch': 0.0}         \n",
      "{'loss': 0.8854, 'learning_rate': 0.0003999797412428658, 'epoch': 0.0}          \n",
      "{'loss': 0.8857, 'learning_rate': 0.0003999796137114655, 'epoch': 0.0}          \n",
      "{'loss': 0.8911, 'learning_rate': 0.00039997948577993116, 'epoch': 0.0}         \n",
      "{'loss': 0.8849, 'learning_rate': 0.000399979357448263, 'epoch': 0.0}           \n",
      "{'loss': 0.8861, 'learning_rate': 0.0003999792287164613, 'epoch': 0.0}          \n",
      "{'loss': 0.8911, 'learning_rate': 0.00039997909958452635, 'epoch': 0.0}         \n",
      "{'loss': 0.9022, 'learning_rate': 0.0003999789700524584, 'epoch': 0.0}          \n",
      "{'loss': 0.897, 'learning_rate': 0.0003999788401202577, 'epoch': 0.0}           \n",
      "{'loss': 0.8921, 'learning_rate': 0.0003999787097879244, 'epoch': 0.0}          \n",
      "{'loss': 0.8846, 'learning_rate': 0.00039997857905545896, 'epoch': 0.0}         \n",
      "{'loss': 0.8854, 'learning_rate': 0.0003999784479228615, 'epoch': 0.0}          \n",
      "{'loss': 0.8835, 'learning_rate': 0.00039997831639013235, 'epoch': 0.0}         \n",
      "{'loss': 0.8854, 'learning_rate': 0.0003999781844572718, 'epoch': 0.0}          \n",
      "{'loss': 0.8811, 'learning_rate': 0.00039997805212428, 'epoch': 0.0}            \n",
      "{'loss': 0.8828, 'learning_rate': 0.0003999779193911573, 'epoch': 0.0}          \n",
      "{'loss': 0.8854, 'learning_rate': 0.0003999777862579039, 'epoch': 0.0}          \n",
      "{'loss': 0.8848, 'learning_rate': 0.0003999776527245202, 'epoch': 0.0}          \n",
      "{'loss': 0.8824, 'learning_rate': 0.00039997751879100634, 'epoch': 0.0}         \n",
      "{'loss': 0.8863, 'learning_rate': 0.00039997738445736263, 'epoch': 0.0}         \n",
      "{'loss': 0.8822, 'learning_rate': 0.0003999772497235893, 'epoch': 0.0}          \n",
      "{'loss': 0.8838, 'learning_rate': 0.0003999771145896867, 'epoch': 0.0}          \n",
      "{'loss': 0.8826, 'learning_rate': 0.000399976979055655, 'epoch': 0.0}           \n",
      "{'loss': 0.8824, 'learning_rate': 0.00039997684312149457, 'epoch': 0.0}         \n",
      "{'loss': 0.8805, 'learning_rate': 0.0003999767067872056, 'epoch': 0.0}          \n",
      "{'loss': 0.88, 'learning_rate': 0.00039997657005278847, 'epoch': 0.0}           \n",
      "{'loss': 0.8886, 'learning_rate': 0.0003999764329182433, 'epoch': 0.0}          \n",
      "{'loss': 0.888, 'learning_rate': 0.00039997629538357054, 'epoch': 0.0}          \n",
      "{'loss': 0.8823, 'learning_rate': 0.00039997615744877035, 'epoch': 0.0}         \n",
      "{'loss': 0.8835, 'learning_rate': 0.000399976019113843, 'epoch': 0.0}           \n",
      "{'loss': 0.8816, 'learning_rate': 0.0003999758803787888, 'epoch': 0.0}          \n",
      "{'loss': 0.8801, 'learning_rate': 0.000399975741243608, 'epoch': 0.0}           \n",
      "{'loss': 0.8824, 'learning_rate': 0.000399975601708301, 'epoch': 0.0}           \n",
      "{'loss': 0.8787, 'learning_rate': 0.00039997546177286783, 'epoch': 0.0}         \n",
      "{'loss': 0.8777, 'learning_rate': 0.000399975321437309, 'epoch': 0.01}          \n",
      "{'loss': 0.8787, 'learning_rate': 0.0003999751807016248, 'epoch': 0.01}         \n",
      "{'loss': 0.8825, 'learning_rate': 0.0003999750395658153, 'epoch': 0.01}         \n",
      "{'loss': 0.8782, 'learning_rate': 0.00039997489802988097, 'epoch': 0.01}        \n",
      "{'loss': 0.877, 'learning_rate': 0.00039997475609382204, 'epoch': 0.01}         \n",
      "{'loss': 0.8815, 'learning_rate': 0.0003999746137576387, 'epoch': 0.01}         \n",
      "{'loss': 0.8842, 'learning_rate': 0.0003999744710213314, 'epoch': 0.01}         \n",
      "{'loss': 0.8807, 'learning_rate': 0.0003999743278849003, 'epoch': 0.01}         \n",
      "{'loss': 0.8819, 'learning_rate': 0.00039997418434834577, 'epoch': 0.01}        \n",
      "{'loss': 0.8808, 'learning_rate': 0.0003999740404116681, 'epoch': 0.01}         \n",
      "{'loss': 0.8752, 'learning_rate': 0.00039997389607486744, 'epoch': 0.01}        \n",
      "{'loss': 0.8831, 'learning_rate': 0.0003999737513379442, 'epoch': 0.01}         \n",
      "{'loss': 0.88, 'learning_rate': 0.0003999736062008987, 'epoch': 0.01}           \n",
      "{'loss': 0.8793, 'learning_rate': 0.00039997346066373115, 'epoch': 0.01}        \n",
      "{'loss': 0.879, 'learning_rate': 0.0003999733147264418, 'epoch': 0.01}          \n",
      "{'loss': 0.8819, 'learning_rate': 0.00039997316838903105, 'epoch': 0.01}        \n",
      "{'loss': 0.8767, 'learning_rate': 0.0003999730216514992, 'epoch': 0.01}         \n",
      "{'loss': 0.8799, 'learning_rate': 0.00039997287451384644, 'epoch': 0.01}        \n",
      "{'loss': 0.8802, 'learning_rate': 0.00039997272697607316, 'epoch': 0.01}        \n",
      "{'loss': 0.8836, 'learning_rate': 0.00039997257903817956, 'epoch': 0.01}        \n",
      "{'loss': 0.8922, 'learning_rate': 0.00039997243070016603, 'epoch': 0.01}        \n",
      "{'loss': 0.8983, 'learning_rate': 0.0003999722819620328, 'epoch': 0.01}         \n",
      "{'loss': 0.8862, 'learning_rate': 0.0003999721328237802, 'epoch': 0.01}         \n",
      "{'loss': 0.8806, 'learning_rate': 0.00039997198328540855, 'epoch': 0.01}        \n",
      "{'loss': 0.8822, 'learning_rate': 0.0003999718333469181, 'epoch': 0.01}         \n",
      "{'loss': 0.8819, 'learning_rate': 0.00039997168300830914, 'epoch': 0.01}        \n",
      "{'loss': 0.8768, 'learning_rate': 0.0003999715322695821, 'epoch': 0.01}         \n",
      "{'loss': 0.8778, 'learning_rate': 0.0003999713811307371, 'epoch': 0.01}         \n",
      "{'loss': 0.8774, 'learning_rate': 0.00039997122959177457, 'epoch': 0.01}        \n",
      "{'loss': 0.8763, 'learning_rate': 0.00039997107765269476, 'epoch': 0.01}        \n",
      "{'loss': 0.8758, 'learning_rate': 0.000399970925313498, 'epoch': 0.01}          \n",
      "{'loss': 0.8787, 'learning_rate': 0.0003999707725741845, 'epoch': 0.01}         \n",
      "{'loss': 0.8769, 'learning_rate': 0.0003999706194347547, 'epoch': 0.01}         \n",
      "{'loss': 0.8764, 'learning_rate': 0.0003999704658952089, 'epoch': 0.01}         \n",
      "{'loss': 0.8768, 'learning_rate': 0.0003999703119555473, 'epoch': 0.01}         \n",
      "{'loss': 0.874, 'learning_rate': 0.0003999701576157703, 'epoch': 0.01}          \n",
      "{'loss': 0.8752, 'learning_rate': 0.00039997000287587817, 'epoch': 0.01}        \n",
      "{'loss': 0.8761, 'learning_rate': 0.00039996984773587116, 'epoch': 0.01}        \n",
      "{'loss': 0.8744, 'learning_rate': 0.00039996969219574974, 'epoch': 0.01}        \n",
      "{'loss': 0.8767, 'learning_rate': 0.0003999695362555141, 'epoch': 0.01}         \n",
      "{'loss': 0.8731, 'learning_rate': 0.0003999693799151645, 'epoch': 0.01}         \n",
      "{'loss': 0.8743, 'learning_rate': 0.00039996922317470145, 'epoch': 0.01}        \n",
      "{'loss': 0.8774, 'learning_rate': 0.00039996906603412507, 'epoch': 0.01}        \n",
      "{'loss': 0.88, 'learning_rate': 0.00039996890849343576, 'epoch': 0.01}          \n",
      "{'loss': 0.8929, 'learning_rate': 0.00039996875055263383, 'epoch': 0.01}        \n",
      "{'loss': 0.8844, 'learning_rate': 0.0003999685922117196, 'epoch': 0.01}         \n",
      "{'loss': 0.8788, 'learning_rate': 0.00039996843347069337, 'epoch': 0.01}        \n",
      "{'loss': 0.8764, 'learning_rate': 0.00039996827432955544, 'epoch': 0.01}        \n",
      "{'loss': 0.876, 'learning_rate': 0.00039996811478830625, 'epoch': 0.01}         \n",
      "  1%|â–                              | 2000/351164 [2:42:47<477:20:13,  4.92s/it]/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1290: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|                                                   | 0/141 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/141 [00:01<02:12,  1.05it/s]\u001b[A\n",
      "  2%|â–‰                                          | 3/141 [00:04<03:19,  1.45s/it]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/141 [00:05<03:03,  1.34s/it]\u001b[A\n",
      "  4%|â–ˆâ–Œ                                         | 5/141 [00:07<03:42,  1.64s/it]\u001b[A\n",
      "  4%|â–ˆâ–Š                                         | 6/141 [00:09<04:02,  1.80s/it]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 7/141 [00:11<04:19,  1.93s/it]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–                                        | 8/141 [00:13<04:27,  2.01s/it]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–‹                                        | 9/141 [00:15<04:18,  1.96s/it]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–‰                                       | 10/141 [00:17<04:21,  2.00s/it]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–Ž                                      | 11/141 [00:19<04:22,  2.02s/it]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–Œ                                      | 12/141 [00:22<04:26,  2.07s/it]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–Š                                      | 13/141 [00:24<04:26,  2.08s/it]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 14/141 [00:26<04:29,  2.12s/it]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/141 [00:28<04:25,  2.11s/it]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 16/141 [00:30<04:23,  2.11s/it]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 17/141 [00:31<03:52,  1.87s/it]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 18/141 [00:34<04:01,  1.97s/it]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                    | 19/141 [00:36<04:07,  2.03s/it]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 20/141 [00:38<03:59,  1.98s/it]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   | 21/141 [00:40<04:07,  2.06s/it]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                   | 22/141 [00:42<04:11,  2.11s/it]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   | 23/141 [00:43<03:32,  1.80s/it]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 24/141 [00:45<03:43,  1.91s/it]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 25/141 [00:47<03:46,  1.95s/it]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                  | 26/141 [00:50<03:54,  2.04s/it]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 27/141 [00:52<03:56,  2.08s/it]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                 | 28/141 [00:54<03:58,  2.11s/it]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 29/141 [00:56<03:57,  2.12s/it]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 30/141 [00:58<03:54,  2.11s/it]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 31/141 [01:00<03:50,  2.09s/it]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 32/141 [01:03<03:51,  2.12s/it]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 33/141 [01:05<03:52,  2.15s/it]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 34/141 [01:07<03:49,  2.14s/it]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 35/141 [01:09<03:46,  2.14s/it]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 36/141 [01:11<03:42,  2.12s/it]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                               | 37/141 [01:13<03:42,  2.14s/it]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                              | 38/141 [01:15<03:42,  2.16s/it]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 39/141 [01:18<03:42,  2.18s/it]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 40/141 [01:20<03:44,  2.22s/it]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 41/141 [01:22<03:38,  2.19s/it]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 42/141 [01:24<03:38,  2.20s/it]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 43/141 [01:27<03:35,  2.20s/it]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 44/141 [01:29<03:31,  2.18s/it]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 45/141 [01:31<03:32,  2.21s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 46/141 [01:33<03:27,  2.18s/it]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 47/141 [01:34<02:54,  1.86s/it]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                           | 48/141 [01:36<03:00,  1.94s/it]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 49/141 [01:38<03:01,  1.97s/it]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 50/141 [01:40<03:03,  2.01s/it]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 51/141 [01:42<02:37,  1.75s/it]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 52/141 [01:43<02:20,  1.57s/it]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 53/141 [01:44<02:11,  1.50s/it]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 54/141 [01:46<02:27,  1.70s/it]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 55/141 [01:48<02:35,  1.81s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 56/141 [01:50<02:31,  1.78s/it]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 57/141 [01:52<02:42,  1.94s/it]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 58/141 [01:54<02:34,  1.87s/it]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 59/141 [01:56<02:38,  1.94s/it]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 60/141 [01:58<02:39,  1.97s/it]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 61/141 [02:00<02:41,  2.01s/it]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 62/141 [02:01<02:19,  1.77s/it]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 63/141 [02:03<02:07,  1.64s/it]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 64/141 [02:05<02:19,  1.81s/it]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 65/141 [02:07<02:24,  1.90s/it]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 66/141 [02:08<02:04,  1.67s/it]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 67/141 [02:10<02:13,  1.81s/it]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 68/141 [02:12<01:59,  1.64s/it]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 69/141 [02:14<02:11,  1.82s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 70/141 [02:16<02:16,  1.92s/it]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 71/141 [02:18<02:17,  1.96s/it]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 72/141 [02:20<02:18,  2.01s/it]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 73/141 [02:22<02:11,  1.93s/it]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 74/141 [02:24<02:13,  2.00s/it]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 75/141 [02:26<02:15,  2.05s/it]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 76/141 [02:28<02:01,  1.87s/it]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 77/141 [02:30<02:01,  1.90s/it]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 78/141 [02:31<01:45,  1.67s/it]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 79/141 [02:32<01:34,  1.52s/it]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 80/141 [02:33<01:25,  1.41s/it]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 81/141 [02:34<01:19,  1.32s/it]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 82/141 [02:36<01:30,  1.53s/it]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 83/141 [02:39<01:40,  1.73s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 84/141 [02:40<01:28,  1.55s/it]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 85/141 [02:41<01:24,  1.51s/it]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 86/141 [02:43<01:35,  1.74s/it]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 87/141 [02:46<01:41,  1.89s/it]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 88/141 [02:48<01:42,  1.94s/it]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 89/141 [02:49<01:28,  1.70s/it]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 90/141 [02:50<01:24,  1.66s/it]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 91/141 [02:53<01:31,  1.84s/it]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 92/141 [02:55<01:34,  1.93s/it]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 93/141 [02:57<01:36,  2.01s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 94/141 [02:59<01:36,  2.06s/it]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 95/141 [03:01<01:36,  2.09s/it]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 96/141 [03:03<01:34,  2.11s/it]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 97/141 [03:06<01:33,  2.14s/it]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 98/141 [03:08<01:31,  2.13s/it]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 99/141 [03:10<01:28,  2.12s/it]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 100/141 [03:12<01:28,  2.16s/it]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž           | 101/141 [03:14<01:28,  2.21s/it]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 102/141 [03:17<01:26,  2.21s/it]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 103/141 [03:19<01:21,  2.14s/it]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 104/141 [03:21<01:19,  2.14s/it]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 105/141 [03:22<01:12,  2.01s/it]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 106/141 [03:24<01:02,  1.78s/it]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 107/141 [03:26<01:03,  1.88s/it]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 108/141 [03:27<00:54,  1.66s/it]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 109/141 [03:29<00:58,  1.83s/it]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 110/141 [03:31<00:59,  1.92s/it]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 111/141 [03:33<00:58,  1.96s/it]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 112/141 [03:35<00:58,  2.01s/it]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 113/141 [03:37<00:49,  1.76s/it]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 114/141 [03:38<00:42,  1.59s/it]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 115/141 [03:40<00:45,  1.75s/it]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 116/141 [03:42<00:42,  1.70s/it]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 117/141 [03:43<00:36,  1.53s/it]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 118/141 [03:44<00:32,  1.41s/it]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 119/141 [03:46<00:35,  1.59s/it]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 120/141 [03:48<00:36,  1.73s/it]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/141 [03:49<00:31,  1.59s/it]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 122/141 [03:50<00:28,  1.48s/it]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 123/141 [03:51<00:24,  1.37s/it]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 124/141 [03:53<00:26,  1.54s/it]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 125/141 [03:55<00:23,  1.49s/it]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 126/141 [03:57<00:24,  1.61s/it]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 127/141 [03:58<00:22,  1.64s/it]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/141 [04:00<00:21,  1.64s/it]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 129/141 [04:01<00:18,  1.56s/it]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 130/141 [04:03<00:16,  1.48s/it]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 131/141 [04:04<00:13,  1.37s/it]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 132/141 [04:05<00:11,  1.29s/it]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 133/141 [04:06<00:09,  1.23s/it]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 134/141 [04:07<00:08,  1.21s/it]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 135/141 [04:09<00:08,  1.45s/it]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 136/141 [04:11<00:07,  1.54s/it]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 137/141 [04:12<00:05,  1.49s/it]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 138/141 [04:13<00:04,  1.39s/it]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 139/141 [04:15<00:02,  1.31s/it]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140/141 [04:16<00:01,  1.25s/it]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [04:18<00:00,  1.48s/it]\u001b[ACC[NH+](CC)[CH](CNC(=O)c1ccc2c(c1)C[CH](OC2=O)c3ccccc3)c4ccco4 CC[NH+](CC)[CH](CNC(=O)c1ccc2c(c1)C[CH](OC2=O)c3ccccc3)c4ccco4\n",
      "                                                                                \n",
      "\u001b[A{'eval_loss': 0.8680647015571594, 'eval_cer': 0.03909202843524632, 'eval_accuracy': 0.7628739850962073, 'eval_runtime': 268.3055, 'eval_samples_per_second': 33.51, 'eval_steps_per_second': 0.526, 'epoch': 0.01}\n",
      "  1%|â–                              | 2000/351164 [2:47:15<477:20:13,  4.92s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [04:19<00:00,  1.48s/it]\u001b[A\n",
      "                                                                                \u001b[ASome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 256}\n",
      "{'loss': 0.8757, 'learning_rate': 0.0003999679548469459, 'epoch': 0.01}         \n",
      "{'loss': 0.8742, 'learning_rate': 0.00039996779450547485, 'epoch': 0.01}        \n",
      "{'loss': 0.874, 'learning_rate': 0.00039996763376389346, 'epoch': 0.01}         \n",
      "{'loss': 0.8782, 'learning_rate': 0.000399967472622202, 'epoch': 0.01}          \n",
      "{'loss': 0.8743, 'learning_rate': 0.00039996731108040074, 'epoch': 0.01}        \n",
      "{'loss': 0.8755, 'learning_rate': 0.0003999671491384901, 'epoch': 0.01}         \n",
      "{'loss': 0.8739, 'learning_rate': 0.0003999669867964703, 'epoch': 0.01}         \n",
      "{'loss': 0.8815, 'learning_rate': 0.00039996682405434177, 'epoch': 0.01}        \n",
      "{'loss': 0.8745, 'learning_rate': 0.00039996666091210475, 'epoch': 0.01}        \n",
      "{'loss': 0.8773, 'learning_rate': 0.0003999664973697596, 'epoch': 0.01}         \n",
      "{'loss': 0.8736, 'learning_rate': 0.00039996633342730673, 'epoch': 0.01}        \n",
      "{'loss': 0.8733, 'learning_rate': 0.0003999661690847463, 'epoch': 0.01}         \n",
      "{'loss': 0.8739, 'learning_rate': 0.0003999660043420787, 'epoch': 0.01}         \n",
      "{'loss': 0.8733, 'learning_rate': 0.00039996583919930435, 'epoch': 0.01}        \n",
      "{'loss': 0.8749, 'learning_rate': 0.0003999656736564235, 'epoch': 0.01}         \n",
      "{'loss': 0.8738, 'learning_rate': 0.0003999655077134365, 'epoch': 0.01}         \n",
      "{'loss': 0.8756, 'learning_rate': 0.0003999653413703436, 'epoch': 0.01}         \n",
      "{'loss': 0.8755, 'learning_rate': 0.0003999651746271454, 'epoch': 0.01}         \n",
      "{'loss': 0.8757, 'learning_rate': 0.0003999650074838419, 'epoch': 0.01}         \n",
      "{'loss': 0.8778, 'learning_rate': 0.0003999648399404336, 'epoch': 0.01}         \n",
      "{'loss': 0.8783, 'learning_rate': 0.00039996467199692083, 'epoch': 0.01}        \n",
      "{'loss': 0.8753, 'learning_rate': 0.0003999645036533039, 'epoch': 0.01}         \n",
      "{'loss': 0.8771, 'learning_rate': 0.0003999643349095832, 'epoch': 0.01}         \n",
      "{'loss': 0.8773, 'learning_rate': 0.00039996416576575897, 'epoch': 0.01}        \n",
      "{'loss': 0.877, 'learning_rate': 0.0003999639962218316, 'epoch': 0.01}          \n",
      "{'loss': 0.8763, 'learning_rate': 0.0003999638262778015, 'epoch': 0.01}         \n",
      "{'loss': 0.8737, 'learning_rate': 0.0003999636559336689, 'epoch': 0.01}         \n",
      "{'loss': 0.8702, 'learning_rate': 0.0003999634851894342, 'epoch': 0.01}         \n",
      "{'loss': 0.8735, 'learning_rate': 0.0003999633140450977, 'epoch': 0.01}         \n",
      "{'loss': 0.8708, 'learning_rate': 0.0003999631425006598, 'epoch': 0.01}         \n",
      "{'loss': 0.8708, 'learning_rate': 0.0003999629705561208, 'epoch': 0.01}         \n",
      "{'loss': 0.8708, 'learning_rate': 0.00039996279821148103, 'epoch': 0.01}        \n",
      "{'loss': 0.8703, 'learning_rate': 0.00039996262546674086, 'epoch': 0.01}        \n",
      "{'loss': 0.8713, 'learning_rate': 0.0003999624523219007, 'epoch': 0.01}         \n",
      "{'loss': 0.8712, 'learning_rate': 0.00039996227877696073, 'epoch': 0.01}        \n",
      "{'loss': 0.8688, 'learning_rate': 0.0003999621048319215, 'epoch': 0.01}         \n",
      "{'loss': 0.8712, 'learning_rate': 0.0003999619304867833, 'epoch': 0.01}         \n",
      "{'loss': 0.8722, 'learning_rate': 0.00039996175574154633, 'epoch': 0.01}        \n",
      "{'loss': 0.869, 'learning_rate': 0.00039996158059621103, 'epoch': 0.01}         \n",
      "{'loss': 0.8671, 'learning_rate': 0.0003999614050507778, 'epoch': 0.01}         \n",
      "{'loss': 0.8684, 'learning_rate': 0.000399961229105247, 'epoch': 0.01}          \n",
      "{'loss': 0.8701, 'learning_rate': 0.0003999610527596189, 'epoch': 0.01}         \n",
      "{'loss': 0.8701, 'learning_rate': 0.0003999608760138939, 'epoch': 0.01}         \n",
      "{'loss': 0.8701, 'learning_rate': 0.00039996069886807234, 'epoch': 0.01}        \n",
      "{'loss': 0.8699, 'learning_rate': 0.0003999605213221546, 'epoch': 0.01}         \n",
      "{'loss': 0.8724, 'learning_rate': 0.000399960343376141, 'epoch': 0.01}          \n",
      "{'loss': 0.8701, 'learning_rate': 0.0003999601650300319, 'epoch': 0.01}         \n",
      "{'loss': 0.8785, 'learning_rate': 0.0003999599862838277, 'epoch': 0.01}         \n",
      "{'loss': 0.8769, 'learning_rate': 0.0003999598071375287, 'epoch': 0.01}         \n",
      "{'loss': 0.8724, 'learning_rate': 0.0003999596275911353, 'epoch': 0.01}         \n",
      "{'loss': 0.8733, 'learning_rate': 0.0003999594476446479, 'epoch': 0.01}         \n",
      "{'loss': 0.8695, 'learning_rate': 0.0003999592672980667, 'epoch': 0.01}         \n",
      "{'loss': 0.8691, 'learning_rate': 0.0003999590865513922, 'epoch': 0.01}         \n",
      "{'loss': 0.874, 'learning_rate': 0.00039995890540462476, 'epoch': 0.01}         \n",
      "{'loss': 0.8791, 'learning_rate': 0.0003999587238577647, 'epoch': 0.01}         \n",
      "{'loss': 0.8784, 'learning_rate': 0.00039995854191081235, 'epoch': 0.01}        \n",
      "{'loss': 0.8751, 'learning_rate': 0.0003999583595637682, 'epoch': 0.01}         \n",
      "{'loss': 0.8703, 'learning_rate': 0.00039995817681663247, 'epoch': 0.01}        \n",
      "{'loss': 0.8685, 'learning_rate': 0.00039995799366940554, 'epoch': 0.01}        \n",
      "{'loss': 0.8697, 'learning_rate': 0.00039995781012208795, 'epoch': 0.01}        \n",
      "{'loss': 0.8704, 'learning_rate': 0.00039995762617467986, 'epoch': 0.01}        \n",
      "{'loss': 0.8679, 'learning_rate': 0.0003999574418271817, 'epoch': 0.01}         \n",
      "{'loss': 0.8696, 'learning_rate': 0.00039995725707959393, 'epoch': 0.01}        \n",
      "{'loss': 0.8677, 'learning_rate': 0.00039995707193191685, 'epoch': 0.01}        \n",
      "{'loss': 0.8686, 'learning_rate': 0.00039995688638415073, 'epoch': 0.01}        \n",
      "{'loss': 0.867, 'learning_rate': 0.00039995670043629607, 'epoch': 0.01}         \n",
      "{'loss': 0.8684, 'learning_rate': 0.00039995651408835324, 'epoch': 0.01}        \n",
      "{'loss': 0.8676, 'learning_rate': 0.00039995632734032263, 'epoch': 0.01}        \n",
      "{'loss': 0.8707, 'learning_rate': 0.0003999561401922045, 'epoch': 0.01}         \n",
      "{'loss': 0.8709, 'learning_rate': 0.00039995595264399935, 'epoch': 0.01}        \n",
      "{'loss': 0.8748, 'learning_rate': 0.00039995576469570743, 'epoch': 0.01}        \n",
      "{'loss': 0.8732, 'learning_rate': 0.0003999555763473292, 'epoch': 0.01}         \n",
      "{'loss': 0.8717, 'learning_rate': 0.000399955387598865, 'epoch': 0.01}          \n",
      "{'loss': 0.8702, 'learning_rate': 0.00039995519845031533, 'epoch': 0.01}        \n",
      "{'loss': 0.8689, 'learning_rate': 0.0003999550089016804, 'epoch': 0.01}         \n",
      "{'loss': 0.8669, 'learning_rate': 0.0003999548189529606, 'epoch': 0.01}         \n",
      "{'loss': 0.8666, 'learning_rate': 0.00039995462860415647, 'epoch': 0.01}        \n",
      "{'loss': 0.8695, 'learning_rate': 0.00039995443785526826, 'epoch': 0.01}        \n",
      "{'loss': 0.8689, 'learning_rate': 0.00039995424670629634, 'epoch': 0.01}        \n",
      "{'loss': 0.869, 'learning_rate': 0.00039995405515724113, 'epoch': 0.01}         \n",
      "{'loss': 0.8683, 'learning_rate': 0.00039995386320810307, 'epoch': 0.01}        \n",
      "{'loss': 0.8657, 'learning_rate': 0.0003999536708588824, 'epoch': 0.01}         \n",
      "{'loss': 0.8659, 'learning_rate': 0.0003999534781095797, 'epoch': 0.01}         \n",
      "{'loss': 0.8698, 'learning_rate': 0.0003999532849601952, 'epoch': 0.01}         \n",
      "{'loss': 0.8691, 'learning_rate': 0.00039995309141072934, 'epoch': 0.01}        \n",
      "{'loss': 0.8679, 'learning_rate': 0.00039995289746118243, 'epoch': 0.01}        \n",
      "{'loss': 0.867, 'learning_rate': 0.000399952703111555, 'epoch': 0.01}           \n",
      "{'loss': 0.8669, 'learning_rate': 0.0003999525083618474, 'epoch': 0.01}         \n",
      "{'loss': 0.8679, 'learning_rate': 0.00039995231321205993, 'epoch': 0.01}        \n",
      "{'loss': 0.8665, 'learning_rate': 0.00039995211766219305, 'epoch': 0.01}        \n",
      "{'loss': 0.8698, 'learning_rate': 0.00039995192171224717, 'epoch': 0.01}        \n",
      "{'loss': 0.8795, 'learning_rate': 0.00039995172536222266, 'epoch': 0.01}        \n",
      "{'loss': 0.8702, 'learning_rate': 0.00039995152861211985, 'epoch': 0.01}        \n",
      "{'loss': 0.867, 'learning_rate': 0.00039995133146193923, 'epoch': 0.01}         \n",
      "{'loss': 0.8669, 'learning_rate': 0.0003999511339116811, 'epoch': 0.01}         \n",
      "{'loss': 0.868, 'learning_rate': 0.00039995093596134596, 'epoch': 0.01}         \n",
      "{'loss': 0.867, 'learning_rate': 0.0003999507376109342, 'epoch': 0.01}          \n",
      "{'loss': 0.8667, 'learning_rate': 0.0003999505388604461, 'epoch': 0.01}         \n",
      "{'loss': 0.8647, 'learning_rate': 0.0003999503397098822, 'epoch': 0.01}         \n",
      "{'loss': 0.8672, 'learning_rate': 0.00039995014015924276, 'epoch': 0.01}        \n",
      "{'loss': 0.865, 'learning_rate': 0.00039994994020852825, 'epoch': 0.01}         \n",
      "{'loss': 0.8704, 'learning_rate': 0.0003999497398577391, 'epoch': 0.01}         \n",
      "{'loss': 0.8799, 'learning_rate': 0.00039994953910687565, 'epoch': 0.01}        \n",
      "{'loss': 0.8709, 'learning_rate': 0.0003999493379559384, 'epoch': 0.01}         \n",
      "{'loss': 0.8682, 'learning_rate': 0.00039994913640492764, 'epoch': 0.01}        \n",
      "{'loss': 0.8668, 'learning_rate': 0.00039994893445384374, 'epoch': 0.01}        \n",
      "{'loss': 0.8699, 'learning_rate': 0.0003999487321026873, 'epoch': 0.01}         \n",
      "{'loss': 0.8672, 'learning_rate': 0.0003999485293514585, 'epoch': 0.01}         \n",
      "{'loss': 0.8685, 'learning_rate': 0.00039994832620015794, 'epoch': 0.01}        \n",
      "{'loss': 0.8698, 'learning_rate': 0.00039994812264878594, 'epoch': 0.01}        \n",
      "{'loss': 0.8691, 'learning_rate': 0.0003999479186973428, 'epoch': 0.01}         \n",
      "{'loss': 0.869, 'learning_rate': 0.0003999477143458291, 'epoch': 0.01}          \n",
      "{'loss': 0.8712, 'learning_rate': 0.00039994750959424516, 'epoch': 0.01}        \n",
      "{'loss': 0.8738, 'learning_rate': 0.00039994730444259147, 'epoch': 0.01}        \n",
      "{'loss': 0.8674, 'learning_rate': 0.00039994709889086837, 'epoch': 0.01}        \n",
      "{'loss': 0.8652, 'learning_rate': 0.00039994689293907623, 'epoch': 0.01}        \n",
      "{'loss': 0.8669, 'learning_rate': 0.00039994668658721556, 'epoch': 0.01}        \n",
      "{'loss': 0.8643, 'learning_rate': 0.00039994647983528667, 'epoch': 0.01}        \n",
      "{'loss': 0.8727, 'learning_rate': 0.0003999462726832901, 'epoch': 0.01}         \n",
      "{'loss': 0.8695, 'learning_rate': 0.00039994606513122613, 'epoch': 0.01}        \n",
      "{'loss': 0.8695, 'learning_rate': 0.0003999458571790953, 'epoch': 0.01}         \n",
      "{'loss': 0.868, 'learning_rate': 0.00039994564882689794, 'epoch': 0.01}         \n",
      "{'loss': 0.8649, 'learning_rate': 0.00039994544007463447, 'epoch': 0.01}        \n",
      "{'loss': 0.866, 'learning_rate': 0.00039994523092230544, 'epoch': 0.01}         \n",
      "{'loss': 0.8641, 'learning_rate': 0.00039994502136991107, 'epoch': 0.01}        \n",
      "{'loss': 0.8649, 'learning_rate': 0.0003999448114174518, 'epoch': 0.01}         \n",
      "{'loss': 0.8637, 'learning_rate': 0.00039994460106492825, 'epoch': 0.01}        \n",
      "{'loss': 0.8639, 'learning_rate': 0.00039994439031234067, 'epoch': 0.01}        \n",
      "{'loss': 0.862, 'learning_rate': 0.0003999441791596895, 'epoch': 0.01}          \n",
      "{'loss': 0.8629, 'learning_rate': 0.0003999439676069752, 'epoch': 0.01}         \n",
      "{'loss': 0.8635, 'learning_rate': 0.00039994375565419817, 'epoch': 0.01}        \n",
      "{'loss': 0.8625, 'learning_rate': 0.00039994354330135886, 'epoch': 0.01}        \n",
      "{'loss': 0.8642, 'learning_rate': 0.0003999433305484577, 'epoch': 0.01}         \n",
      "{'loss': 0.8661, 'learning_rate': 0.00039994311739549506, 'epoch': 0.01}        \n",
      "{'loss': 0.8678, 'learning_rate': 0.0003999429038424713, 'epoch': 0.01}         \n",
      "{'loss': 0.8656, 'learning_rate': 0.00039994268988938713, 'epoch': 0.01}        \n",
      "{'loss': 0.867, 'learning_rate': 0.0003999424755362427, 'epoch': 0.01}          \n",
      "{'loss': 0.8647, 'learning_rate': 0.00039994226078303853, 'epoch': 0.01}        \n",
      "{'loss': 0.8676, 'learning_rate': 0.00039994204562977505, 'epoch': 0.01}        \n",
      "{'loss': 0.8669, 'learning_rate': 0.00039994183007645274, 'epoch': 0.01}        \n",
      "{'loss': 0.8717, 'learning_rate': 0.00039994161412307196, 'epoch': 0.01}        \n",
      "{'loss': 0.8674, 'learning_rate': 0.00039994139776963316, 'epoch': 0.01}        \n",
      "{'loss': 0.8659, 'learning_rate': 0.00039994118101613677, 'epoch': 0.01}        \n",
      "{'loss': 0.865, 'learning_rate': 0.0003999409638625833, 'epoch': 0.01}          \n",
      "{'loss': 0.8645, 'learning_rate': 0.00039994074630897306, 'epoch': 0.01}        \n",
      "{'loss': 0.8643, 'learning_rate': 0.0003999405283553065, 'epoch': 0.01}         \n",
      "{'loss': 0.8635, 'learning_rate': 0.0003999403100015842, 'epoch': 0.01}         \n",
      "{'loss': 0.8642, 'learning_rate': 0.00039994009124780645, 'epoch': 0.01}        \n",
      "{'loss': 0.8644, 'learning_rate': 0.00039993987209397373, 'epoch': 0.01}        \n",
      "{'loss': 0.8676, 'learning_rate': 0.00039993965254008645, 'epoch': 0.01}        \n",
      "{'loss': 0.8675, 'learning_rate': 0.00039993943258614516, 'epoch': 0.01}        \n",
      "{'loss': 0.8639, 'learning_rate': 0.0003999392122321502, 'epoch': 0.01}         \n",
      "{'loss': 0.8664, 'learning_rate': 0.000399938991478102, 'epoch': 0.01}          \n",
      "{'loss': 0.866, 'learning_rate': 0.000399938770324001, 'epoch': 0.01}           \n",
      "{'loss': 0.8665, 'learning_rate': 0.0003999385487698478, 'epoch': 0.01}         \n",
      "{'loss': 0.863, 'learning_rate': 0.0003999383268156426, 'epoch': 0.01}          \n",
      "{'loss': 0.8677, 'learning_rate': 0.000399938104461386, 'epoch': 0.01}          \n",
      "{'loss': 0.8642, 'learning_rate': 0.00039993788170707845, 'epoch': 0.01}        \n",
      "{'loss': 0.8646, 'learning_rate': 0.0003999376585527203, 'epoch': 0.01}         \n",
      "{'loss': 0.8646, 'learning_rate': 0.00039993743499831206, 'epoch': 0.01}        \n",
      "{'loss': 0.8647, 'learning_rate': 0.0003999372110438542, 'epoch': 0.01}         \n",
      "{'loss': 0.866, 'learning_rate': 0.00039993698668934713, 'epoch': 0.01}         \n",
      "{'loss': 0.8664, 'learning_rate': 0.0003999367619347913, 'epoch': 0.01}         \n",
      "{'loss': 0.8664, 'learning_rate': 0.0003999365367801872, 'epoch': 0.01}         \n",
      "{'loss': 0.8638, 'learning_rate': 0.0003999363112255352, 'epoch': 0.01}         \n",
      "{'loss': 0.864, 'learning_rate': 0.0003999360852708358, 'epoch': 0.01}          \n",
      "{'loss': 0.8641, 'learning_rate': 0.00039993585891608943, 'epoch': 0.01}        \n",
      "{'loss': 0.8632, 'learning_rate': 0.00039993563216129664, 'epoch': 0.01}        \n",
      "{'loss': 0.8649, 'learning_rate': 0.0003999354050064577, 'epoch': 0.01}         \n",
      "{'loss': 0.8673, 'learning_rate': 0.00039993517745157326, 'epoch': 0.01}        \n",
      "{'loss': 0.8666, 'learning_rate': 0.00039993494949664365, 'epoch': 0.01}        \n",
      "{'loss': 0.8721, 'learning_rate': 0.00039993472114166934, 'epoch': 0.01}        \n",
      "{'loss': 0.8793, 'learning_rate': 0.0003999344923866509, 'epoch': 0.01}         \n",
      "{'loss': 0.8717, 'learning_rate': 0.0003999342632315886, 'epoch': 0.01}         \n",
      "{'loss': 0.8671, 'learning_rate': 0.00039993403367648305, 'epoch': 0.01}        \n",
      "{'loss': 0.8649, 'learning_rate': 0.0003999338037213346, 'epoch': 0.01}         \n",
      "{'loss': 0.8628, 'learning_rate': 0.00039993357336614384, 'epoch': 0.01}        \n",
      "{'loss': 0.8616, 'learning_rate': 0.0003999333426109111, 'epoch': 0.01}         \n",
      "{'loss': 0.8656, 'learning_rate': 0.0003999331114556369, 'epoch': 0.01}         \n",
      "{'loss': 0.8611, 'learning_rate': 0.00039993287990032174, 'epoch': 0.01}        \n",
      "{'loss': 0.8642, 'learning_rate': 0.000399932647944966, 'epoch': 0.01}          \n",
      "{'loss': 0.8638, 'learning_rate': 0.0003999324155895702, 'epoch': 0.01}         \n",
      "{'loss': 0.8601, 'learning_rate': 0.0003999321828341348, 'epoch': 0.01}         \n",
      "{'loss': 0.8627, 'learning_rate': 0.0003999319496786603, 'epoch': 0.01}         \n",
      "{'loss': 0.8641, 'learning_rate': 0.00039993171612314707, 'epoch': 0.01}        \n",
      "{'loss': 0.862, 'learning_rate': 0.0003999314821675956, 'epoch': 0.01}          \n",
      "{'loss': 0.8636, 'learning_rate': 0.00039993124781200645, 'epoch': 0.01}        \n",
      "{'loss': 0.8637, 'learning_rate': 0.00039993101305638005, 'epoch': 0.01}        \n",
      "{'loss': 0.8614, 'learning_rate': 0.00039993077790071677, 'epoch': 0.01}        \n",
      "{'loss': 0.8616, 'learning_rate': 0.0003999305423450172, 'epoch': 0.01}         \n",
      "{'loss': 0.8618, 'learning_rate': 0.0003999303063892818, 'epoch': 0.01}         \n",
      "{'loss': 0.8637, 'learning_rate': 0.000399930070033511, 'epoch': 0.01}          \n",
      "{'loss': 0.8611, 'learning_rate': 0.00039992983327770523, 'epoch': 0.01}        \n",
      "{'loss': 0.8637, 'learning_rate': 0.00039992959612186507, 'epoch': 0.01}        \n",
      "{'loss': 0.8782, 'learning_rate': 0.0003999293585659909, 'epoch': 0.01}         \n",
      "{'loss': 0.8685, 'learning_rate': 0.0003999291206100833, 'epoch': 0.01}         \n",
      "{'loss': 0.8649, 'learning_rate': 0.0003999288822541426, 'epoch': 0.01}         \n",
      "{'loss': 0.8632, 'learning_rate': 0.0003999286434981695, 'epoch': 0.01}         \n",
      "{'loss': 0.8606, 'learning_rate': 0.0003999284043421642, 'epoch': 0.01}         \n",
      "{'loss': 0.8627, 'learning_rate': 0.0003999281647861274, 'epoch': 0.01}         \n",
      "{'loss': 0.8634, 'learning_rate': 0.0003999279248300595, 'epoch': 0.01}         \n",
      "{'loss': 0.8615, 'learning_rate': 0.0003999276844739609, 'epoch': 0.01}         \n",
      "{'loss': 0.8614, 'learning_rate': 0.00039992744371783223, 'epoch': 0.01}        \n",
      "{'loss': 0.8633, 'learning_rate': 0.0003999272025616739, 'epoch': 0.01}         \n",
      "{'loss': 0.8628, 'learning_rate': 0.0003999269610054864, 'epoch': 0.01}         \n",
      "{'loss': 0.8661, 'learning_rate': 0.00039992671904927015, 'epoch': 0.01}        \n",
      "{'loss': 0.8639, 'learning_rate': 0.00039992647669302576, 'epoch': 0.01}        \n",
      "{'loss': 0.8617, 'learning_rate': 0.0003999262339367536, 'epoch': 0.01}         \n",
      "{'loss': 0.8637, 'learning_rate': 0.00039992599078045413, 'epoch': 0.01}        \n",
      "{'loss': 0.8646, 'learning_rate': 0.00039992574722412804, 'epoch': 0.01}        \n",
      "{'loss': 0.8609, 'learning_rate': 0.00039992550326777563, 'epoch': 0.01}        \n",
      "{'loss': 0.8624, 'learning_rate': 0.0003999252589113974, 'epoch': 0.01}         \n",
      "{'loss': 0.862, 'learning_rate': 0.00039992501415499394, 'epoch': 0.01}         \n",
      "{'loss': 0.8631, 'learning_rate': 0.0003999247689985657, 'epoch': 0.01}         \n",
      "{'loss': 0.861, 'learning_rate': 0.0003999245234421131, 'epoch': 0.01}          \n",
      "{'loss': 0.8622, 'learning_rate': 0.0003999242774856367, 'epoch': 0.01}         \n",
      "{'loss': 0.8631, 'learning_rate': 0.000399924031129137, 'epoch': 0.01}          \n",
      "{'loss': 0.8615, 'learning_rate': 0.00039992378437261444, 'epoch': 0.01}        \n",
      "{'loss': 0.8614, 'learning_rate': 0.00039992353721606953, 'epoch': 0.01}        \n",
      "{'loss': 0.8615, 'learning_rate': 0.00039992328965950276, 'epoch': 0.01}        \n",
      "{'loss': 0.8623, 'learning_rate': 0.0003999230417029147, 'epoch': 0.01}         \n",
      "{'loss': 0.863, 'learning_rate': 0.0003999227933463058, 'epoch': 0.01}          \n",
      "{'loss': 0.8645, 'learning_rate': 0.0003999225445896765, 'epoch': 0.01}         \n",
      "{'loss': 0.8606, 'learning_rate': 0.00039992229543302736, 'epoch': 0.01}        \n",
      "{'loss': 0.8609, 'learning_rate': 0.00039992204587635885, 'epoch': 0.01}        \n",
      "{'loss': 0.8613, 'learning_rate': 0.0003999217959196715, 'epoch': 0.01}         \n",
      "{'loss': 0.8635, 'learning_rate': 0.0003999215455629658, 'epoch': 0.01}         \n",
      "{'loss': 0.8627, 'learning_rate': 0.0003999212948062422, 'epoch': 0.01}         \n",
      "{'loss': 0.8607, 'learning_rate': 0.00039992104364950124, 'epoch': 0.01}        \n",
      "{'loss': 0.8614, 'learning_rate': 0.0003999207920927435, 'epoch': 0.01}         \n",
      "{'loss': 0.8619, 'learning_rate': 0.0003999205401359693, 'epoch': 0.01}         \n",
      "{'loss': 0.8629, 'learning_rate': 0.00039992028777917934, 'epoch': 0.01}        \n",
      "{'loss': 0.8628, 'learning_rate': 0.000399920035022374, 'epoch': 0.01}          \n",
      "{'loss': 0.8589, 'learning_rate': 0.0003999197818655539, 'epoch': 0.01}         \n",
      "{'loss': 0.8606, 'learning_rate': 0.00039991952830871935, 'epoch': 0.01}        \n",
      "{'loss': 0.8599, 'learning_rate': 0.0003999192743518711, 'epoch': 0.01}         \n",
      "{'loss': 0.8598, 'learning_rate': 0.00039991901999500944, 'epoch': 0.01}        \n",
      "{'loss': 0.8576, 'learning_rate': 0.000399918765238135, 'epoch': 0.01}          \n",
      "{'loss': 0.8609, 'learning_rate': 0.00039991851008124826, 'epoch': 0.01}        \n",
      "{'loss': 0.8605, 'learning_rate': 0.0003999182545243498, 'epoch': 0.01}         \n",
      "{'loss': 0.8613, 'learning_rate': 0.00039991799856744, 'epoch': 0.01}           \n",
      "{'loss': 0.8632, 'learning_rate': 0.00039991774221051946, 'epoch': 0.01}        \n",
      "{'loss': 0.8612, 'learning_rate': 0.0003999174854535887, 'epoch': 0.01}         \n",
      "{'loss': 0.8597, 'learning_rate': 0.00039991722829664815, 'epoch': 0.01}        \n",
      "{'loss': 0.8601, 'learning_rate': 0.0003999169707396984, 'epoch': 0.01}         \n",
      "{'loss': 0.8601, 'learning_rate': 0.00039991671278273995, 'epoch': 0.01}        \n",
      "{'loss': 0.8601, 'learning_rate': 0.00039991645442577327, 'epoch': 0.01}        \n",
      "{'loss': 0.8619, 'learning_rate': 0.000399916195668799, 'epoch': 0.01}          \n",
      "{'loss': 0.8748, 'learning_rate': 0.0003999159365118175, 'epoch': 0.01}         \n",
      "{'loss': 0.8693, 'learning_rate': 0.0003999156769548294, 'epoch': 0.01}         \n",
      "{'loss': 0.8638, 'learning_rate': 0.00039991541699783517, 'epoch': 0.01}        \n",
      "{'loss': 0.8616, 'learning_rate': 0.0003999151566408354, 'epoch': 0.01}         \n",
      "{'loss': 0.8609, 'learning_rate': 0.00039991489588383047, 'epoch': 0.01}        \n",
      "{'loss': 0.8614, 'learning_rate': 0.00039991463472682104, 'epoch': 0.01}        \n",
      "{'loss': 0.8586, 'learning_rate': 0.0003999143731698076, 'epoch': 0.01}         \n",
      "{'loss': 0.8578, 'learning_rate': 0.0003999141112127906, 'epoch': 0.01}         \n",
      "{'loss': 0.8607, 'learning_rate': 0.00039991384885577067, 'epoch': 0.01}        \n",
      "{'loss': 0.8607, 'learning_rate': 0.0003999135860987482, 'epoch': 0.01}         \n",
      "{'loss': 0.8586, 'learning_rate': 0.0003999133229417239, 'epoch': 0.01}         \n",
      "{'loss': 0.8598, 'learning_rate': 0.00039991305938469817, 'epoch': 0.01}        \n",
      "{'loss': 0.8583, 'learning_rate': 0.0003999127954276715, 'epoch': 0.01}         \n",
      "{'loss': 0.858, 'learning_rate': 0.00039991253107064453, 'epoch': 0.01}         \n",
      "{'loss': 0.8574, 'learning_rate': 0.0003999122663136177, 'epoch': 0.01}         \n",
      "{'loss': 0.8574, 'learning_rate': 0.00039991200115659163, 'epoch': 0.01}        \n",
      "{'loss': 0.8616, 'learning_rate': 0.0003999117355995668, 'epoch': 0.01}         \n",
      "{'loss': 0.8593, 'learning_rate': 0.0003999114696425437, 'epoch': 0.01}         \n",
      "{'loss': 0.8603, 'learning_rate': 0.00039991120328552294, 'epoch': 0.01}        \n",
      "{'loss': 0.859, 'learning_rate': 0.000399910936528505, 'epoch': 0.01}           \n",
      "{'loss': 0.8607, 'learning_rate': 0.0003999106693714904, 'epoch': 0.01}         \n",
      "{'loss': 0.859, 'learning_rate': 0.0003999104018144797, 'epoch': 0.01}          \n",
      "{'loss': 0.8591, 'learning_rate': 0.0003999101338574735, 'epoch': 0.01}         \n",
      "{'loss': 0.8612, 'learning_rate': 0.00039990986550047223, 'epoch': 0.01}        \n",
      "{'loss': 0.8583, 'learning_rate': 0.0003999095967434765, 'epoch': 0.01}         \n",
      "{'loss': 0.8582, 'learning_rate': 0.0003999093275864868, 'epoch': 0.01}         \n",
      "{'loss': 0.8612, 'learning_rate': 0.00039990905802950367, 'epoch': 0.01}        \n",
      "{'loss': 0.8614, 'learning_rate': 0.00039990878807252773, 'epoch': 0.01}        \n",
      "{'loss': 0.8588, 'learning_rate': 0.00039990851771555937, 'epoch': 0.01}        \n",
      "{'loss': 0.8572, 'learning_rate': 0.0003999082469585993, 'epoch': 0.01}         \n",
      "{'loss': 0.8577, 'learning_rate': 0.0003999079758016479, 'epoch': 0.01}         \n",
      "{'loss': 0.8582, 'learning_rate': 0.00039990770424470586, 'epoch': 0.01}        \n",
      "{'loss': 0.8601, 'learning_rate': 0.00039990743228777363, 'epoch': 0.01}        \n",
      "{'loss': 0.8594, 'learning_rate': 0.00039990715993085184, 'epoch': 0.01}        \n",
      "{'loss': 0.858, 'learning_rate': 0.0003999068871739409, 'epoch': 0.01}          \n",
      "{'loss': 0.8609, 'learning_rate': 0.0003999066140170415, 'epoch': 0.01}         \n",
      "{'loss': 0.8597, 'learning_rate': 0.00039990634046015405, 'epoch': 0.01}        \n",
      "{'loss': 0.8601, 'learning_rate': 0.0003999060665032792, 'epoch': 0.01}         \n",
      "{'loss': 0.8606, 'learning_rate': 0.0003999057921464174, 'epoch': 0.01}         \n",
      "{'loss': 0.8587, 'learning_rate': 0.0003999055173895693, 'epoch': 0.01}         \n",
      "{'loss': 0.8575, 'learning_rate': 0.00039990524223273543, 'epoch': 0.01}        \n",
      "{'loss': 0.8588, 'learning_rate': 0.00039990496667591637, 'epoch': 0.01}        \n",
      "{'loss': 0.8589, 'learning_rate': 0.0003999046907191126, 'epoch': 0.01}         \n",
      "{'loss': 0.8613, 'learning_rate': 0.00039990441436232464, 'epoch': 0.01}        \n",
      "{'loss': 0.8584, 'learning_rate': 0.0003999041376055531, 'epoch': 0.01}         \n",
      "{'loss': 0.8582, 'learning_rate': 0.0003999038604487986, 'epoch': 0.01}         \n",
      "{'loss': 0.8573, 'learning_rate': 0.0003999035828920616, 'epoch': 0.01}         \n",
      "{'loss': 0.8576, 'learning_rate': 0.00039990330493534267, 'epoch': 0.01}        \n",
      "{'loss': 0.8568, 'learning_rate': 0.00039990302657864235, 'epoch': 0.01}        \n",
      "{'loss': 0.8577, 'learning_rate': 0.0003999027478219613, 'epoch': 0.01}         \n",
      "{'loss': 0.856, 'learning_rate': 0.0003999024686652999, 'epoch': 0.01}          \n",
      "{'loss': 0.8569, 'learning_rate': 0.00039990218910865896, 'epoch': 0.01}        \n",
      "{'loss': 0.8557, 'learning_rate': 0.00039990190915203873, 'epoch': 0.01}        \n",
      "{'loss': 0.855, 'learning_rate': 0.00039990162879544007, 'epoch': 0.01}         \n",
      "{'loss': 0.8541, 'learning_rate': 0.0003999013480388634, 'epoch': 0.01}         \n",
      "{'loss': 0.8569, 'learning_rate': 0.00039990106688230913, 'epoch': 0.01}        \n",
      "{'loss': 0.8574, 'learning_rate': 0.00039990078532577806, 'epoch': 0.01}        \n",
      "{'loss': 0.8566, 'learning_rate': 0.00039990050336927076, 'epoch': 0.01}        \n",
      "{'loss': 0.8572, 'learning_rate': 0.0003999002210127876, 'epoch': 0.01}         \n",
      "{'loss': 0.8557, 'learning_rate': 0.0003998999382563293, 'epoch': 0.01}         \n",
      "{'loss': 0.8577, 'learning_rate': 0.00039989965509989637, 'epoch': 0.01}        \n",
      "{'loss': 0.859, 'learning_rate': 0.00039989937154348934, 'epoch': 0.01}         \n",
      "{'loss': 0.8557, 'learning_rate': 0.0003998990875871089, 'epoch': 0.01}         \n",
      "{'loss': 0.8559, 'learning_rate': 0.00039989880323075544, 'epoch': 0.01}        \n",
      "{'loss': 0.8568, 'learning_rate': 0.00039989851847442966, 'epoch': 0.01}        \n",
      "{'loss': 0.8564, 'learning_rate': 0.0003998982333181321, 'epoch': 0.01}         \n",
      "{'loss': 0.8576, 'learning_rate': 0.00039989794776186337, 'epoch': 0.01}        \n",
      "{'loss': 0.857, 'learning_rate': 0.0003998976618056239, 'epoch': 0.01}          \n",
      "{'loss': 0.8594, 'learning_rate': 0.0003998973754494145, 'epoch': 0.01}         \n",
      "{'loss': 0.8614, 'learning_rate': 0.0003998970886932355, 'epoch': 0.01}         \n",
      "{'loss': 0.8556, 'learning_rate': 0.0003998968015370876, 'epoch': 0.01}         \n",
      "{'loss': 0.8554, 'learning_rate': 0.00039989651398097134, 'epoch': 0.01}        \n",
      "{'loss': 0.8542, 'learning_rate': 0.00039989622602488735, 'epoch': 0.01}        \n",
      "{'loss': 0.8553, 'learning_rate': 0.0003998959376688361, 'epoch': 0.01}         \n",
      "{'loss': 0.8537, 'learning_rate': 0.00039989564891281833, 'epoch': 0.01}        \n",
      "{'loss': 0.8542, 'learning_rate': 0.0003998953597568344, 'epoch': 0.01}         \n",
      "{'loss': 0.8539, 'learning_rate': 0.0003998950702008851, 'epoch': 0.01}         \n",
      "{'loss': 0.8545, 'learning_rate': 0.0003998947802449708, 'epoch': 0.01}         \n",
      "{'loss': 0.8567, 'learning_rate': 0.0003998944898890924, 'epoch': 0.01}         \n",
      "{'loss': 0.854, 'learning_rate': 0.0003998941991332501, 'epoch': 0.01}          \n",
      "{'loss': 0.8532, 'learning_rate': 0.00039989390797744466, 'epoch': 0.01}        \n",
      "{'loss': 0.8528, 'learning_rate': 0.0003998936164216767, 'epoch': 0.01}         \n",
      "{'loss': 0.8532, 'learning_rate': 0.0003998933244659468, 'epoch': 0.01}         \n",
      "{'loss': 0.8538, 'learning_rate': 0.0003998930321102555, 'epoch': 0.01}         \n",
      "{'loss': 0.8554, 'learning_rate': 0.00039989273935460336, 'epoch': 0.01}        \n",
      "{'loss': 0.8543, 'learning_rate': 0.000399892446198991, 'epoch': 0.01}          \n",
      "{'loss': 0.8541, 'learning_rate': 0.00039989215264341906, 'epoch': 0.01}        \n",
      "{'loss': 0.8543, 'learning_rate': 0.00039989185868788803, 'epoch': 0.01}        \n",
      "{'loss': 0.8532, 'learning_rate': 0.00039989156433239856, 'epoch': 0.01}        \n",
      "{'loss': 0.8551, 'learning_rate': 0.0003998912695769512, 'epoch': 0.01}         \n",
      "{'loss': 0.8548, 'learning_rate': 0.00039989097442154657, 'epoch': 0.01}        \n",
      "{'loss': 0.8534, 'learning_rate': 0.00039989067886618524, 'epoch': 0.01}        \n",
      "{'loss': 0.8537, 'learning_rate': 0.0003998903829108678, 'epoch': 0.01}         \n",
      "{'loss': 0.8516, 'learning_rate': 0.00039989008655559485, 'epoch': 0.01}        \n",
      "{'loss': 0.8534, 'learning_rate': 0.00039988978980036704, 'epoch': 0.01}        \n",
      "{'loss': 0.8532, 'learning_rate': 0.00039988949264518485, 'epoch': 0.01}        \n",
      "{'loss': 0.8557, 'learning_rate': 0.00039988919509004905, 'epoch': 0.01}        \n",
      "{'loss': 0.8559, 'learning_rate': 0.00039988889713496, 'epoch': 0.01}           \n",
      "{'loss': 0.8528, 'learning_rate': 0.00039988859877991843, 'epoch': 0.01}        \n",
      "{'loss': 0.8578, 'learning_rate': 0.000399888300024925, 'epoch': 0.01}          \n",
      "{'loss': 0.8589, 'learning_rate': 0.0003998880008699802, 'epoch': 0.01}         \n",
      "{'loss': 0.8573, 'learning_rate': 0.0003998877013150846, 'epoch': 0.01}         \n",
      "{'loss': 0.8549, 'learning_rate': 0.00039988740136023896, 'epoch': 0.01}        \n",
      "{'loss': 0.8542, 'learning_rate': 0.0003998871010054437, 'epoch': 0.01}         \n",
      "{'loss': 0.8541, 'learning_rate': 0.00039988680025069954, 'epoch': 0.01}        \n",
      "{'loss': 0.8513, 'learning_rate': 0.000399886499096007, 'epoch': 0.01}          \n",
      "{'loss': 0.8535, 'learning_rate': 0.0003998861975413668, 'epoch': 0.01}         \n",
      "{'loss': 0.852, 'learning_rate': 0.00039988589558677943, 'epoch': 0.01}         \n",
      "{'loss': 0.8531, 'learning_rate': 0.0003998855932322455, 'epoch': 0.01}         \n",
      "{'loss': 0.8546, 'learning_rate': 0.0003998852904777657, 'epoch': 0.01}         \n",
      "{'loss': 0.8544, 'learning_rate': 0.0003998849873233406, 'epoch': 0.01}         \n",
      "{'loss': 0.8534, 'learning_rate': 0.0003998846837689707, 'epoch': 0.01}         \n",
      "{'loss': 0.8538, 'learning_rate': 0.00039988437981465676, 'epoch': 0.01}        \n",
      "{'loss': 0.8523, 'learning_rate': 0.0003998840754603993, 'epoch': 0.01}         \n",
      "{'loss': 0.8565, 'learning_rate': 0.00039988377070619897, 'epoch': 0.01}        \n",
      "{'loss': 0.8527, 'learning_rate': 0.0003998834655520564, 'epoch': 0.01}         \n",
      "  1%|â–Ž                              | 3822/351164 [5:15:30<471:14:07,  4.88s/it]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --mixed_precision=fp16 --num_cpu_threads_per_process=12 ./run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684676df-fc24-45ca-93ac-b370be5f258e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
