{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "rSz8BJJZl6Xw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I3A6M2zlbX-",
        "outputId": "796cf33d-d5fa-4d95-e8cb-00204ca0b69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cloud-tpu-client in /usr/local/lib/python3.10/dist-packages (0.10)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.1.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.34.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->cloud-tpu-client) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (5.3.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install cloud-tpu-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm transformers pytorch-lightning Levenshtein rdkit wandb"
      ],
      "metadata": {
        "id": "9oJrzdKillys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch_xla>=1.13"
      ],
      "metadata": {
        "id": "BuqqrskpxmsD"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "9ZRnFbWQmc6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import torch\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.backends.cudnn as cudnn\n",
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence\n",
        "from transformers import (AutoProcessor,\n",
        "                          AutoTokenizer,\n",
        "                          VisionEncoderDecoderModel,\n",
        "                          RobertaTokenizerFast,\n",
        "                          TrOCRForCausalLM,\n",
        "                          AutoModel,\n",
        "                          TrOCRConfig,\n",
        "                          ViTModel,\n",
        "                          ViTConfig,\n",
        "                          ViTImageProcessor\n",
        "                         )\n",
        "from sklearn.model_selection import train_test_split\n",
        "from rdkit import RDLogger,Chem\n",
        "from rdkit.Chem import AllChem,DataStructs,MolFromSmiles,Draw\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from rdkit.DataStructs import TanimotoSimilarity\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "pl.seed_everything(56)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwppK2FImLY1",
        "outputId": "d98d0541-212e-4b77-d136-bff676dcd5bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 56\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDLogger.DisableLog('rdApp.*')\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "mGpprPY2myTr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    wandb=False\n",
        "    encoder=\"google/vit-base-patch16-384\"\n",
        "    decoder=\"entropy/roberta_zinc_480m\"\n",
        "    train_path = './validation-00000-of-00001-53255e68092e122d.parquet'#'./all_ChEMBLSmiles.csv'\n",
        "    train_folder = './train/'\n",
        "    betas=(0.9, 0.999)\n",
        "    img_size = 512\n",
        "    max_pred_len = 128\n",
        "    val_split_size = 1e-6\n",
        "    scheduler = None\n",
        "    emb_dim = 512\n",
        "    attention_dim = 512\n",
        "    freq_threshold = 2\n",
        "    decoder_dim = 512\n",
        "    img_size=512\n",
        "    dropout = 0.4\n",
        "    eps=1e-6\n",
        "    num_workers = 2\n",
        "    batch_size = 64\n",
        "    encoder_lr = 1e-4\n",
        "    decoder_lr = 2e-4\n",
        "    weight_decay = 0.01\n",
        "    fine_tune_encoder = False\n",
        "    max_epoches=6\n",
        "    seed=56"
      ],
      "metadata": {
        "id": "cmR61pBpnC6u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "dzigGDRPnAgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PLDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer,processor):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.fn = lambda x : 255 if x == 255 else 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.df.iloc[idx]['smiles']\n",
        "        image = self.processor(self._gen_smiles(label),return_tensors='pt').pixel_values\n",
        "        label_enc = self.tokenizer.encode_plus(label, padding='max_length',max_length=128, truncation=True, return_tensors='pt')\n",
        "        return {'image':image.squeeze(0),\n",
        "                'input_ids':label_enc.input_ids.squeeze(0),\n",
        "                'attention_mask':label_enc.attention_mask.squeeze(0)}\n",
        "\n",
        "    def _gen_smiles(self,smiles):\n",
        "        try:\n",
        "            mol = MolFromSmiles(smiles)\n",
        "            img = Draw.MolToImage(mol,size=(384,384))\n",
        "            img = img.convert('L').convert('RGB')\n",
        "            return img\n",
        "        except:\n",
        "            return Image.open('/content/Без названия (3).png')\n"
      ],
      "metadata": {
        "id": "9SbXFynEnB6T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PLDataModule(pl.LightningDataModule):\n",
        "    def __init__(self,tokenizer,processor):\n",
        "        super().__init__()\n",
        "        self.cfg = CFG()\n",
        "        self.is_setup = False\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.train_df = pd.read_parquet(CFG.train_path)\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        #self.train_df, self.val_df = train_test_split(self.train_data, test_size=self.cfg.val_split_size,random_state=self.cfg.seed)\n",
        "        #self.train_df = self.train_df.reset_index(drop=True)\n",
        "        #self.val_df = self.val_df.reset_index(drop=True)\n",
        "        self.train_dataset = PLDataset(self.train_df,self.tokenizer,self.processor)\n",
        "        #self.val_dataset = PLDataset(self.val_df,self.tokenizer,self.processor)\n",
        "        self.is_setup = True\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          batch_size=self.cfg.batch_size,\n",
        "                          num_workers=self.cfg.num_workers,\n",
        "                          pin_memory=True,\n",
        "                          shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          batch_size=self.cfg.batch_size,\n",
        "                          num_workers=self.cfg.num_workers,\n",
        "                          pin_memory=True,\n",
        "                          shuffle=False)"
      ],
      "metadata": {
        "id": "RZptAezBoK2m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_accuracy(y_p,y):\n",
        "    y_p,y = list(y_p),list(y)\n",
        "    ln = min(len(y_p),len(y))\n",
        "    score = 0\n",
        "    for i in range(ln):\n",
        "        if y_p[i] == y[i]:\n",
        "            score += 1\n",
        "    return score / max(len(y_p),len(y))\n",
        "\n",
        "def correct_part(y_p):\n",
        "    if Chem.MolFromSmiles(y_p) is None:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def tanimoto(y_p,y):\n",
        "    try:\n",
        "        mol1 = Chem.MolFromSmiles(y_p)\n",
        "        mol2 = Chem.MolFromSmiles(y)\n",
        "\n",
        "        vec_1 = AllChem.RDKFingerprint(mol1)\n",
        "        vec_2 = AllChem.RDKFingerprint(mol2)\n",
        "        return DataStructs.TanimotoSimilarity(vec_1,vec_2)\n",
        "    except:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "PzvzUacioPyN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.preds = []\n",
        "        self.targets = []\n",
        "\n",
        "    def update(self,preds,targets):\n",
        "        self.preds += preds\n",
        "        self.targets += targets\n",
        "\n",
        "    def calc_metrics(self):\n",
        "        f = dict()\n",
        "\n",
        "        f['char_acc'] = np.mean([char_accuracy(x,y) for x,y in zip(self.preds,self.targets)])\n",
        "        f['corrent_part'] = np.mean([correct_part(x) for x in self.preds])\n",
        "        f['tanimoto'] = np.mean([tanimoto(x,y) for x,y in zip(self.preds,self.targets)])\n",
        "\n",
        "        return f"
      ],
      "metadata": {
        "id": "fcCPrWZjoRWM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PLModule(pl.LightningModule):\n",
        "    def __init__(self,model,tokenizer):\n",
        "        super().__init__()\n",
        "        self.cfg = CFG()\n",
        "        self.avg_meter = AverageMeter()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def forward(self,image,input_ids=None,attention_mask=None):\n",
        "        return self.model(pixel_values=image,labels=input_ids,decoder_attention_mask=attention_mask)\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        loss = self(**batch).loss\n",
        "        self.log_dict({'train_loss':loss.item()})\n",
        "        return loss\n",
        "\n",
        "    def validation_stepPAS(self, batch, _):\n",
        "        labels = batch['input_ids'].detach().cpu().numpy()\n",
        "        labels = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
        "\n",
        "        logits = self.model.generate(\n",
        "            batch['image'],\n",
        "            num_beams=4,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        logits = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in logits]\n",
        "        self.avg_meter.update(logits,labels)\n",
        "\n",
        "    def predict_step(self,batch,_):\n",
        "\n",
        "        logits = self.model.generate(\n",
        "            batch['image'],\n",
        "            num_beams=1,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        logits = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in logits]\n",
        "        return logits\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        f = self.avg_meter.calc_metrics()\n",
        "        self.log_dict(f)\n",
        "        print(f)\n",
        "        self.avg_meter.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.encoder.named_parameters()],\n",
        "                \"lr\":self.cfg.encoder_lr\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.decoder.named_parameters()],\n",
        "                \"lr\": self.cfg.decoder_lr\n",
        "            },\n",
        "        ]\n",
        "        return torch.optim.AdamW(optimizer_grouped_parameters,\n",
        "                                 betas=self.cfg.betas,\n",
        "                                 weight_decay=self.cfg.weight_decay,\n",
        "                                 eps=self.cfg.eps)"
      ],
      "metadata": {
        "id": "Odz98Vu_oTBg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(CFG.decoder)\n",
        "processor = AutoProcessor.from_pretrained(CFG.encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0Xg0WEdoY3d",
        "outputId": "5b942144-83fb-4489-dbd4-d3944154b4cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ViTModel(ViTConfig(hidden_size=384,\n",
        "                             hidden_act='gelu',\n",
        "                             image_size=384,\n",
        "                             num_attention_heads=6,\n",
        "                             num_hidden_layers=12,\n",
        "                             num_channels=3,\n",
        "                             intermediate_size=384 * 4,\n",
        "                             patch_size=16))"
      ],
      "metadata": {
        "id": "j5w0Zproofvq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = TrOCRForCausalLM(TrOCRConfig(vocab_size=len(tokenizer),\n",
        "                                       d_model=256,\n",
        "                                       decoder_attention_heads=8,\n",
        "                                       decoder_ffn_dim=1024,\n",
        "                                       decoder_layers=6,\n",
        "                                       activation_function='gelu',\n",
        "                                       max_position_embeddings=384,\n",
        "                                       dropout=0.2))"
      ],
      "metadata": {
        "id": "hc1x76dxqmcp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionEncoderDecoderModel(encoder=encoder,decoder=decoder)\n",
        "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.vocab_size = model.config.decoder.vocab_size"
      ],
      "metadata": {
        "id": "Rs21SfM-tBeF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dm = PLDataModule(tokenizer,processor)\n",
        "dm.prepare_data()\n",
        "dm.setup(0)"
      ],
      "metadata": {
        "id": "YNHCZTEcuPfI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pl = PLModule(model,tokenizer)"
      ],
      "metadata": {
        "id": "qtFvFGXcwi1v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"673ae6e9b51cc896110db5327738b993795fffad\")\n",
        "os.environ['WANDB_API_KEY'] = \"673ae6e9b51cc896110db5327738b993795fffad\"\n",
        "wandb.init(project='MOLECULA',name='TrOCR_tiny')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "FH3JfBH1xN3O",
        "outputId": "2f42144c-da81-4f46-d1e3-eb763535e7dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey20007\u001b[0m (\u001b[33mandrey2007\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240107_211944-5f9qcuc9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrey2007/MOLECULA/runs/5f9qcuc9' target=\"_blank\">TrOCR_tiny</a></strong> to <a href='https://wandb.ai/andrey2007/MOLECULA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/andrey2007/MOLECULA' target=\"_blank\">https://wandb.ai/andrey2007/MOLECULA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/andrey2007/MOLECULA/runs/5f9qcuc9' target=\"_blank\">https://wandb.ai/andrey2007/MOLECULA/runs/5f9qcuc9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/andrey2007/MOLECULA/runs/5f9qcuc9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7bcb14f63760>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
        "checkpoint_cb = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath='./outputs/',\n",
        "    filename='base_model{epoch:02d}',\n",
        "    monitor='tanimoto',\n",
        "    mode='max',\n",
        "    save_last=True\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"tpu\",\n",
        "    precision=32,\n",
        "    callbacks = [lr_monitor,checkpoint_cb],\n",
        "    logger = pl.loggers.WandbLogger(),\n",
        "    min_epochs=1,\n",
        "    devices='auto',\n",
        "    check_val_every_n_epoch=1,\n",
        "    max_epochs=CFG.max_epoches\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "tpDrrDR7xS0g",
        "outputId": "96b4fdea-5f92-4b5c-b33b-37a52f3735c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
            "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MisconfigurationException",
          "evalue": "`XLAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-eef353f0c7c0>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m trainer = pl.Trainer(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         self._accelerator_connector = _AcceleratorConnector(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_device_config_and_set_final_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_parallel_devices_and_init_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# 3. Instantiate ClusterEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mAcceleratorRegistry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accelerator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             ]\n\u001b[0;32m--> 381\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0;34mf\"`{accelerator_cls.__qualname__}` can not run on your system\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;34m\" since the accelerator is not available. The following accelerator(s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMisconfigurationException\u001b[0m: `XLAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-4lahcHxggy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}