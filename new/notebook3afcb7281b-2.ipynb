{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaL4","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/sam2.git\n!wget -O /content/sam2/sam2/configs/train.yaml 'https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3'\n%cd ./sam2/\n!pip install -e .[dev] -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:41:50.392199Z","iopub.execute_input":"2025-01-26T14:41:50.392478Z","iopub.status.idle":"2025-01-26T14:45:27.359291Z","shell.execute_reply.started":"2025-01-26T14:41:50.392457Z","shell.execute_reply":"2025-01-26T14:45:27.358503Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'sam2'...\nremote: Enumerating objects: 1070, done.\u001b[K\nremote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\nReceiving objects: 100% (1070/1070), 134.70 MiB | 53.42 MiB/s, done.\nResolving deltas: 100% (375/375), done.\n/content/sam2/sam2/configs/train.yaml: No such file or directory\n/kaggle/working/sam2\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.9/359.9 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install roboflow\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"5m7KUUT814dfZC9t6Jy8\")\nproject = rf.workspace(\"huiiii\").project(\"hui-kuxax\")\nversion = project.version(5)\ndataset = version.download(\"sam2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:45:34.529669Z","iopub.execute_input":"2025-01-26T14:45:34.529962Z","iopub.status.idle":"2025-01-26T14:46:32.441250Z","shell.execute_reply.started":"2025-01-26T14:45:34.529939Z","shell.execute_reply":"2025-01-26T14:46:32.440389Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.51-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from roboflow) (2024.12.14)\nCollecting idna==3.7 (from roboflow)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: cycler in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.7)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.5)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.4)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (11.0.0)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\nCollecting python-dotenv (from roboflow)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.2.3)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\nCollecting filetype (from roboflow)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.55.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.4.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nDownloading roboflow-1.1.51-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: filetype, python-dotenv, idna, roboflow\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed filetype-1.2.0 idna-3.7 python-dotenv-1.0.1 roboflow-1.1.51\nloading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in hui-5 to sam2:: 100%|██████████| 2803570/2803570 [00:43<00:00, 65008.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to hui-5 in sam2:: 100%|██████████| 33245/33245 [00:08<00:00, 4027.18it/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!cd ./checkpoints && ./download_ckpts.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:46:32.442187Z","iopub.execute_input":"2025-01-26T14:46:32.442403Z","iopub.status.idle":"2025-01-26T14:46:38.776262Z","shell.execute_reply.started":"2025-01-26T14:46:32.442384Z","shell.execute_reply":"2025-01-26T14:46:38.775495Z"}},"outputs":[{"name":"stdout","text":"Downloading sam2.1_hiera_tiny.pt checkpoint...\n--2025-01-26 14:46:32--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.118, 3.171.22.13, 3.171.22.68, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.118|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 156008466 (149M) [application/vnd.snesdev-page-table]\nSaving to: ‘sam2.1_hiera_tiny.pt’\n\nsam2.1_hiera_tiny.p 100%[===================>] 148.78M   307MB/s    in 0.5s    \n\n2025-01-26 14:46:33 (307 MB/s) - ‘sam2.1_hiera_tiny.pt’ saved [156008466/156008466]\n\nDownloading sam2.1_hiera_small.pt checkpoint...\n--2025-01-26 14:46:33--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.68, 3.171.22.118, 3.171.22.33, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.68|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 184416285 (176M) [application/vnd.snesdev-page-table]\nSaving to: ‘sam2.1_hiera_small.pt’\n\nsam2.1_hiera_small. 100%[===================>] 175.87M   296MB/s    in 0.6s    \n\n2025-01-26 14:46:33 (296 MB/s) - ‘sam2.1_hiera_small.pt’ saved [184416285/184416285]\n\nDownloading sam2.1_hiera_base_plus.pt checkpoint...\n--2025-01-26 14:46:33--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.13, 3.171.22.68, 3.171.22.118, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.13|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 323606802 (309M) [application/vnd.snesdev-page-table]\nSaving to: ‘sam2.1_hiera_base_plus.pt’\n\nsam2.1_hiera_base_p 100%[===================>] 308.62M   261MB/s    in 1.2s    \n\n2025-01-26 14:46:35 (261 MB/s) - ‘sam2.1_hiera_base_plus.pt’ saved [323606802/323606802]\n\nDownloading sam2.1_hiera_large.pt checkpoint...\n--2025-01-26 14:46:35--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.171.22.68, 3.171.22.118, 3.171.22.33, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.171.22.68|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 898083611 (856M) [application/vnd.snesdev-page-table]\nSaving to: ‘sam2.1_hiera_large.pt’\n\nsam2.1_hiera_large. 100%[===================>] 856.48M   279MB/s    in 3.3s    \n\n2025-01-26 14:46:38 (259 MB/s) - ‘sam2.1_hiera_large.pt’ saved [898083611/898083611]\n\nAll checkpoints are downloaded successfully.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import multiprocessing as mp\nmp.cpu_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:46:44.060928Z","iopub.execute_input":"2025-01-26T14:46:44.061242Z","iopub.status.idle":"2025-01-26T14:46:44.065552Z","shell.execute_reply.started":"2025-01-26T14:46:44.061215Z","shell.execute_reply":"2025-01-26T14:46:44.064964Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"48"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"%%writefile /kaggle/working/sam2/training/utils/data_utils.py\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nMisc functions, including distributed helpers.\n\nMostly copy-paste from torchvision references.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\n\nfrom PIL import Image as PILImage\nfrom tensordict import tensorclass\n\n\n@tensorclass\nclass BatchedVideoMetaData:\n    \"\"\"\n    This class represents metadata about a batch of videos.\n    Attributes:\n        unique_objects_identifier: A tensor of shape Bx3 containing unique identifiers for each object in the batch. Index consists of (video_id, obj_id, frame_id)\n        frame_orig_size: A tensor of shape Bx2 containing the original size of each frame in the batch.\n    \"\"\"\n\n    unique_objects_identifier: torch.LongTensor\n    frame_orig_size: torch.LongTensor\n\nNORM_C = 2**63\n\n@tensorclass\nclass BatchedVideoDatapoint:\n    \"\"\"\n    This class represents a batch of videos with associated annotations and metadata.\n    Attributes:\n        img_batch: A [TxBxCxHxW] tensor containing the image data for each frame in the batch, where T is the number of frames per video, and B is the number of videos in the batch.\n        obj_to_frame_idx: A [TxOx2] tensor containing the image_batch index which the object belongs to. O is the number of objects in the batch.\n        masks: A [TxOxHxW] tensor containing binary masks for each object in the batch.\n        metadata: An instance of BatchedVideoMetaData containing metadata about the batch.\n        dict_key: A string key used to identify the batch.\n    \"\"\"\n\n    img_batch: torch.FloatTensor\n    obj_to_frame_idx: torch.IntTensor\n    masks: torch.BoolTensor\n    metadata: BatchedVideoMetaData\n\n    dict_key: str\n\n    def pin_memory(self, device=None):\n        return self.apply(torch.Tensor.pin_memory, device=device)\n\n    @property\n    def num_frames(self) -> int:\n        \"\"\"\n        Returns the number of frames per video.\n        \"\"\"\n        return self.batch_size[0]\n\n    @property\n    def num_videos(self) -> int:\n        \"\"\"\n        Returns the number of videos in the batch.\n        \"\"\"\n        return self.img_batch.shape[1]\n\n    @property\n    def flat_obj_to_img_idx(self) -> torch.IntTensor:\n        \"\"\"\n        Returns a flattened tensor containing the object to img index.\n        The flat index can be used to access a flattened img_batch of shape [(T*B)xCxHxW]\n        \"\"\"\n        frame_idx, video_idx = self.obj_to_frame_idx.unbind(dim=-1)\n        flat_idx = video_idx * self.num_frames + frame_idx\n        return flat_idx\n\n    @property\n    def flat_img_batch(self) -> torch.FloatTensor:\n        \"\"\"\n        Returns a flattened img_batch_tensor of shape [(B*T)xCxHxW]\n        \"\"\"\n\n        return self.img_batch.transpose(0, 1).flatten(0, 1)\n\n\n@dataclass\nclass Object:\n    # Id of the object in the media\n    object_id: int\n    # Index of the frame in the media (0 if single image)\n    frame_index: int\n    segment: Union[torch.Tensor, dict]  # RLE dict or binary mask\n\n\n@dataclass\nclass Frame:\n    data: Union[torch.Tensor, PILImage.Image]\n    objects: List[Object]\n\n\n@dataclass\nclass VideoDatapoint:\n    \"\"\"Refers to an image/video and all its annotations\"\"\"\n\n    frames: List[Frame]\n    video_id: int\n    size: Tuple[int, int]\n\n\ndef collate_fn(\n    batch: List[VideoDatapoint],\n    dict_key,\n) -> BatchedVideoDatapoint:\n    \"\"\"\n    Args:\n        batch: A list of VideoDatapoint instances.\n        dict_key (str): A string key used to identify the batch.\n    \"\"\"\n    img_batch = []\n    for video in batch:\n        img_batch += [torch.stack([frame.data for frame in video.frames], dim=0)]\n\n    img_batch = torch.stack(img_batch, dim=0).permute((1, 0, 2, 3, 4))\n    T = img_batch.shape[0]\n    # Prepare data structures for sequential processing. Per-frame processing but batched across videos.\n    step_t_objects_identifier = [[] for _ in range(T)]\n    step_t_frame_orig_size = [[] for _ in range(T)]\n\n    step_t_masks = [[] for _ in range(T)]\n    step_t_obj_to_frame_idx = [\n        [] for _ in range(T)\n    ]  # List to store frame indices for each time step\n\n    for video_idx, video in enumerate(batch):\n        orig_video_id = video.video_id\n        orig_frame_size = video.size\n        for t, frame in enumerate(video.frames):\n            objects = frame.objects\n            for obj in objects:\n                orig_obj_id = obj.object_id\n                orig_frame_idx = obj.frame_index\n                step_t_obj_to_frame_idx[t].append(\n                    torch.tensor([t, video_idx], dtype=torch.int)\n                )\n                step_t_masks[t].append(obj.segment.to(torch.bool))\n                step_t_objects_identifier[t].append(\n                    torch.tensor([orig_video_id % NORM_C, orig_obj_id % NORM_C, orig_frame_idx % NORM_C])\n                )\n                step_t_frame_orig_size[t].append(torch.tensor(orig_frame_size))\n\n    obj_to_frame_idx = torch.stack(\n        [\n            torch.stack(obj_to_frame_idx, dim=0)\n            for obj_to_frame_idx in step_t_obj_to_frame_idx\n        ],\n        dim=0,\n    )\n    masks = torch.stack([torch.stack(masks, dim=0) for masks in step_t_masks], dim=0)\n    objects_identifier = torch.stack(\n        [torch.stack(id, dim=0) for id in step_t_objects_identifier], dim=0\n    )\n    frame_orig_size = torch.stack(\n        [torch.stack(id, dim=0) for id in step_t_frame_orig_size], dim=0\n    )\n    return BatchedVideoDatapoint(\n        img_batch=img_batch,\n        obj_to_frame_idx=obj_to_frame_idx,\n        masks=masks,\n        metadata=BatchedVideoMetaData(\n            unique_objects_identifier=objects_identifier,\n            frame_orig_size=frame_orig_size,\n        ),\n        dict_key=dict_key,\n        batch_size=[T],\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:46:46.442765Z","iopub.execute_input":"2025-01-26T14:46:46.443062Z","iopub.status.idle":"2025-01-26T14:46:46.448582Z","shell.execute_reply.started":"2025-01-26T14:46:46.443041Z","shell.execute_reply":"2025-01-26T14:46:46.447967Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/sam2/training/utils/data_utils.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile /kaggle/working/sam2/sam2/configs/train.yaml\n# @package _global_\n\nscratch:\n  resolution: 1024\n  train_batch_size: 1\n  num_train_workers: 46\n  num_frames: 1\n  max_num_objects: 3\n  base_lr: 5.0e-6\n  vision_lr: 3.0e-06\n  phases_per_epoch: 1\n  num_epochs: 5\n\ndataset:\n  # PATHS to Dataset\n  img_folder: /kaggle/working/sam2/hui-5/train # PATH to MOSE JPEGImages folder\n  gt_folder: /kaggle/working/sam2/hui-5/train  # PATH to MOSE Annotations folder\n  #file_list_txt: training/assets/MOSE_sample_train_list.txt # Optional PATH to filelist containing a subset of videos to be used for training\n  multiplier: 2\n\n# Video transforms\nvos:\n  train_transforms:\n    - _target_: training.dataset.transforms.ComposeAPI\n      transforms:\n        - _target_: training.dataset.transforms.RandomHorizontalFlip\n          consistent_transform: True\n        - _target_: training.dataset.transforms.RandomAffine\n          degrees: 25\n          shear: 20\n          image_interpolation: bilinear\n          consistent_transform: True\n        - _target_: training.dataset.transforms.RandomResizeAPI\n          sizes: ${scratch.resolution}\n          square: true\n          consistent_transform: True\n        - _target_: training.dataset.transforms.ColorJitter\n          consistent_transform: True\n          brightness: 0.1\n          contrast: 0.03\n          saturation: 0.03\n          hue: null\n        - _target_: training.dataset.transforms.RandomGrayscale\n          p: 0.05\n          consistent_transform: True\n        - _target_: training.dataset.transforms.ColorJitter\n          consistent_transform: False\n          brightness: 0.1\n          contrast: 0.05\n          saturation: 0.05\n          hue: null\n        - _target_: training.dataset.transforms.ToTensorAPI\n        - _target_: training.dataset.transforms.NormalizeAPI\n          mean: [0.485, 0.456, 0.406]\n          std: [0.229, 0.224, 0.225]\n\ntrainer:\n  _target_: training.trainer.Trainer\n  mode: train_only\n  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n  accelerator: cuda\n  seed_value: 56\n\n  model:\n    _target_: training.model.sam2.SAM2Train\n    image_encoder:\n      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n      scalp: 1\n      trunk:\n        _target_: sam2.modeling.backbones.hieradet.Hiera\n        embed_dim: 144\n        num_heads: 2\n        stages: [2, 6, 36, 4]\n        global_att_blocks: [23, 33, 43]\n        window_pos_embed_bkg_spatial_size: [7, 7]\n        window_spec: [8, 4, 16, 8]\n        drop_path_rate: 0.1\n      neck:\n        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 256\n          normalize: true\n          scale: null\n          temperature: 10000\n        d_model: 256\n        backbone_channel_list: [1152, 576, 288, 144]\n        fpn_top_down_levels: [2, 3]  # output level 0 and 1 directly use the backbone features\n        fpn_interp_model: nearest\n\n    memory_attention:\n      _target_: sam2.modeling.memory_attention.MemoryAttention\n      d_model: 256\n      pos_enc_at_input: true\n      layer:\n        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n        activation: relu\n        dim_feedforward: 2048\n        dropout: 0.1\n        pos_enc_at_attn: false\n        self_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes: [64, 64]\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n        d_model: 256\n        pos_enc_at_cross_attn_keys: true\n        pos_enc_at_cross_attn_queries: false\n        cross_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes: [64, 64]\n          rope_k_repeat: True\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n          kv_in_dim: 64\n      num_layers: 4\n\n    memory_encoder:\n        _target_: sam2.modeling.memory_encoder.MemoryEncoder\n        out_dim: 64\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 64\n          normalize: true\n          scale: null\n          temperature: 10000\n        mask_downsampler:\n          _target_: sam2.modeling.memory_encoder.MaskDownSampler\n          kernel_size: 3\n          stride: 2\n          padding: 1\n        fuser:\n          _target_: sam2.modeling.memory_encoder.Fuser\n          layer:\n            _target_: sam2.modeling.memory_encoder.CXBlock\n            dim: 256\n            kernel_size: 7\n            padding: 3\n            layer_scale_init_value: 1e-6\n            use_dwconv: True  # depth-wise convs\n          num_layers: 2\n\n    num_maskmem: 7\n    image_size: ${scratch.resolution}\n    # apply scaled sigmoid on mask logits for memory encoder, and directly feed input mask as output mask\n    sigmoid_scale_for_mem_enc: 20.0\n    sigmoid_bias_for_mem_enc: -10.0\n    use_mask_input_as_output_without_sam: true\n    # Memory\n    directly_add_no_mem_embed: true\n    no_obj_embed_spatial: true\n    # use high-resolution feature map in the SAM mask decoder\n    use_high_res_features_in_sam: true\n    # output 3 masks on the first click on initial conditioning frames\n    multimask_output_in_sam: true\n    # SAM heads\n    iou_prediction_use_sigmoid: True\n    # cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder\n    use_obj_ptrs_in_encoder: true\n    add_tpos_enc_to_obj_ptrs: true\n    proj_tpos_enc_in_obj_ptrs: true\n    use_signed_tpos_enc_to_obj_ptrs: true\n    only_obj_ptrs_in_the_past_for_eval: true\n    # object occlusion prediction\n    pred_obj_scores: true\n    pred_obj_scores_mlp: true\n    fixed_no_obj_ptr: true\n    # multimask tracking settings\n    multimask_output_for_tracking: true\n    use_multimask_token_for_obj_ptr: true\n    multimask_min_pt_num: 0\n    multimask_max_pt_num: 1\n    use_mlp_for_obj_ptr_proj: true\n    # Compilation flag\n    # compile_image_encoder: False\n\n    ####### Training specific params #######\n    # box/point input and corrections\n    prob_to_use_pt_input_for_train: 0.5\n    prob_to_use_pt_input_for_eval: 0.0\n    prob_to_use_box_input_for_train: 0.5  # 0.5*0.5 = 0.25 prob to use box instead of points\n    prob_to_use_box_input_for_eval: 0.0\n    prob_to_sample_from_gt_for_train: 0.1  # with a small prob, sampling correction points from GT mask instead of prediction errors\n    num_frames_to_correct_for_train: 2  # iteratively sample on random 1~2 frames (always include the first frame)\n    num_frames_to_correct_for_eval: 1  # only iteratively sample on first frame\n    rand_frames_to_correct_for_train: True  # random #init-cond-frame ~ 2\n    add_all_frames_to_correct_as_cond: True  # when a frame receives a correction click, it becomes a conditioning frame (even if it's not initially a conditioning frame)\n    # maximum 2 initial conditioning frames\n    num_init_cond_frames_for_train: 2\n    rand_init_cond_frames_for_train: True  # random 1~2\n    num_correction_pt_per_frame: 7\n    use_act_ckpt_iterative_pt_sampling: false\n    \n\n    \n    num_init_cond_frames_for_eval: 1  # only mask on the first frame\n    forward_backbone_per_frame_for_eval: True\n    \n\n  data:\n    train:\n      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n      phases_per_epoch: ${scratch.phases_per_epoch}\n      batch_sizes:\n        - ${scratch.train_batch_size}\n\n      datasets:\n        - _target_: training.dataset.vos_dataset.VOSDataset\n          transforms: ${vos.train_transforms}\n          training: true\n          video_dataset:\n            _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n            img_folder: ${dataset.img_folder}\n            gt_folder: ${dataset.gt_folder}\n            # file_list_txt: ${dataset.file_list_txt}\n          multiplier: ${dataset.multiplier}\n          sampler:\n            _target_: training.dataset.vos_sampler.RandomUniformSampler\n            num_frames: 1\n            max_num_objects: ${scratch.max_num_objects}\n      shuffle: True\n      num_workers: ${scratch.num_train_workers}\n      pin_memory: True\n      drop_last: True\n      collate_fn:\n        _target_: training.utils.data_utils.collate_fn\n        _partial_: true\n        dict_key: all\n\n  optim:\n    amp:\n      enabled: True\n      amp_dtype: bfloat16\n\n    optimizer:\n      _target_: torch.optim.AdamW\n\n    gradient_clip:\n      _target_: training.optimizer.GradientClipper\n      max_norm: 0.1\n      norm_type: 2\n\n    param_group_modifiers:\n      - _target_: training.optimizer.layer_decay_param_modifier\n        _partial_: True\n        layer_decay_value: 0.9\n        apply_to: 'image_encoder.trunk'\n        overrides:\n          - pattern: '*pos_embed*'\n            value: 1.0\n\n    options:\n      lr:\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.CosineParamScheduler\n            start_value: ${scratch.base_lr}\n            end_value: ${divide:${scratch.base_lr},10}\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.CosineParamScheduler\n            start_value: ${scratch.vision_lr}\n            end_value: ${divide:${scratch.vision_lr},10}\n          param_names:\n            - 'image_encoder.*'\n      weight_decay:\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n            value: 0.1\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n            value: 0.0\n          param_names:\n            - '*bias*'\n          module_cls_names: ['torch.nn.LayerNorm']\n\n  loss:\n    all:\n      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n      weight_dict:\n        loss_mask: 20\n        loss_dice: 1\n        loss_iou: 1\n        loss_class: 1\n      supervise_all_iou: true\n      iou_use_l1_loss: true\n      pred_obj_scores: true\n      focal_gamma_obj_score: 0.0\n      focal_alpha_obj_score: -1.0\n\n  distributed:\n    backend: nccl\n    find_unused_parameters: True\n\n  logging:\n    tensorboard_writer:\n      _target_: training.utils.logger.make_tensorboard_logger\n      log_dir:  ${launcher.experiment_log_dir}/tensorboard\n      flush_secs: 120\n      should_log: True\n    log_dir: ${launcher.experiment_log_dir}/logs\n    log_freq: 10\n\n  # initialize from a SAM 2 checkpoint\n  checkpoint:\n    save_dir: ./checkpoints_finetune\n    save_freq: 5 # 0 only last checkpoint is saved.\n    model_weight_initializer:\n      _partial_: True\n      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n      strict: True\n      ignore_unexpected_keys: null\n      ignore_missing_keys: null\n\n      state_dict:\n        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n        checkpoint_path: ./checkpoints/sam2.1_hiera_large.pt # PATH to SAM 2.1 checkpoint\n        ckpt_state_dict_keys: ['model']\n\nlauncher:\n  num_nodes: 1\n  gpus_per_node: 8\n  experiment_log_dir: null # Path to log directory, defaults to ./sam2_logs/${config_name}\n\n# SLURM args if running on a cluster\nsubmitit:\n  partition: null\n  account: null\n  qos: null\n  cpus_per_task: 10\n  use_cluster: false\n  timeout_hour: 24\n  name: null\n  port_range: [10000, 65000]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:48:02.097351Z","iopub.execute_input":"2025-01-26T14:48:02.097676Z","iopub.status.idle":"2025-01-26T14:48:02.103182Z","shell.execute_reply.started":"2025-01-26T14:48:02.097652Z","shell.execute_reply":"2025-01-26T14:48:02.102603Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/sam2/sam2/configs/train.yaml\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile /kaggle/working/sam2/sam2/configs/train.yaml\n# @package _global_\n\nscratch:\n  resolution: 1024\n  train_batch_size: 2\n  num_train_workers: 46\n  num_frames: 1\n  max_num_objects: 3\n  base_lr: 5.0e-6\n  vision_lr: 3.0e-06\n  phases_per_epoch: 1\n  num_epochs: 5\n\ndataset:\n  # PATHS to Dataset\n  img_folder: /kaggle/working/sam2/hui-5/train # PATH to MOSE JPEGImages folder\n  gt_folder: /kaggle/working/sam2/hui-5/train  # PATH to MOSE Annotations folder\n  #file_list_txt: training/assets/MOSE_sample_train_list.txt # Optional PATH to filelist containing a subset of videos to be used for training\n  multiplier: 2\n\n# Video transforms\nvos:\n  train_transforms:\n    - _target_: training.dataset.transforms.ComposeAPI\n      transforms:\n        - _target_: training.dataset.transforms.RandomHorizontalFlip\n          consistent_transform: True\n        - _target_: training.dataset.transforms.RandomAffine\n          degrees: 25\n          shear: 20\n          image_interpolation: bilinear\n          consistent_transform: True\n        - _target_: training.dataset.transforms.RandomResizeAPI\n          sizes: ${scratch.resolution}\n          square: true\n          consistent_transform: True\n        - _target_: training.dataset.transforms.ColorJitter\n          consistent_transform: True\n          brightness: 0.1\n          contrast: 0.03\n          saturation: 0.03\n          hue: null\n        - _target_: training.dataset.transforms.RandomGrayscale\n          p: 0.05\n          consistent_transform: True\n        - _target_: training.dataset.transforms.ColorJitter\n          consistent_transform: False\n          brightness: 0.1\n          contrast: 0.05\n          saturation: 0.05\n          hue: null\n        - _target_: training.dataset.transforms.ToTensorAPI\n        - _target_: training.dataset.transforms.NormalizeAPI\n          mean: [0.485, 0.456, 0.406]\n          std: [0.229, 0.224, 0.225]\n\ntrainer:\n  _target_: training.trainer.Trainer\n  mode: train_only\n  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n  accelerator: cuda\n  seed_value: 123\n\n  model:\n    _target_: training.model.sam2.SAM2Train\n    image_encoder:\n      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n      scalp: 1\n      trunk:\n        _target_: sam2.modeling.backbones.hieradet.Hiera\n        embed_dim: 112\n        num_heads: 2\n        drop_path_rate: 0.1\n      neck:\n        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 256\n          normalize: true\n          scale: null\n          temperature: 10000\n        d_model: 256\n        backbone_channel_list: [896, 448, 224, 112]\n        fpn_top_down_levels: [2, 3]  # output level 0 and 1 directly use the backbone features\n        fpn_interp_model: nearest\n\n    memory_attention:\n      _target_: sam2.modeling.memory_attention.MemoryAttention\n      d_model: 256\n      pos_enc_at_input: true\n      layer:\n        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n        activation: relu\n        dim_feedforward: 2048\n        dropout: 0.1\n        pos_enc_at_attn: false\n        self_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes: [32, 32]\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n        d_model: 256\n        pos_enc_at_cross_attn_keys: true\n        pos_enc_at_cross_attn_queries: false\n        cross_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes: [32, 32]\n          rope_k_repeat: True\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n          kv_in_dim: 64\n      num_layers: 4\n\n    memory_encoder:\n        _target_: sam2.modeling.memory_encoder.MemoryEncoder\n        out_dim: 64\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 64\n          normalize: true\n          scale: null\n          temperature: 10000\n        mask_downsampler:\n          _target_: sam2.modeling.memory_encoder.MaskDownSampler\n          kernel_size: 3\n          stride: 2\n          padding: 1\n        fuser:\n          _target_: sam2.modeling.memory_encoder.Fuser\n          layer:\n            _target_: sam2.modeling.memory_encoder.CXBlock\n            dim: 256\n            kernel_size: 7\n            padding: 3\n            layer_scale_init_value: 1e-6\n            use_dwconv: True  # depth-wise convs\n          num_layers: 2\n\n    num_maskmem: 7\n    image_size: ${scratch.resolution}\n    # apply scaled sigmoid on mask logits for memory encoder, and directly feed input mask as output mask\n    sigmoid_scale_for_mem_enc: 20.0\n    sigmoid_bias_for_mem_enc: -10.0\n    use_mask_input_as_output_without_sam: true\n    # Memory\n    directly_add_no_mem_embed: true\n    no_obj_embed_spatial: true\n    # use high-resolution feature map in the SAM mask decoder\n    use_high_res_features_in_sam: true\n    # output 3 masks on the first click on initial conditioning frames\n    multimask_output_in_sam: true\n    # SAM heads\n    iou_prediction_use_sigmoid: True\n    # cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder\n    use_obj_ptrs_in_encoder: true\n    add_tpos_enc_to_obj_ptrs: true\n    proj_tpos_enc_in_obj_ptrs: true\n    use_signed_tpos_enc_to_obj_ptrs: true\n    only_obj_ptrs_in_the_past_for_eval: true\n    # object occlusion prediction\n    pred_obj_scores: true\n    pred_obj_scores_mlp: true\n    fixed_no_obj_ptr: true\n    # multimask tracking settings\n    multimask_output_for_tracking: true\n    use_multimask_token_for_obj_ptr: true\n    multimask_min_pt_num: 0\n    multimask_max_pt_num: 1\n    use_mlp_for_obj_ptr_proj: true\n    # Compilation flag\n    # compile_image_encoder: False\n\n    ####### Training specific params #######\n    # box/point input and corrections\n    prob_to_use_pt_input_for_train: 0.5\n    prob_to_use_pt_input_for_eval: 0.0\n    prob_to_use_box_input_for_train: 0.5  # 0.5*0.5 = 0.25 prob to use box instead of points\n    prob_to_use_box_input_for_eval: 0.0\n    prob_to_sample_from_gt_for_train: 0.1  # with a small prob, sampling correction points from GT mask instead of prediction errors\n    num_frames_to_correct_for_train: 2  # iteratively sample on random 1~2 frames (always include the first frame)\n    num_frames_to_correct_for_eval: 1  # only iteratively sample on first frame\n    rand_frames_to_correct_for_train: True  # random #init-cond-frame ~ 2\n    add_all_frames_to_correct_as_cond: True  # when a frame receives a correction click, it becomes a conditioning frame (even if it's not initially a conditioning frame)\n    # maximum 2 initial conditioning frames\n    num_init_cond_frames_for_train: 2\n    rand_init_cond_frames_for_train: True  # random 1~2\n    num_correction_pt_per_frame: 7\n    use_act_ckpt_iterative_pt_sampling: false\n    \n\n    \n    num_init_cond_frames_for_eval: 1  # only mask on the first frame\n    forward_backbone_per_frame_for_eval: True\n    \n\n  data:\n    train:\n      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n      phases_per_epoch: ${scratch.phases_per_epoch}\n      batch_sizes:\n        - ${scratch.train_batch_size}\n\n      datasets:\n        - _target_: training.dataset.vos_dataset.VOSDataset\n          transforms: ${vos.train_transforms}\n          training: true\n          video_dataset:\n            _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n            img_folder: ${dataset.img_folder}\n            gt_folder: ${dataset.gt_folder}\n            # file_list_txt: ${dataset.file_list_txt}\n          multiplier: ${dataset.multiplier}\n          sampler:\n            _target_: training.dataset.vos_sampler.RandomUniformSampler\n            num_frames: 1\n            max_num_objects: ${scratch.max_num_objects}\n      shuffle: True\n      num_workers: ${scratch.num_train_workers}\n      pin_memory: True\n      drop_last: True\n      collate_fn:\n        _target_: training.utils.data_utils.collate_fn\n        _partial_: true\n        dict_key: all\n\n  optim:\n    amp:\n      enabled: True\n      amp_dtype: bfloat16\n\n    optimizer:\n      _target_: torch.optim.AdamW\n\n    gradient_clip:\n      _target_: training.optimizer.GradientClipper\n      max_norm: 0.1\n      norm_type: 2\n\n    param_group_modifiers:\n      - _target_: training.optimizer.layer_decay_param_modifier\n        _partial_: True\n        layer_decay_value: 0.9\n        apply_to: 'image_encoder.trunk'\n        overrides:\n          - pattern: '*pos_embed*'\n            value: 1.0\n\n    options:\n      lr:\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.CosineParamScheduler\n            start_value: ${scratch.base_lr}\n            end_value: ${divide:${scratch.base_lr},10}\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.CosineParamScheduler\n            start_value: ${scratch.vision_lr}\n            end_value: ${divide:${scratch.vision_lr},10}\n          param_names:\n            - 'image_encoder.*'\n      weight_decay:\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n            value: 0.1\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n            value: 0.0\n          param_names:\n            - '*bias*'\n          module_cls_names: ['torch.nn.LayerNorm']\n\n  loss:\n    all:\n      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n      weight_dict:\n        loss_mask: 20\n        loss_dice: 1\n        loss_iou: 1\n        loss_class: 1\n      supervise_all_iou: true\n      iou_use_l1_loss: true\n      pred_obj_scores: true\n      focal_gamma_obj_score: 0.0\n      focal_alpha_obj_score: -1.0\n\n  distributed:\n    backend: nccl\n    find_unused_parameters: True\n\n  logging:\n    tensorboard_writer:\n      _target_: training.utils.logger.make_tensorboard_logger\n      log_dir:  ${launcher.experiment_log_dir}/tensorboard\n      flush_secs: 120\n      should_log: True\n    log_dir: ${launcher.experiment_log_dir}/logs\n    log_freq: 10\n\n  # initialize from a SAM 2 checkpoint\n  checkpoint:\n    save_dir: ./checkpoints_finetune\n    save_freq: 5 # 0 only last checkpoint is saved.\n    model_weight_initializer:\n      _partial_: True\n      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n      strict: True\n      ignore_unexpected_keys: null\n      ignore_missing_keys: null\n\n      state_dict:\n        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt # PATH to SAM 2.1 checkpoint\n        ckpt_state_dict_keys: ['model']\n\nlauncher:\n  num_nodes: 1\n  gpus_per_node: 8\n  experiment_log_dir: null # Path to log directory, defaults to ./sam2_logs/${config_name}\n\n# SLURM args if running on a cluster\nsubmitit:\n  partition: null\n  account: null\n  qos: null\n  cpus_per_task: 10\n  use_cluster: false\n  timeout_hour: 24\n  name: null\n  port_range: [10000, 65000]\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Script to rename roboflow filenames to something SAM 2.1 compatible.\n# Maybe it is possible to remove this step tweaking sam2/sam2/configs/train.yaml.\nimport os\nimport re\n\nFOLDER = \"/kaggle/working/sam2/hui-5/train\"\nc = 0\nfor filename in os.listdir(FOLDER):\n    # Replace all except last dot with underscore\n    new_filename = filename.replace(\".\", \"_\", filename.count(\".\") - 1)\n    if not re.search(r\"_\\d+\\.\\w+$\", new_filename):\n        # Add an int to the end of base name\n        new_filename = new_filename.replace(\".\", \"_1.\")\n    os.rename(os.path.join(FOLDER, filename), os.path.join(FOLDER, new_filename))\n    c += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:48:37.802192Z","iopub.execute_input":"2025-01-26T14:48:37.802446Z","iopub.status.idle":"2025-01-26T14:48:38.483086Z","shell.execute_reply.started":"2025-01-26T14:48:37.802425Z","shell.execute_reply":"2025-01-26T14:48:38.482411Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"os.listdir('./')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:49:07.650596Z","iopub.execute_input":"2025-01-26T14:49:07.650858Z","iopub.status.idle":"2025-01-26T14:49:07.655086Z","shell.execute_reply.started":"2025-01-26T14:49:07.650838Z","shell.execute_reply":"2025-01-26T14:49:07.654490Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['.clang-format',\n 'RELEASE_NOTES.md',\n 'CONTRIBUTING.md',\n 'INSTALL.md',\n 'tools',\n 'docker-compose.yaml',\n 'LICENSE',\n 'checkpoints',\n 'training',\n 'notebooks',\n 'demo',\n 'README.md',\n 'LICENSE_cctorch',\n 'CODE_OF_CONDUCT.md',\n 'assets',\n 'sam2',\n 'MANIFEST.in',\n '.github',\n 'pyproject.toml',\n '.watchmanconfig',\n 'hui-5',\n 'SAM_2.egg-info',\n '.gitignore',\n 'backend.Dockerfile',\n '.git',\n 'setup.py',\n 'sav_dataset']"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"%cd /kaggle/working/sam2\n!python training/train.py -c './configs/train.yaml' --use-cluster 0 --num-gpus 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T14:49:13.099969Z","iopub.execute_input":"2025-01-26T14:49:13.100269Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/sam2\n###################### Train App Config ####################\nscratch:\n  resolution: 1024\n  train_batch_size: 1\n  num_train_workers: 46\n  num_frames: 1\n  max_num_objects: 3\n  base_lr: 5.0e-06\n  vision_lr: 3.0e-06\n  phases_per_epoch: 1\n  num_epochs: 5\ndataset:\n  img_folder: /kaggle/working/sam2/hui-5/train\n  gt_folder: /kaggle/working/sam2/hui-5/train\n  multiplier: 2\nvos:\n  train_transforms:\n  - _target_: training.dataset.transforms.ComposeAPI\n    transforms:\n    - _target_: training.dataset.transforms.RandomHorizontalFlip\n      consistent_transform: true\n    - _target_: training.dataset.transforms.RandomAffine\n      degrees: 25\n      shear: 20\n      image_interpolation: bilinear\n      consistent_transform: true\n    - _target_: training.dataset.transforms.RandomResizeAPI\n      sizes: ${scratch.resolution}\n      square: true\n      consistent_transform: true\n    - _target_: training.dataset.transforms.ColorJitter\n      consistent_transform: true\n      brightness: 0.1\n      contrast: 0.03\n      saturation: 0.03\n      hue: null\n    - _target_: training.dataset.transforms.RandomGrayscale\n      p: 0.05\n      consistent_transform: true\n    - _target_: training.dataset.transforms.ColorJitter\n      consistent_transform: false\n      brightness: 0.1\n      contrast: 0.05\n      saturation: 0.05\n      hue: null\n    - _target_: training.dataset.transforms.ToTensorAPI\n    - _target_: training.dataset.transforms.NormalizeAPI\n      mean:\n      - 0.485\n      - 0.456\n      - 0.406\n      std:\n      - 0.229\n      - 0.224\n      - 0.225\ntrainer:\n  _target_: training.trainer.Trainer\n  mode: train_only\n  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n  accelerator: cuda\n  seed_value: 56\n  model:\n    _target_: training.model.sam2.SAM2Train\n    image_encoder:\n      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n      scalp: 1\n      trunk:\n        _target_: sam2.modeling.backbones.hieradet.Hiera\n        embed_dim: 144\n        num_heads: 2\n        stages:\n        - 2\n        - 6\n        - 36\n        - 4\n        global_att_blocks:\n        - 23\n        - 33\n        - 43\n        window_pos_embed_bkg_spatial_size:\n        - 7\n        - 7\n        window_spec:\n        - 8\n        - 4\n        - 16\n        - 8\n        drop_path_rate: 0.1\n      neck:\n        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 256\n          normalize: true\n          scale: null\n          temperature: 10000\n        d_model: 256\n        backbone_channel_list:\n        - 1152\n        - 576\n        - 288\n        - 144\n        fpn_top_down_levels:\n        - 2\n        - 3\n        fpn_interp_model: nearest\n    memory_attention:\n      _target_: sam2.modeling.memory_attention.MemoryAttention\n      d_model: 256\n      pos_enc_at_input: true\n      layer:\n        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n        activation: relu\n        dim_feedforward: 2048\n        dropout: 0.1\n        pos_enc_at_attn: false\n        self_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes:\n          - 64\n          - 64\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n        d_model: 256\n        pos_enc_at_cross_attn_keys: true\n        pos_enc_at_cross_attn_queries: false\n        cross_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes:\n          - 64\n          - 64\n          rope_k_repeat: true\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n          kv_in_dim: 64\n      num_layers: 4\n    memory_encoder:\n      _target_: sam2.modeling.memory_encoder.MemoryEncoder\n      out_dim: 64\n      position_encoding:\n        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n        num_pos_feats: 64\n        normalize: true\n        scale: null\n        temperature: 10000\n      mask_downsampler:\n        _target_: sam2.modeling.memory_encoder.MaskDownSampler\n        kernel_size: 3\n        stride: 2\n        padding: 1\n      fuser:\n        _target_: sam2.modeling.memory_encoder.Fuser\n        layer:\n          _target_: sam2.modeling.memory_encoder.CXBlock\n          dim: 256\n          kernel_size: 7\n          padding: 3\n          layer_scale_init_value: 1.0e-06\n          use_dwconv: true\n        num_layers: 2\n    num_maskmem: 7\n    image_size: ${scratch.resolution}\n    sigmoid_scale_for_mem_enc: 20.0\n    sigmoid_bias_for_mem_enc: -10.0\n    use_mask_input_as_output_without_sam: true\n    directly_add_no_mem_embed: true\n    no_obj_embed_spatial: true\n    use_high_res_features_in_sam: true\n    multimask_output_in_sam: true\n    iou_prediction_use_sigmoid: true\n    use_obj_ptrs_in_encoder: true\n    add_tpos_enc_to_obj_ptrs: true\n    proj_tpos_enc_in_obj_ptrs: true\n    use_signed_tpos_enc_to_obj_ptrs: true\n    only_obj_ptrs_in_the_past_for_eval: true\n    pred_obj_scores: true\n    pred_obj_scores_mlp: true\n    fixed_no_obj_ptr: true\n    multimask_output_for_tracking: true\n    use_multimask_token_for_obj_ptr: true\n    multimask_min_pt_num: 0\n    multimask_max_pt_num: 1\n    use_mlp_for_obj_ptr_proj: true\n    prob_to_use_pt_input_for_train: 0.5\n    prob_to_use_pt_input_for_eval: 0.0\n    prob_to_use_box_input_for_train: 0.5\n    prob_to_use_box_input_for_eval: 0.0\n    prob_to_sample_from_gt_for_train: 0.1\n    num_frames_to_correct_for_train: 2\n    num_frames_to_correct_for_eval: 1\n    rand_frames_to_correct_for_train: true\n    add_all_frames_to_correct_as_cond: true\n    num_init_cond_frames_for_train: 2\n    rand_init_cond_frames_for_train: true\n    num_correction_pt_per_frame: 7\n    use_act_ckpt_iterative_pt_sampling: false\n    num_init_cond_frames_for_eval: 1\n    forward_backbone_per_frame_for_eval: true\n  data:\n    train:\n      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n      phases_per_epoch: ${scratch.phases_per_epoch}\n      batch_sizes:\n      - ${scratch.train_batch_size}\n      datasets:\n      - _target_: training.dataset.vos_dataset.VOSDataset\n        transforms: ${vos.train_transforms}\n        training: true\n        video_dataset:\n          _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n          img_folder: ${dataset.img_folder}\n          gt_folder: ${dataset.gt_folder}\n        multiplier: ${dataset.multiplier}\n        sampler:\n          _target_: training.dataset.vos_sampler.RandomUniformSampler\n          num_frames: 1\n          max_num_objects: ${scratch.max_num_objects}\n      shuffle: true\n      num_workers: ${scratch.num_train_workers}\n      pin_memory: true\n      drop_last: true\n      collate_fn:\n        _target_: training.utils.data_utils.collate_fn\n        _partial_: true\n        dict_key: all\n  optim:\n    amp:\n      enabled: true\n      amp_dtype: bfloat16\n    optimizer:\n      _target_: torch.optim.AdamW\n    gradient_clip:\n      _target_: training.optimizer.GradientClipper\n      max_norm: 0.1\n      norm_type: 2\n    param_group_modifiers:\n    - _target_: training.optimizer.layer_decay_param_modifier\n      _partial_: true\n      layer_decay_value: 0.9\n      apply_to: image_encoder.trunk\n      overrides:\n      - pattern: '*pos_embed*'\n        value: 1.0\n    options:\n      lr:\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n          start_value: ${scratch.base_lr}\n          end_value: ${divide:${scratch.base_lr},10}\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n          start_value: ${scratch.vision_lr}\n          end_value: ${divide:${scratch.vision_lr},10}\n        param_names:\n        - image_encoder.*\n      weight_decay:\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n          value: 0.1\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n          value: 0.0\n        param_names:\n        - '*bias*'\n        module_cls_names:\n        - torch.nn.LayerNorm\n  loss:\n    all:\n      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n      weight_dict:\n        loss_mask: 20\n        loss_dice: 1\n        loss_iou: 1\n        loss_class: 1\n      supervise_all_iou: true\n      iou_use_l1_loss: true\n      pred_obj_scores: true\n      focal_gamma_obj_score: 0.0\n      focal_alpha_obj_score: -1.0\n  distributed:\n    backend: nccl\n    find_unused_parameters: true\n  logging:\n    tensorboard_writer:\n      _target_: training.utils.logger.make_tensorboard_logger\n      log_dir: ${launcher.experiment_log_dir}/tensorboard\n      flush_secs: 120\n      should_log: true\n    log_dir: ${launcher.experiment_log_dir}/logs\n    log_freq: 10\n  checkpoint:\n    save_dir: ./checkpoints_finetune\n    save_freq: 5\n    model_weight_initializer:\n      _partial_: true\n      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n      strict: true\n      ignore_unexpected_keys: null\n      ignore_missing_keys: null\n      state_dict:\n        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n        checkpoint_path: ./checkpoints/sam2.1_hiera_large.pt\n        ckpt_state_dict_keys:\n        - model\nlauncher:\n  num_nodes: 1\n  gpus_per_node: 8\n  experiment_log_dir: /kaggle/working/sam2/sam2_logs/./configs/train.yaml\nsubmitit:\n  partition: null\n  account: null\n  qos: null\n  cpus_per_task: 10\n  use_cluster: false\n  timeout_hour: 24\n  name: null\n  port_range:\n  - 10000\n  - 65000\n\n############################################################\n2025-01-26 14:49:26.563307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-26 14:49:26.563322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-26 14:49:26.563322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-26 14:49:26.563328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-26 14:49:26.810188: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-26 14:49:26.810199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-26 14:49:26.810198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-26 14:49:26.810214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-26 14:49:26.887525: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-26 14:49:26.887537: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-26 14:49:26.887536: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-26 14:49:26.887545: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 2025-01-26 14:49:39,798 train_utils.py: 108: MACHINE SEED: 280\nINFO 2025-01-26 14:49:39,804 train_utils.py: 154: Logging ENV_VARIABLES\nINFO 2025-01-26 14:49:39,804 train_utils.py: 155: BUILD_DATE=20250109-122511\nCLICOLOR=1\nCLOUDSDK_CONFIG=/content/.config\nCLOUDSDK_PYTHON=python3\nCOLAB_DEBUG_ADAPTER_MUX_PATH=/usr/local/bin/dap_multiplexer\nCOLAB_FILE_HANDLER_ADDR=localhost:3453\nCOLAB_HUMAN_READABLE_NODE_LOGS=1\nCOLAB_JUPYTER_ALLOW_ORIGIN_PAT=https://colab\\.(sandbox|research)\\.google\\.com\nCOLAB_JUPYTER_IP=127.0.0.1\nCOLAB_KERNEL_MANAGER_PROXY_PORT=6000\nCOLAB_LANGUAGE_SERVER_PROXY=/usr/colab/bin/language_service\nCOLAB_RELEASE_TAG=release-colab_20241217-060132_RC00\nCOLAB_TPU_1VM=\nCOLAB_WARMUP_DEFAULTS=1\nCOLUMNS=100\nCUDA_HOME=/usr/local/cuda\nCUDA_MAJOR_VERSION=12\nCUDA_MINOR_VERSION=2\nCUDA_MODULE_LOADING=LAZY\nCUDA_VERSION=12.2.2\nDEBIAN_FRONTEND=noninteractive\nENV=/root/.bashrc\nGIT_COMMIT=55c18210abfa1f3e2b75e9ffd25950ebfbbb6694\nGIT_PAGER=cat\nHOME=/root\nHOSTNAME=d7127f201a05\nHYDRA_FULL_ERROR=1\nJPY_PARENT_PID=1\nKAGGLE_API_V1_TOKEN=/etc/secrets/kaggle/api-v1-token\nKAGGLE_CONTAINER_NAME=kaggle_4ccv3B4S1cKl99M9A9Y4FaIVz8YW18WWlzAapo7mm98-219344335-webtier\nKAGGLE_DATA_PROXY_PROJECT=kaggle-161607\nKAGGLE_DATA_PROXY_TOKEN=eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.MGp3c-fVbb6q3ZXqIg9MwGIIrPlOdCawZG2s6DEICLKtk4z8f_aAWA.NKCMaaB2i9pdX5MHiFtetw.CHZko0oJnfs_1mq0dyuEFAKuPEPPIBVIwKzhx5807NQFhWYT1F_A3byYqGBa4eTPzihANKAjw8qPmDopudezIkqjA_yBRGb-Hhs-EJk1Epo6ZCq4H1lhUiKC1V9d7ugmkBaS5BfmowtFfc0sEFp8MkYdFKYh1a51uJnEJ2OfjErTT7zh4yIturP0kC8F4GAi9ACDO6nwIIKAa3s8kxPjfoKpuHD0tCAXeBFjnF814lbEaEU0yYTHv4UrrUJJlBR7gtd0KTO1qIh4ND1MHv8d5Pt8tpmW4G6c4WoPzfckHBHhMlMYPN3Rkk396TRRZvP5.I13UHlGyi34XqiNLmKpSiw\nKAGGLE_DATA_PROXY_URL=https://dp.kaggle.net\nKAGGLE_DOCKER_IMAGE=gcr.io/kaggle-gpu-images/python@sha256:57cb636a65386fd6c74fc9969211623034c487f7d483f9cd2c8456ebe2619345\nKAGGLE_GCP_ZONE=us-central1-c\nKAGGLE_GRPC_DATA_PROXY_URL=dp.kaggle.net:443\nKAGGLE_KERNEL_INTEGRATIONS=\nKAGGLE_KERNEL_RUN_TYPE=Interactive\nKAGGLE_URL_BASE=https://www.kaggle.com\nKAGGLE_USER_SECRETS_TOKEN=eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..-TvctxLKXbdT14m2-aoClw.PxFV_GBPbZF9rlft56YeJyv3Pu9XHUj1GYi0IcLW_0T3-_dWhKh56CYN3xskhOE6PT2q5T_TgXNqmK2zHY2EFVbU09Gb8hsTVzrjal5R20wZszpQTerPEi7LznBm-frp5lUAoxTKnTLHECjzTwQZog.ZwroTyau5OqFK24gb5u0SA\nKMP_LISTEN_PORT=6000\nKMP_TARGET_PORT=9000\nLANG=en_US.UTF-8\nLANGUAGE=en_US\nLAST_FORCED_REBUILD=20241216\nLC_ALL=en_US.UTF-8\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nLIBRARY_PATH=/usr/local/cuda/lib64/stubs\nLOCAL_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT=35269\nMKL_THREADING_LAYER=GNU\nMPLBACKEND=module://ipykernel.pylab.backend_inline\nNCCL_VERSION=2.19.3-1\nNVARCH=x86_64\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nNVIDIA_REQUIRE_CUDA=cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNVIDIA_VISIBLE_DEVICES=all\nNV_CUDA_COMPAT_PACKAGE=cuda-compat-12-2\nNV_CUDA_CUDART_DEV_VERSION=12.2.140-1\nNV_CUDA_CUDART_VERSION=12.2.140-1\nNV_CUDA_LIB_VERSION=12.2.2-1\nNV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-2=12.2.2-1\nNV_CUDA_NSIGHT_COMPUTE_VERSION=12.2.2-1\nNV_CUDNN_PACKAGE=libcudnn8=8.9.6.50-1+cuda12.2\nNV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.6.50-1+cuda12.2\nNV_CUDNN_PACKAGE_NAME=libcudnn8\nNV_CUDNN_VERSION=8.9.6.50\nNV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-2=12.2.5.6-1\nNV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-2\nNV_LIBCUBLAS_DEV_VERSION=12.2.5.6-1\nNV_LIBCUBLAS_PACKAGE=libcublas-12-2=12.2.5.6-1\nNV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-2\nNV_LIBCUBLAS_VERSION=12.2.5.6-1\nNV_LIBCUSPARSE_DEV_VERSION=12.1.2.141-1\nNV_LIBCUSPARSE_VERSION=12.1.2.141-1\nNV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\nNV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\nNV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\nNV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\nNV_LIBNCCL_PACKAGE_NAME=libnccl2\nNV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\nNV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-2=12.2.1.4-1\nNV_LIBNPP_DEV_VERSION=12.2.1.4-1\nNV_LIBNPP_PACKAGE=libnpp-12-2=12.2.1.4-1\nNV_LIBNPP_VERSION=12.2.1.4-1\nNV_NVML_DEV_VERSION=12.2.140-1\nNV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-2=12.2.142-1\nNV_NVPROF_VERSION=12.2.142-1\nNV_NVTX_VERSION=12.2.140-1\nPAGER=cat\nPATH=~/.local/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\nPWD=/kaggle/working/sam2\nPYTHONPATH=/kaggle/lib/kagglegym:/kaggle/lib:/kaggle/input/ai-mathematical-olympiad-progress-prize-2\nPYTHONUSERBASE=/root/.local\nRANK=0\nSHELL=/bin/bash\nSHLVL=0\nTCLLIBPATH=/usr/share/tcltk/tcllib1.20\nTERM=xterm-color\nTESSERACT_PATH=/usr/bin/tesseract\nTF2_BEHAVIOR=1\nTF_CPP_MIN_LOG_LEVEL=2\nTF_FORCE_GPU_ALLOW_GROWTH=true\nTORCH_NCCL_ASYNC_ERROR_HANDLING=1\nTPU_ML_PLATFORM=Tensorflow\nTPU_ML_PLATFORM_VERSION=2.17.1\nVM_GCE_METADATA_HOST=169.254.169.254\nWORLD_SIZE=4\n_=/usr/local/bin/python\n\nINFO 2025-01-26 14:49:39,804 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.\nINFO 2025-01-26 14:49:39,807 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /kaggle/working/sam2/sam2_logs/./configs/train.yaml/tensorboard\nINFO 2025-01-26 14:49:42,684 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5\nINFO 2025-01-26 14:49:42,689 trainer.py:1059: ====================\nINFO 2025-01-26 14:49:42,689 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>\nINFO 2025-01-26 14:49:42,694 trainer.py:1061: Model is SAM2Train(\n  (image_encoder): ImageEncoder(\n    (trunk): Hiera(\n      (patch_embed): PatchEmbed(\n        (proj): Conv2d(3, 144, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n      )\n      (blocks): ModuleList(\n        (0): MultiScaleBlock(\n          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=144, out_features=432, bias=True)\n            (proj): Linear(in_features=144, out_features=144, bias=True)\n          )\n          (drop_path): Identity()\n          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=144, out_features=576, bias=True)\n              (1): Linear(in_features=576, out_features=144, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (1): MultiScaleBlock(\n          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=144, out_features=432, bias=True)\n            (proj): Linear(in_features=144, out_features=144, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=144, out_features=576, bias=True)\n              (1): Linear(in_features=576, out_features=144, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (2): MultiScaleBlock(\n          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n          (attn): MultiScaleAttention(\n            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n            (qkv): Linear(in_features=144, out_features=864, bias=True)\n            (proj): Linear(in_features=288, out_features=288, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=288, out_features=1152, bias=True)\n              (1): Linear(in_features=1152, out_features=288, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n          (proj): Linear(in_features=144, out_features=288, bias=True)\n        )\n        (3-7): 5 x MultiScaleBlock(\n          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=288, out_features=864, bias=True)\n            (proj): Linear(in_features=288, out_features=288, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=288, out_features=1152, bias=True)\n              (1): Linear(in_features=1152, out_features=288, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (8): MultiScaleBlock(\n          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n          (attn): MultiScaleAttention(\n            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n            (qkv): Linear(in_features=288, out_features=1728, bias=True)\n            (proj): Linear(in_features=576, out_features=576, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=576, out_features=2304, bias=True)\n              (1): Linear(in_features=2304, out_features=576, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n          (proj): Linear(in_features=288, out_features=576, bias=True)\n        )\n        (9-43): 35 x MultiScaleBlock(\n          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=576, out_features=1728, bias=True)\n            (proj): Linear(in_features=576, out_features=576, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=576, out_features=2304, bias=True)\n              (1): Linear(in_features=2304, out_features=576, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (44): MultiScaleBlock(\n          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n          (attn): MultiScaleAttention(\n            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n            (qkv): Linear(in_features=576, out_features=3456, bias=True)\n            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=1152, out_features=4608, bias=True)\n              (1): Linear(in_features=4608, out_features=1152, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n          (proj): Linear(in_features=576, out_features=1152, bias=True)\n        )\n        (45-47): 3 x MultiScaleBlock(\n          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=1152, out_features=4608, bias=True)\n              (1): Linear(in_features=4608, out_features=1152, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n      )\n    )\n    (neck): FpnNeck(\n      (position_encoding): PositionEmbeddingSine()\n      (convs): ModuleList(\n        (0): Sequential(\n          (conv): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Sequential(\n          (conv): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Sequential(\n          (conv): Conv2d(288, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Sequential(\n          (conv): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n  (memory_attention): MemoryAttention(\n    (layers): ModuleList(\n      (0-3): 4 x MemoryAttentionLayer(\n        (self_attn): RoPEAttention(\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n        )\n        (cross_attn_image): RoPEAttention(\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (memory_encoder): MemoryEncoder(\n    (mask_downsampler): MaskDownSampler(\n      (encoder): Sequential(\n        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): LayerNorm2d()\n        (2): GELU(approximate='none')\n        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (4): LayerNorm2d()\n        (5): GELU(approximate='none')\n        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (7): LayerNorm2d()\n        (8): GELU(approximate='none')\n        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (10): LayerNorm2d()\n        (11): GELU(approximate='none')\n        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fuser): Fuser(\n      (proj): Identity()\n      (layers): ModuleList(\n        (0-1): 2 x CXBlock(\n          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n          (norm): LayerNorm2d()\n          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n          (act): GELU(approximate='none')\n          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n          (drop_path): Identity()\n        )\n      )\n    )\n    (position_encoding): PositionEmbeddingSine()\n    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (sam_prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (sam_mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=256, out_features=2048, bias=True)\n              (1): Linear(in_features=2048, out_features=256, bias=True)\n            )\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (obj_score_token): Embedding(1, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n      (act): ReLU()\n    )\n    (pred_obj_score_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=1, bias=True)\n      )\n      (act): ReLU()\n    )\n  )\n  (obj_ptr_proj): MLP(\n    (layers): ModuleList(\n      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n    )\n    (act): ReLU()\n  )\n  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n)\nINFO 2025-01-26 14:49:42,695 trainer.py:1062: \tTotal parameters 224 M\nINFO 2025-01-26 14:49:42,695 trainer.py:1063: \tTrainable parameters 224 M\nINFO 2025-01-26 14:49:42,695 trainer.py:1066: \tNon-Trainable parameters 0  \nINFO 2025-01-26 14:49:42,695 trainer.py:1069: ====================\nINFO 2025-01-26 14:49:42,700 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.\nINFO 2025-01-26 14:49:42,700 trainer.py: 314: Moving components to device cuda:0 and local rank 0.\nINFO 2025-01-26 14:49:43,010 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.\nINFO 2025-01-26 14:49:43,029 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.attn.qkv.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.mlp.layers.0.weight', 'image_encoder.trunk.blocks.36.attn.qkv.bias', 'image_encoder.trunk.blocks.27.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.34.attn.qkv.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.25.mlp.layers.0.weight', 'image_encoder.trunk.blocks.42.norm2.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.29.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.40.attn.qkv.weight', 'image_encoder.trunk.blocks.45.norm1.bias', 'image_encoder.trunk.blocks.26.mlp.layers.0.weight', 'image_encoder.trunk.blocks.34.attn.proj.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.24.attn.proj.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.44.mlp.layers.1.bias', 'image_encoder.trunk.blocks.43.mlp.layers.0.weight', 'image_encoder.trunk.blocks.37.attn.proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.35.norm1.weight', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.35.attn.proj.weight', 'image_encoder.trunk.blocks.47.norm1.bias', 'image_encoder.trunk.blocks.44.norm1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.33.norm2.weight', 'image_encoder.trunk.blocks.41.norm2.bias', 'image_encoder.trunk.blocks.28.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.37.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.33.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.31.mlp.layers.0.bias', 'image_encoder.trunk.blocks.40.norm1.bias', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.36.attn.qkv.weight', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.26.norm2.bias', 'image_encoder.trunk.blocks.41.attn.qkv.weight', 'image_encoder.trunk.blocks.38.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.norm2.bias', 'image_encoder.trunk.blocks.45.norm2.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.norm2.bias', 'image_encoder.trunk.blocks.42.attn.qkv.weight', 'image_encoder.trunk.blocks.37.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.34.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.37.norm1.weight', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.30.norm1.weight', 'image_encoder.trunk.blocks.41.attn.qkv.bias', 'image_encoder.trunk.blocks.33.mlp.layers.0.weight', 'image_encoder.trunk.blocks.36.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.30.norm2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.34.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.39.mlp.layers.1.weight', 'image_encoder.trunk.blocks.26.norm2.weight', 'image_encoder.trunk.blocks.24.attn.proj.bias', 'image_encoder.trunk.blocks.35.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.44.proj.weight', 'image_encoder.trunk.blocks.26.attn.proj.weight', 'image_encoder.trunk.blocks.34.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.24.norm2.weight', 'image_encoder.trunk.blocks.43.norm2.bias', 'image_encoder.trunk.blocks.47.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.44.norm2.weight', 'image_encoder.trunk.blocks.29.mlp.layers.1.bias', 'image_encoder.trunk.blocks.32.norm1.bias', 'image_encoder.trunk.blocks.28.norm2.bias', 'image_encoder.trunk.blocks.38.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.46.norm1.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.32.mlp.layers.0.weight', 'image_encoder.trunk.blocks.39.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.40.attn.proj.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.37.norm2.bias', 'image_encoder.trunk.blocks.39.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.24.norm1.bias', 'image_encoder.trunk.blocks.39.norm1.bias', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.36.attn.proj.weight', 'image_encoder.trunk.blocks.40.attn.proj.weight', 'image_encoder.trunk.blocks.26.attn.qkv.weight', 'image_encoder.trunk.blocks.45.attn.qkv.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.44.proj.bias', 'image_encoder.trunk.blocks.37.norm2.weight', 'image_encoder.trunk.blocks.30.mlp.layers.1.weight', 'image_encoder.trunk.blocks.47.mlp.layers.1.bias', 'image_encoder.trunk.blocks.40.mlp.layers.0.weight', 'image_encoder.trunk.blocks.29.attn.qkv.bias', 'image_encoder.trunk.blocks.43.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.38.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.45.attn.qkv.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.39.attn.proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.43.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.26.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.mlp.layers.1.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.27.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.37.attn.qkv.bias', 'image_encoder.trunk.blocks.28.mlp.layers.1.weight', 'image_encoder.trunk.blocks.35.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.32.norm2.weight', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.43.norm2.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.27.norm2.weight', 'image_encoder.trunk.blocks.29.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.31.attn.qkv.weight', 'image_encoder.trunk.blocks.38.norm1.bias', 'image_encoder.trunk.blocks.30.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.35.mlp.layers.1.weight', 'image_encoder.trunk.blocks.36.mlp.layers.1.weight', 'image_encoder.trunk.blocks.34.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.attn.proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.41.mlp.layers.0.bias', 'image_encoder.trunk.blocks.31.mlp.layers.1.weight', 'image_encoder.trunk.blocks.24.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.25.attn.proj.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.34.norm1.weight', 'image_encoder.trunk.blocks.45.attn.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.46.norm2.bias', 'image_encoder.trunk.blocks.42.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.33.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.39.mlp.layers.0.bias', 'image_encoder.trunk.blocks.45.attn.proj.bias', 'image_encoder.trunk.blocks.46.norm2.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.29.norm2.weight', 'image_encoder.trunk.blocks.39.attn.proj.weight', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.27.norm1.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.25.norm2.bias', 'image_encoder.trunk.blocks.43.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.norm1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.0.bias', 'image_encoder.trunk.blocks.38.attn.qkv.weight', 'image_encoder.trunk.blocks.38.attn.proj.weight', 'image_encoder.trunk.blocks.44.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.42.attn.proj.weight', 'image_encoder.trunk.blocks.38.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.attn.proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.28.norm1.bias', 'image_encoder.trunk.blocks.29.attn.qkv.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.39.norm1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.32.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.44.attn.proj.weight', 'image_encoder.trunk.blocks.43.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.46.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.47.attn.proj.weight', 'image_encoder.trunk.blocks.39.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.28.attn.qkv.weight', 'image_encoder.trunk.blocks.38.norm2.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.46.norm1.weight', 'image_encoder.trunk.blocks.47.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.29.norm1.weight', 'image_encoder.trunk.blocks.40.norm1.weight', 'image_encoder.trunk.blocks.26.mlp.layers.1.bias', 'image_encoder.trunk.blocks.28.norm1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.42.attn.proj.bias', 'image_encoder.trunk.blocks.44.attn.proj.bias', 'image_encoder.trunk.blocks.47.attn.qkv.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.32.norm2.bias', 'image_encoder.trunk.blocks.35.norm1.bias', 'image_encoder.trunk.blocks.39.attn.qkv.weight', 'image_encoder.trunk.blocks.32.attn.qkv.weight', 'image_encoder.trunk.blocks.46.attn.proj.weight', 'image_encoder.trunk.blocks.25.attn.qkv.weight', 'image_encoder.trunk.blocks.31.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.28.attn.qkv.bias', 'image_encoder.trunk.blocks.38.norm1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.26.norm1.weight', 'image_encoder.trunk.blocks.45.mlp.layers.1.bias', 'image_encoder.trunk.blocks.25.mlp.layers.1.bias', 'image_encoder.trunk.blocks.29.mlp.layers.1.weight', 'image_encoder.trunk.blocks.33.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.44.mlp.layers.0.bias', 'image_encoder.trunk.blocks.29.attn.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.42.norm2.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.28.norm2.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.40.mlp.layers.1.bias', 'image_encoder.trunk.blocks.34.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.36.norm2.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.32.attn.qkv.bias', 'image_encoder.trunk.blocks.32.mlp.layers.0.bias', 'image_encoder.trunk.blocks.36.mlp.layers.0.weight', 'image_encoder.trunk.blocks.29.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.37.attn.proj.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.27.mlp.layers.1.weight', 'image_encoder.trunk.blocks.47.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.28.attn.proj.weight', 'image_encoder.trunk.blocks.26.mlp.layers.1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.27.attn.proj.bias', 'image_encoder.trunk.blocks.32.norm1.weight', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.41.norm2.weight', 'image_encoder.trunk.blocks.44.norm1.weight', 'image_encoder.trunk.blocks.25.norm2.weight', 'image_encoder.trunk.blocks.28.attn.proj.bias', 'image_encoder.trunk.blocks.38.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.25.attn.qkv.bias', 'image_encoder.trunk.blocks.40.norm2.weight', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.27.mlp.layers.0.weight', 'image_encoder.trunk.blocks.35.attn.qkv.bias', 'image_encoder.trunk.blocks.30.norm2.bias', 'image_encoder.trunk.blocks.31.mlp.layers.0.weight', 'image_encoder.trunk.blocks.46.attn.qkv.weight', 'image_encoder.trunk.blocks.47.norm2.weight', 'image_encoder.trunk.blocks.45.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.37.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.24.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.44.attn.qkv.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.29.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.norm2.weight', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.28.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.32.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.28.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.39.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.41.attn.proj.weight', 'image_encoder.trunk.blocks.37.mlp.layers.1.bias', 'image_encoder.trunk.blocks.38.norm2.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.36.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.41.norm1.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.35.mlp.layers.0.weight', 'image_encoder.trunk.blocks.30.attn.qkv.bias', 'image_encoder.trunk.blocks.30.mlp.layers.1.bias', 'image_encoder.trunk.blocks.27.attn.qkv.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.43.attn.qkv.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.25.norm1.weight', 'image_encoder.trunk.blocks.40.norm2.bias', 'image_encoder.trunk.blocks.24.mlp.layers.0.weight', 'image_encoder.trunk.blocks.38.attn.proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.40.mlp.layers.1.weight', 'image_encoder.trunk.blocks.33.attn.qkv.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.44.norm2.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.42.attn.qkv.bias', 'image_encoder.trunk.blocks.31.attn.proj.weight', 'image_encoder.trunk.blocks.46.mlp.layers.1.bias', 'image_encoder.trunk.blocks.31.norm2.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.36.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.30.attn.proj.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.33.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.43.attn.qkv.bias', 'image_encoder.trunk.blocks.33.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.mlp.layers.1.bias', 'image_encoder.trunk.blocks.33.attn.proj.weight', 'image_encoder.trunk.blocks.31.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.30.norm1.bias', 'image_encoder.trunk.blocks.30.attn.qkv.weight', 'image_encoder.trunk.blocks.25.norm1.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.41.norm1.bias', 'image_encoder.trunk.blocks.42.norm1.weight', 'image_encoder.trunk.blocks.40.attn.qkv.bias', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.26.attn.proj.bias', 'image_encoder.trunk.blocks.31.attn.proj.bias', 'image_encoder.trunk.blocks.34.attn.qkv.bias', 'image_encoder.trunk.blocks.31.norm2.bias', 'image_encoder.trunk.blocks.24.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.33.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.8.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.27.attn.proj.weight', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.42.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.proj.weight', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.35.attn.qkv.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.33.norm2.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.24.attn.qkv.weight', 'image_encoder.trunk.blocks.47.attn.proj.bias', 'image_encoder.trunk.blocks.35.norm2.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.43.norm1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.30.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.47.attn.qkv.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.33.attn.proj.bias', 'image_encoder.trunk.blocks.42.mlp.layers.0.weight', 'image_encoder.trunk.blocks.41.mlp.layers.1.weight', 'image_encoder.trunk.blocks.35.mlp.layers.1.bias', 'image_encoder.trunk.blocks.35.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.41.mlp.layers.0.weight', 'image_encoder.trunk.blocks.44.mlp.layers.0.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.41.attn.proj.bias', 'image_encoder.trunk.blocks.24.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.47.norm2.bias', 'image_encoder.trunk.blocks.42.norm1.bias', 'image_encoder.trunk.blocks.47.mlp.layers.1.weight', 'image_encoder.trunk.blocks.36.attn.proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.43.attn.proj.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.36.norm2.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.41.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.32.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.31.attn.qkv.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.45.norm1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.29.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.45.norm2.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.27.attn.qkv.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.43.norm1.bias', 'image_encoder.trunk.blocks.46.attn.qkv.bias', 'image_encoder.trunk.blocks.46.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.36.norm1.weight', 'image_encoder.trunk.blocks.31.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.45.mlp.layers.0.weight', 'image_encoder.trunk.blocks.40.mlp.layers.0.bias', 'image_encoder.trunk.blocks.44.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.27.mlp.layers.1.bias', 'image_encoder.trunk.blocks.37.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.27.norm1.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.30.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.32.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.37.attn.qkv.weight', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.39.norm2.weight', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.42.mlp.layers.1.weight'}\nINFO 2025-01-26 14:49:43,033 optimizer.py: 248: Matches for param_name [*bias*]: {'memory_attention.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.36.attn.qkv.bias', 'image_encoder.trunk.blocks.27.norm2.bias', 'memory_attention.layers.3.norm3.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.42.norm2.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.29.attn.proj.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.45.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.44.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.37.attn.proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.47.norm1.bias', 'image_encoder.trunk.blocks.44.norm1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'image_encoder.trunk.blocks.41.norm2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'sam_mask_decoder.conv_s1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.37.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'memory_attention.layers.0.linear1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'image_encoder.trunk.blocks.31.mlp.layers.0.bias', 'image_encoder.trunk.blocks.40.norm1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.26.norm2.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.38.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.norm2.bias', 'image_encoder.trunk.blocks.34.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.41.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.36.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.24.attn.proj.bias', 'image_encoder.trunk.blocks.35.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.43.norm2.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.29.mlp.layers.1.bias', 'image_encoder.trunk.blocks.32.norm1.bias', 'image_encoder.trunk.blocks.28.norm2.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.46.norm1.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.40.attn.proj.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.37.norm2.bias', 'image_encoder.trunk.blocks.39.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.24.norm1.bias', 'image_encoder.trunk.blocks.39.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.45.attn.qkv.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.44.proj.bias', 'image_encoder.trunk.blocks.47.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'image_encoder.trunk.blocks.29.attn.qkv.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.38.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.39.attn.proj.bias', 'image_encoder.trunk.blocks.43.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.26.mlp.layers.0.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.27.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.37.attn.qkv.bias', 'image_encoder.trunk.blocks.35.attn.proj.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.29.norm2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.38.norm1.bias', 'image_encoder.trunk.blocks.30.attn.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.34.mlp.layers.1.bias', 'image_encoder.trunk.blocks.46.attn.proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.2.linear2.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.41.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.46.norm2.bias', 'image_encoder.trunk.blocks.42.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.33.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.39.mlp.layers.0.bias', 'image_encoder.trunk.blocks.45.attn.proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.25.norm2.bias', 'image_encoder.trunk.blocks.43.mlp.layers.0.bias', 'image_encoder.trunk.blocks.34.norm1.bias', 'image_encoder.trunk.blocks.45.mlp.layers.0.bias', 'image_encoder.trunk.blocks.44.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.38.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.attn.proj.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.28.norm1.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'image_encoder.trunk.blocks.32.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'memory_encoder.pix_feat_proj.bias', 'image_encoder.trunk.blocks.43.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'obj_ptr_proj.layers.1.bias', 'memory_attention.layers.1.linear2.bias', 'image_encoder.trunk.blocks.39.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.38.norm2.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.47.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'image_encoder.trunk.blocks.26.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.42.attn.proj.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'image_encoder.trunk.blocks.44.attn.proj.bias', 'image_encoder.trunk.blocks.32.norm2.bias', 'image_encoder.trunk.blocks.35.norm1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'image_encoder.trunk.blocks.31.norm1.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'obj_ptr_tpos_proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.28.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.45.mlp.layers.1.bias', 'image_encoder.trunk.blocks.25.mlp.layers.1.bias', 'image_encoder.trunk.blocks.33.attn.qkv.bias', 'image_encoder.trunk.blocks.44.mlp.layers.0.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.40.mlp.layers.1.bias', 'image_encoder.trunk.blocks.34.mlp.layers.0.bias', 'image_encoder.trunk.blocks.26.norm1.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.36.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.32.attn.qkv.bias', 'sam_mask_decoder.conv_s0.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.32.mlp.layers.0.bias', 'image_encoder.trunk.blocks.29.norm1.bias', 'memory_attention.layers.0.linear2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'image_encoder.trunk.blocks.27.attn.proj.bias', 'image_encoder.trunk.blocks.28.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.25.attn.qkv.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.35.attn.qkv.bias', 'image_encoder.trunk.blocks.30.norm2.bias', 'memory_attention.layers.3.linear2.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.24.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.29.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.28.mlp.layers.1.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.28.mlp.layers.0.bias', 'memory_encoder.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.39.mlp.layers.1.bias', 'image_encoder.trunk.blocks.37.mlp.layers.1.bias', 'image_encoder.trunk.blocks.36.mlp.layers.1.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.30.attn.qkv.bias', 'image_encoder.trunk.blocks.30.mlp.layers.1.bias', 'image_encoder.trunk.blocks.27.attn.qkv.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.40.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.38.attn.proj.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.44.norm2.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.42.attn.qkv.bias', 'image_encoder.trunk.blocks.46.mlp.layers.1.bias', 'image_encoder.neck.convs.2.conv.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.36.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'obj_ptr_proj.layers.0.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.33.mlp.layers.0.bias', 'image_encoder.trunk.blocks.43.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.24.mlp.layers.1.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.30.norm1.bias', 'image_encoder.trunk.blocks.25.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'image_encoder.trunk.blocks.41.norm1.bias', 'mask_downsample.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.40.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'image_encoder.trunk.blocks.26.attn.proj.bias', 'image_encoder.trunk.blocks.31.attn.proj.bias', 'image_encoder.trunk.blocks.34.attn.qkv.bias', 'image_encoder.trunk.blocks.31.norm2.bias', 'memory_attention.layers.3.linear1.bias', 'image_encoder.trunk.blocks.24.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.33.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.8.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.42.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.33.norm2.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.47.attn.proj.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.47.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.33.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.35.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.35.norm2.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.41.attn.proj.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.47.norm2.bias', 'image_encoder.trunk.blocks.42.norm1.bias', 'image_encoder.trunk.blocks.36.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.41.mlp.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.32.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.31.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.45.norm2.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.43.norm1.bias', 'image_encoder.trunk.blocks.46.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'image_encoder.trunk.blocks.46.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.31.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.40.mlp.layers.0.bias', 'image_encoder.trunk.blocks.27.mlp.layers.1.bias', 'image_encoder.trunk.blocks.37.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.27.norm1.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.30.mlp.layers.0.bias', 'image_encoder.trunk.blocks.25.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'memory_attention.layers.3.self_attn.q_proj.bias'}\nINFO 2025-01-26 14:49:43,035 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.26.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.36.norm2.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.31.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.32.norm2.weight', 'memory_attention.layers.2.norm1.weight', 'image_encoder.trunk.blocks.43.norm2.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.27.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.27.norm2.weight', 'image_encoder.trunk.blocks.29.norm2.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.29.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.38.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.42.norm2.bias', 'memory_attention.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.31.norm1.weight', 'image_encoder.trunk.blocks.45.norm1.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.30.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.25.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.47.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.41.norm1.bias', 'image_encoder.trunk.blocks.42.norm1.weight', 'memory_attention.layers.0.norm1.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.35.norm1.weight', 'image_encoder.trunk.blocks.47.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.32.norm1.weight', 'image_encoder.trunk.blocks.31.norm2.bias', 'image_encoder.trunk.blocks.44.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.24.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.34.norm1.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.33.norm2.weight', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.41.norm2.bias', 'image_encoder.trunk.blocks.41.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.46.norm2.bias', 'image_encoder.trunk.blocks.44.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.33.norm1.bias', 'image_encoder.trunk.blocks.33.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.25.norm2.weight', 'image_encoder.trunk.blocks.37.norm1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.46.norm2.weight', 'image_encoder.trunk.blocks.33.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.29.norm2.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.35.norm2.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.40.norm1.bias', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.40.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.26.norm2.bias', 'image_encoder.trunk.blocks.43.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.27.norm1.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'image_encoder.trunk.blocks.24.norm2.bias', 'image_encoder.trunk.blocks.45.norm2.weight', 'image_encoder.trunk.blocks.34.norm2.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.30.norm2.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.25.norm2.bias', 'image_encoder.trunk.blocks.47.norm2.weight', 'image_encoder.trunk.blocks.34.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.37.norm1.weight', 'memory_attention.layers.2.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.30.norm1.weight', 'image_encoder.trunk.blocks.35.norm2.bias', 'image_encoder.trunk.blocks.28.norm1.bias', 'image_encoder.trunk.blocks.36.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.30.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.39.norm1.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.47.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.42.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.26.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.34.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.24.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.36.norm2.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.43.norm2.bias', 'memory_attention.layers.2.norm3.bias', 'memory_attention.layers.3.norm1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.44.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.32.norm1.bias', 'image_encoder.trunk.blocks.28.norm2.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.46.norm1.bias', 'image_encoder.trunk.blocks.45.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.38.norm2.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.46.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.29.norm1.weight', 'image_encoder.trunk.blocks.40.norm1.weight', 'image_encoder.trunk.blocks.38.norm2.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'memory_attention.layers.3.norm1.weight', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.28.norm1.weight', 'image_encoder.trunk.blocks.41.norm1.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.45.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.32.norm2.bias', 'image_encoder.trunk.blocks.35.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.37.norm2.bias', 'image_encoder.trunk.blocks.39.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.24.norm1.bias', 'image_encoder.trunk.blocks.43.norm1.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.39.norm1.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.36.norm1.weight', 'image_encoder.trunk.blocks.31.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.37.norm2.weight', 'image_encoder.trunk.blocks.38.norm1.weight', 'image_encoder.trunk.blocks.27.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.26.norm1.weight', 'image_encoder.trunk.blocks.25.norm1.weight', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.40.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.42.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.39.norm2.weight', 'image_encoder.trunk.blocks.28.norm2.weight', 'image_encoder.trunk.blocks.44.norm2.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'memory_attention.norm.weight', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.weight'} \nRaw dataset length = 16621\nRaw dataset length = 16621\nRaw dataset length = 16621\nRaw dataset length = 16621\nINFO 2025-01-26 14:49:44,117 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n/kaggle/working/sam2/training/utils/checkpoint_utils.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f, map_location=map_location)\n/kaggle/working/sam2/training/utils/checkpoint_utils.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f, map_location=map_location)\n/kaggle/working/sam2/training/utils/checkpoint_utils.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f, map_location=map_location)\n/kaggle/working/sam2/training/utils/checkpoint_utils.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f, map_location=map_location)\nINFO 2025-01-26 14:49:44,736 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': './checkpoints/sam2.1_hiera_large.pt', 'ckpt_state_dict_keys': ['model']}}\n/kaggle/working/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(\n/kaggle/working/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(\n/kaggle/working/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(\n/kaggle/working/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\nbucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\nbucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\nbucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\ngrad.sizes() = [64, 256, 1, 1], strides() = [256, 1, 256, 256]\nbucket_view.sizes() = [64, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nINFO 2025-01-26 14:52:27,616 train_utils.py: 271: Train Epoch: [0][   0/4156] | Batch Time: 161.93 (161.93) | Data Time: 158.05 (158.05) | Mem (GB): 9.00 (9.00/9.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 5.10e-01 (5.10e-01)\nINFO 2025-01-26 14:52:35,291 train_utils.py: 271: Train Epoch: [0][  10/4156] | Batch Time: 0.70 (15.42) | Data Time: 0.00 (14.37) | Mem (GB): 10.00 (10.00/11.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 2.46e+00 (7.09e-01)\nINFO 2025-01-26 14:52:42,625 train_utils.py: 271: Train Epoch: [0][  20/4156] | Batch Time: 0.96 (8.43) | Data Time: 0.00 (7.53) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 5.63e-01 (7.66e-01)\nINFO 2025-01-26 14:52:49,678 train_utils.py: 271: Train Epoch: [0][  30/4156] | Batch Time: 0.71 (5.94) | Data Time: 0.00 (5.10) | Mem (GB): 10.00 (10.06/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 3.60e-01 (8.44e-01)\nINFO 2025-01-26 14:52:56,933 train_utils.py: 271: Train Epoch: [0][  40/4156] | Batch Time: 0.72 (4.66) | Data Time: 0.00 (3.86) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 4.97e-01 (7.78e-01)\nINFO 2025-01-26 14:53:04,219 train_utils.py: 271: Train Epoch: [0][  50/4156] | Batch Time: 0.97 (3.89) | Data Time: 0.00 (3.10) | Mem (GB): 10.00 (10.06/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.14e-01 (8.26e-01)\nINFO 2025-01-26 14:53:11,297 train_utils.py: 271: Train Epoch: [0][  60/4156] | Batch Time: 0.69 (3.37) | Data Time: 0.00 (2.59) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 9.48e-01 (7.61e-01)\nINFO 2025-01-26 14:53:18,597 train_utils.py: 271: Train Epoch: [0][  70/4156] | Batch Time: 0.69 (3.00) | Data Time: 0.00 (2.23) | Mem (GB): 10.00 (10.04/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.01e+00 (7.86e-01)\nINFO 2025-01-26 14:53:25,589 train_utils.py: 271: Train Epoch: [0][  80/4156] | Batch Time: 0.72 (2.71) | Data Time: 0.00 (1.95) | Mem (GB): 10.00 (10.04/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.09e+00 (7.84e-01)\nINFO 2025-01-26 14:53:32,604 train_utils.py: 271: Train Epoch: [0][  90/4156] | Batch Time: 0.72 (2.49) | Data Time: 0.00 (1.74) | Mem (GB): 10.00 (10.03/11.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 3.36e-01 (8.28e-01)\nINFO 2025-01-26 14:53:39,662 train_utils.py: 271: Train Epoch: [0][ 100/4156] | Batch Time: 0.73 (2.32) | Data Time: 0.00 (1.57) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.01e-01 (8.09e-01)\nINFO 2025-01-26 14:53:46,868 train_utils.py: 271: Train Epoch: [0][ 110/4156] | Batch Time: 0.68 (2.17) | Data Time: 0.00 (1.42) | Mem (GB): 10.00 (10.06/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.02e-01 (8.44e-01)\nINFO 2025-01-26 14:53:53,986 train_utils.py: 271: Train Epoch: [0][ 120/4156] | Batch Time: 0.70 (2.05) | Data Time: 0.00 (1.31) | Mem (GB): 10.00 (10.06/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.72e-01 (8.26e-01)\nINFO 2025-01-26 14:54:01,007 train_utils.py: 271: Train Epoch: [0][ 130/4156] | Batch Time: 0.69 (1.95) | Data Time: 0.00 (1.21) | Mem (GB): 10.00 (10.06/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.38e+00 (9.64e-01)\nINFO 2025-01-26 14:54:08,108 train_utils.py: 271: Train Epoch: [0][ 140/4156] | Batch Time: 0.72 (1.86) | Data Time: 0.00 (1.12) | Mem (GB): 10.00 (10.07/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.93e-01 (9.63e-01)\nINFO 2025-01-26 14:54:15,061 train_utils.py: 271: Train Epoch: [0][ 150/4156] | Batch Time: 0.69 (1.78) | Data Time: 0.00 (1.05) | Mem (GB): 11.00 (10.07/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.09e+00 (9.82e-01)\nINFO 2025-01-26 14:54:22,148 train_utils.py: 271: Train Epoch: [0][ 160/4156] | Batch Time: 0.72 (1.72) | Data Time: 0.00 (0.98) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.65e-01 (9.61e-01)\nINFO 2025-01-26 14:54:29,089 train_utils.py: 271: Train Epoch: [0][ 170/4156] | Batch Time: 0.71 (1.66) | Data Time: 0.00 (0.93) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.16e-01 (9.29e-01)\nINFO 2025-01-26 14:54:36,066 train_utils.py: 271: Train Epoch: [0][ 180/4156] | Batch Time: 0.68 (1.60) | Data Time: 0.00 (0.87) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 4.49e-01 (9.24e-01)\nINFO 2025-01-26 14:54:43,093 train_utils.py: 271: Train Epoch: [0][ 190/4156] | Batch Time: 0.71 (1.56) | Data Time: 0.00 (0.83) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 2.11e-01 (9.46e-01)\nINFO 2025-01-26 14:54:50,147 train_utils.py: 271: Train Epoch: [0][ 200/4156] | Batch Time: 0.70 (1.51) | Data Time: 0.00 (0.79) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 2.80e-01 (9.27e-01)\nINFO 2025-01-26 14:54:57,186 train_utils.py: 271: Train Epoch: [0][ 210/4156] | Batch Time: 0.69 (1.48) | Data Time: 0.00 (0.75) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.25e+00 (9.30e-01)\nINFO 2025-01-26 14:55:04,213 train_utils.py: 271: Train Epoch: [0][ 220/4156] | Batch Time: 0.71 (1.44) | Data Time: 0.00 (0.72) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 6.62e-01 (9.59e-01)\nINFO 2025-01-26 14:55:11,289 train_utils.py: 271: Train Epoch: [0][ 230/4156] | Batch Time: 0.74 (1.41) | Data Time: 0.00 (0.69) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 4.23e-01 (9.53e-01)\nINFO 2025-01-26 14:55:18,491 train_utils.py: 271: Train Epoch: [0][ 240/4156] | Batch Time: 0.72 (1.38) | Data Time: 0.00 (0.66) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 4.61e-01 (9.55e-01)\nINFO 2025-01-26 14:55:25,493 train_utils.py: 271: Train Epoch: [0][ 250/4156] | Batch Time: 0.68 (1.35) | Data Time: 0.00 (0.63) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 7.56e-01 (9.55e-01)\nINFO 2025-01-26 14:55:32,517 train_utils.py: 271: Train Epoch: [0][ 260/4156] | Batch Time: 0.71 (1.33) | Data Time: 0.00 (0.61) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 3.80e-01 (9.48e-01)\nINFO 2025-01-26 14:55:39,523 train_utils.py: 271: Train Epoch: [0][ 270/4156] | Batch Time: 0.68 (1.31) | Data Time: 0.00 (0.58) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.82e+00 (9.61e-01)\nINFO 2025-01-26 14:55:46,528 train_utils.py: 271: Train Epoch: [0][ 280/4156] | Batch Time: 0.69 (1.28) | Data Time: 0.00 (0.56) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 6.95e-01 (9.47e-01)\nINFO 2025-01-26 14:55:53,611 train_utils.py: 271: Train Epoch: [0][ 290/4156] | Batch Time: 0.70 (1.26) | Data Time: 0.00 (0.54) | Mem (GB): 10.00 (10.08/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 3.80e-01 (9.47e-01)\nINFO 2025-01-26 14:56:00,566 train_utils.py: 271: Train Epoch: [0][ 300/4156] | Batch Time: 0.70 (1.25) | Data Time: 0.00 (0.53) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.96e-01 (9.38e-01)\nINFO 2025-01-26 14:56:07,614 train_utils.py: 271: Train Epoch: [0][ 310/4156] | Batch Time: 0.72 (1.23) | Data Time: 0.00 (0.51) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 4.27e-01 (9.32e-01)\nINFO 2025-01-26 14:56:14,728 train_utils.py: 271: Train Epoch: [0][ 320/4156] | Batch Time: 0.72 (1.21) | Data Time: 0.00 (0.49) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.37e-01 (9.28e-01)\nINFO 2025-01-26 14:56:21,765 train_utils.py: 271: Train Epoch: [0][ 330/4156] | Batch Time: 0.68 (1.20) | Data Time: 0.00 (0.48) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.91e-01 (9.32e-01)\nINFO 2025-01-26 14:56:29,042 train_utils.py: 271: Train Epoch: [0][ 340/4156] | Batch Time: 0.72 (1.18) | Data Time: 0.00 (0.46) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 7.19e-01 (9.33e-01)\nINFO 2025-01-26 14:56:36,121 train_utils.py: 271: Train Epoch: [0][ 350/4156] | Batch Time: 0.69 (1.17) | Data Time: 0.00 (0.45) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.10e+00 (9.29e-01)\nINFO 2025-01-26 14:56:43,134 train_utils.py: 271: Train Epoch: [0][ 360/4156] | Batch Time: 0.72 (1.16) | Data Time: 0.00 (0.44) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.84e-01 (9.56e-01)\nINFO 2025-01-26 14:56:50,045 train_utils.py: 271: Train Epoch: [0][ 370/4156] | Batch Time: 0.72 (1.14) | Data Time: 0.00 (0.43) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 5.01e-01 (9.48e-01)\nINFO 2025-01-26 14:56:57,076 train_utils.py: 271: Train Epoch: [0][ 380/4156] | Batch Time: 0.71 (1.13) | Data Time: 0.00 (0.42) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 6.12e-01 (9.45e-01)\nINFO 2025-01-26 14:57:04,047 train_utils.py: 271: Train Epoch: [0][ 390/4156] | Batch Time: 0.69 (1.12) | Data Time: 0.00 (0.41) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.22e-01 (9.53e-01)\nINFO 2025-01-26 14:57:11,070 train_utils.py: 271: Train Epoch: [0][ 400/4156] | Batch Time: 0.74 (1.11) | Data Time: 0.00 (0.39) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 5.53e-01 (9.47e-01)\nINFO 2025-01-26 14:57:18,186 train_utils.py: 271: Train Epoch: [0][ 410/4156] | Batch Time: 0.68 (1.10) | Data Time: 0.00 (0.39) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.69e-01 (9.50e-01)\nINFO 2025-01-26 14:57:25,192 train_utils.py: 271: Train Epoch: [0][ 420/4156] | Batch Time: 0.69 (1.09) | Data Time: 0.00 (0.38) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.81e-01 (9.53e-01)\nINFO 2025-01-26 14:57:32,353 train_utils.py: 271: Train Epoch: [0][ 430/4156] | Batch Time: 0.71 (1.08) | Data Time: 0.00 (0.37) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 3.41e-01 (9.61e-01)\nINFO 2025-01-26 14:57:39,342 train_utils.py: 271: Train Epoch: [0][ 440/4156] | Batch Time: 0.72 (1.07) | Data Time: 0.00 (0.36) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.01e-01 (9.57e-01)\nINFO 2025-01-26 14:57:46,384 train_utils.py: 271: Train Epoch: [0][ 450/4156] | Batch Time: 0.68 (1.07) | Data Time: 0.00 (0.35) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 8.08e-01 (9.96e-01)\nINFO 2025-01-26 14:57:53,423 train_utils.py: 271: Train Epoch: [0][ 460/4156] | Batch Time: 0.69 (1.06) | Data Time: 0.00 (0.34) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 3.77e-01 (1.00e+00)\nINFO 2025-01-26 14:58:00,478 train_utils.py: 271: Train Epoch: [0][ 470/4156] | Batch Time: 0.71 (1.05) | Data Time: 0.00 (0.34) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 3.11e-01 (9.91e-01)\nINFO 2025-01-26 14:58:07,572 train_utils.py: 271: Train Epoch: [0][ 480/4156] | Batch Time: 0.71 (1.04) | Data Time: 0.00 (0.33) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 5.32e-01 (9.86e-01)\nINFO 2025-01-26 14:58:14,600 train_utils.py: 271: Train Epoch: [0][ 490/4156] | Batch Time: 0.72 (1.04) | Data Time: 0.00 (0.32) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.21e-01 (9.80e-01)\nINFO 2025-01-26 14:58:21,812 train_utils.py: 271: Train Epoch: [0][ 500/4156] | Batch Time: 0.73 (1.03) | Data Time: 0.00 (0.32) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 5.57e-01 (9.69e-01)\nINFO 2025-01-26 14:58:28,838 train_utils.py: 271: Train Epoch: [0][ 510/4156] | Batch Time: 0.71 (1.02) | Data Time: 0.00 (0.31) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.27e-01 (9.67e-01)\nINFO 2025-01-26 14:58:35,946 train_utils.py: 271: Train Epoch: [0][ 520/4156] | Batch Time: 0.72 (1.02) | Data Time: 0.00 (0.30) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 9.01e-01 (9.59e-01)\nINFO 2025-01-26 14:58:42,989 train_utils.py: 271: Train Epoch: [0][ 530/4156] | Batch Time: 0.72 (1.01) | Data Time: 0.00 (0.30) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 3.73e-01 (9.64e-01)\nINFO 2025-01-26 14:58:49,962 train_utils.py: 271: Train Epoch: [0][ 540/4156] | Batch Time: 0.68 (1.01) | Data Time: 0.00 (0.29) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 4.77e-01 (9.58e-01)\nINFO 2025-01-26 14:58:56,984 train_utils.py: 271: Train Epoch: [0][ 550/4156] | Batch Time: 0.69 (1.00) | Data Time: 0.00 (0.29) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 2.05e-01 (9.50e-01)\nINFO 2025-01-26 14:59:04,120 train_utils.py: 271: Train Epoch: [0][ 560/4156] | Batch Time: 0.71 (1.00) | Data Time: 0.00 (0.28) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 3.09e-01 (9.43e-01)\nINFO 2025-01-26 14:59:11,227 train_utils.py: 271: Train Epoch: [0][ 570/4156] | Batch Time: 0.72 (0.99) | Data Time: 0.00 (0.28) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 6.17e-01 (9.37e-01)\nINFO 2025-01-26 14:59:18,291 train_utils.py: 271: Train Epoch: [0][ 580/4156] | Batch Time: 0.68 (0.99) | Data Time: 0.00 (0.27) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 2.80e-01 (9.33e-01)\nINFO 2025-01-26 14:59:25,337 train_utils.py: 271: Train Epoch: [0][ 590/4156] | Batch Time: 0.72 (0.98) | Data Time: 0.00 (0.27) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.08e+00 (9.25e-01)\nINFO 2025-01-26 14:59:32,359 train_utils.py: 271: Train Epoch: [0][ 600/4156] | Batch Time: 0.73 (0.98) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 6.57e-01 (9.42e-01)\nINFO 2025-01-26 14:59:39,456 train_utils.py: 271: Train Epoch: [0][ 610/4156] | Batch Time: 0.69 (0.97) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 4.67e-01 (9.42e-01)\nINFO 2025-01-26 14:59:46,633 train_utils.py: 271: Train Epoch: [0][ 620/4156] | Batch Time: 0.72 (0.97) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.00e-01 (9.35e-01)\nINFO 2025-01-26 14:59:53,701 train_utils.py: 271: Train Epoch: [0][ 630/4156] | Batch Time: 0.69 (0.96) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.35e-01 (9.26e-01)\nINFO 2025-01-26 15:00:00,688 train_utils.py: 271: Train Epoch: [0][ 640/4156] | Batch Time: 0.69 (0.96) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.85e-01 (9.22e-01)\nINFO 2025-01-26 15:00:07,761 train_utils.py: 271: Train Epoch: [0][ 650/4156] | Batch Time: 0.74 (0.96) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.32e-01 (9.18e-01)\nINFO 2025-01-26 15:00:14,860 train_utils.py: 271: Train Epoch: [0][ 660/4156] | Batch Time: 0.69 (0.95) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.11e-01 (9.09e-01)\nINFO 2025-01-26 15:00:22,054 train_utils.py: 271: Train Epoch: [0][ 670/4156] | Batch Time: 0.71 (0.95) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 3.62e-01 (9.10e-01)\nINFO 2025-01-26 15:00:29,075 train_utils.py: 271: Train Epoch: [0][ 680/4156] | Batch Time: 0.72 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 1.53e+00 (9.20e-01)\nINFO 2025-01-26 15:00:36,155 train_utils.py: 271: Train Epoch: [0][ 690/4156] | Batch Time: 0.69 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 1.86e-01 (9.18e-01)\nINFO 2025-01-26 15:00:43,193 train_utils.py: 271: Train Epoch: [0][ 700/4156] | Batch Time: 0.73 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.49e+00 (9.26e-01)\nINFO 2025-01-26 15:00:50,209 train_utils.py: 271: Train Epoch: [0][ 710/4156] | Batch Time: 0.69 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 3.19e-01 (9.21e-01)\nINFO 2025-01-26 15:00:57,261 train_utils.py: 271: Train Epoch: [0][ 720/4156] | Batch Time: 0.71 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.25e+00 (9.20e-01)\nINFO 2025-01-26 15:01:04,367 train_utils.py: 271: Train Epoch: [0][ 730/4156] | Batch Time: 0.71 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 5.24e-01 (9.20e-01)\nINFO 2025-01-26 15:01:11,385 train_utils.py: 271: Train Epoch: [0][ 740/4156] | Batch Time: 0.69 (0.93) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 2.10e-01 (9.23e-01)\nINFO 2025-01-26 15:01:18,541 train_utils.py: 271: Train Epoch: [0][ 750/4156] | Batch Time: 0.73 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 9.73e-01 (9.17e-01)\nINFO 2025-01-26 15:01:25,642 train_utils.py: 271: Train Epoch: [0][ 760/4156] | Batch Time: 0.70 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 7.51e-01 (9.15e-01)\nINFO 2025-01-26 15:01:32,744 train_utils.py: 271: Train Epoch: [0][ 770/4156] | Batch Time: 0.69 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 4.42e-01 (9.11e-01)\nINFO 2025-01-26 15:01:39,702 train_utils.py: 271: Train Epoch: [0][ 780/4156] | Batch Time: 0.69 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 4.40e-01 (9.08e-01)\nINFO 2025-01-26 15:01:46,667 train_utils.py: 271: Train Epoch: [0][ 790/4156] | Batch Time: 0.69 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 5.71e-01 (9.05e-01)\nINFO 2025-01-26 15:01:53,736 train_utils.py: 271: Train Epoch: [0][ 800/4156] | Batch Time: 0.73 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 4.09e-01 (9.05e-01)\nINFO 2025-01-26 15:02:00,812 train_utils.py: 271: Train Epoch: [0][ 810/4156] | Batch Time: 0.72 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 4.25e-01 (8.98e-01)\nINFO 2025-01-26 15:02:07,869 train_utils.py: 271: Train Epoch: [0][ 820/4156] | Batch Time: 0.72 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.95e-01 (8.94e-01)\nINFO 2025-01-26 15:02:14,913 train_utils.py: 271: Train Epoch: [0][ 830/4156] | Batch Time: 0.69 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.19e+00 (8.90e-01)\nINFO 2025-01-26 15:02:21,986 train_utils.py: 271: Train Epoch: [0][ 840/4156] | Batch Time: 0.72 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 8.70e-01 (9.17e-01)\nINFO 2025-01-26 15:02:28,997 train_utils.py: 271: Train Epoch: [0][ 850/4156] | Batch Time: 0.72 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.53e+00 (9.15e-01)\nINFO 2025-01-26 15:02:36,056 train_utils.py: 271: Train Epoch: [0][ 860/4156] | Batch Time: 0.72 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.56e+00 (9.13e-01)\nINFO 2025-01-26 15:02:43,009 train_utils.py: 271: Train Epoch: [0][ 870/4156] | Batch Time: 0.68 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 1.45e+00 (9.10e-01)\nINFO 2025-01-26 15:02:50,041 train_utils.py: 271: Train Epoch: [0][ 880/4156] | Batch Time: 0.71 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 5.49e+00 (9.17e-01)\nINFO 2025-01-26 15:02:57,022 train_utils.py: 271: Train Epoch: [0][ 890/4156] | Batch Time: 0.69 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 9.95e-01 (9.17e-01)\nINFO 2025-01-26 15:03:04,105 train_utils.py: 271: Train Epoch: [0][ 900/4156] | Batch Time: 0.73 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 2.21e-01 (9.16e-01)\nINFO 2025-01-26 15:03:11,066 train_utils.py: 271: Train Epoch: [0][ 910/4156] | Batch Time: 0.68 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 6.11e-01 (9.22e-01)\nINFO 2025-01-26 15:03:18,105 train_utils.py: 271: Train Epoch: [0][ 920/4156] | Batch Time: 0.70 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 2.60e+00 (9.20e-01)\nINFO 2025-01-26 15:03:25,048 train_utils.py: 271: Train Epoch: [0][ 930/4156] | Batch Time: 0.68 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 4.73e-01 (9.19e-01)\nINFO 2025-01-26 15:03:32,022 train_utils.py: 271: Train Epoch: [0][ 940/4156] | Batch Time: 0.71 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 5.52e-01 (9.15e-01)\nINFO 2025-01-26 15:03:39,039 train_utils.py: 271: Train Epoch: [0][ 950/4156] | Batch Time: 0.71 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 5.77e-01 (9.13e-01)\nINFO 2025-01-26 15:03:46,077 train_utils.py: 271: Train Epoch: [0][ 960/4156] | Batch Time: 0.68 (0.87) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 4.91e-01 (9.10e-01)\nINFO 2025-01-26 15:03:53,106 train_utils.py: 271: Train Epoch: [0][ 970/4156] | Batch Time: 0.69 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 3.34e-01 (9.07e-01)\nINFO 2025-01-26 15:04:00,226 train_utils.py: 271: Train Epoch: [0][ 980/4156] | Batch Time: 0.72 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.02e+00 (9.07e-01)\nINFO 2025-01-26 15:04:07,278 train_utils.py: 271: Train Epoch: [0][ 990/4156] | Batch Time: 0.68 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.08e+00 (9.03e-01)\nINFO 2025-01-26 15:04:14,402 train_utils.py: 271: Train Epoch: [0][1000/4156] | Batch Time: 0.73 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 2.99e-01 (9.00e-01)\nINFO 2025-01-26 15:04:21,442 train_utils.py: 271: Train Epoch: [0][1010/4156] | Batch Time: 0.68 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 1.09e+00 (9.02e-01)\nINFO 2025-01-26 15:04:28,492 train_utils.py: 271: Train Epoch: [0][1020/4156] | Batch Time: 0.72 (0.86) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 2.39e-01 (8.99e-01)\nINFO 2025-01-26 15:04:35,537 train_utils.py: 271: Train Epoch: [0][1030/4156] | Batch Time: 0.69 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 9.85e-01 (8.94e-01)\nINFO 2025-01-26 15:04:42,698 train_utils.py: 271: Train Epoch: [0][1040/4156] | Batch Time: 0.88 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 3.15e-01 (8.92e-01)\nINFO 2025-01-26 15:04:50,056 train_utils.py: 271: Train Epoch: [0][1050/4156] | Batch Time: 0.71 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 6.17e-01 (8.88e-01)\nINFO 2025-01-26 15:04:57,068 train_utils.py: 271: Train Epoch: [0][1060/4156] | Batch Time: 0.69 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 7.43e-01 (8.89e-01)\nINFO 2025-01-26 15:05:04,165 train_utils.py: 271: Train Epoch: [0][1070/4156] | Batch Time: 0.70 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 2.49e+00 (8.87e-01)\nINFO 2025-01-26 15:05:11,166 train_utils.py: 271: Train Epoch: [0][1080/4156] | Batch Time: 0.70 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 8.13e-01 (8.93e-01)\nINFO 2025-01-26 15:05:18,472 train_utils.py: 271: Train Epoch: [0][1090/4156] | Batch Time: 1.00 (0.85) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 1.30e+00 (8.92e-01)\nINFO 2025-01-26 15:05:25,630 train_utils.py: 271: Train Epoch: [0][1100/4156] | Batch Time: 0.74 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 2.49e-01 (8.90e-01)\nINFO 2025-01-26 15:05:32,674 train_utils.py: 271: Train Epoch: [0][1110/4156] | Batch Time: 0.72 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 3.75e-01 (8.94e-01)\nINFO 2025-01-26 15:05:39,795 train_utils.py: 271: Train Epoch: [0][1120/4156] | Batch Time: 0.72 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.65e-01 (8.92e-01)\nINFO 2025-01-26 15:05:46,932 train_utils.py: 271: Train Epoch: [0][1130/4156] | Batch Time: 0.72 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 2.54e-01 (8.90e-01)\nINFO 2025-01-26 15:05:53,961 train_utils.py: 271: Train Epoch: [0][1140/4156] | Batch Time: 0.69 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.38e+00 (8.87e-01)\nINFO 2025-01-26 15:06:01,024 train_utils.py: 271: Train Epoch: [0][1150/4156] | Batch Time: 0.73 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.08e+00 (8.86e-01)\nINFO 2025-01-26 15:06:08,098 train_utils.py: 271: Train Epoch: [0][1160/4156] | Batch Time: 0.72 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 5.66e-01 (8.82e-01)\nINFO 2025-01-26 15:06:15,183 train_utils.py: 271: Train Epoch: [0][1170/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 6.70e-01 (8.79e-01)\nINFO 2025-01-26 15:06:22,110 train_utils.py: 271: Train Epoch: [0][1180/4156] | Batch Time: 0.71 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 4.81e-01 (8.77e-01)\nINFO 2025-01-26 15:06:29,259 train_utils.py: 271: Train Epoch: [0][1190/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 3.94e-01 (8.75e-01)\nINFO 2025-01-26 15:06:36,384 train_utils.py: 271: Train Epoch: [0][1200/4156] | Batch Time: 0.75 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 3.03e-01 (8.73e-01)\nINFO 2025-01-26 15:06:43,555 train_utils.py: 271: Train Epoch: [0][1210/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.11e+00 (8.72e-01)\nINFO 2025-01-26 15:06:50,519 train_utils.py: 271: Train Epoch: [0][1220/4156] | Batch Time: 0.71 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 2.76e-01 (8.70e-01)\nINFO 2025-01-26 15:06:57,584 train_utils.py: 271: Train Epoch: [0][1230/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 5.02e-01 (8.66e-01)\nINFO 2025-01-26 15:07:04,672 train_utils.py: 271: Train Epoch: [0][1240/4156] | Batch Time: 0.68 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 4.30e-01 (8.65e-01)\nINFO 2025-01-26 15:07:11,737 train_utils.py: 271: Train Epoch: [0][1250/4156] | Batch Time: 0.68 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 5.75e-01 (8.63e-01)\nINFO 2025-01-26 15:07:18,784 train_utils.py: 271: Train Epoch: [0][1260/4156] | Batch Time: 0.71 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 5.77e-01 (8.62e-01)\nINFO 2025-01-26 15:07:25,800 train_utils.py: 271: Train Epoch: [0][1270/4156] | Batch Time: 0.72 (0.83) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.00e+00 (8.75e-01)\nINFO 2025-01-26 15:07:32,881 train_utils.py: 271: Train Epoch: [0][1280/4156] | Batch Time: 0.74 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 5.84e-01 (8.71e-01)\nINFO 2025-01-26 15:07:39,883 train_utils.py: 271: Train Epoch: [0][1290/4156] | Batch Time: 0.68 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 5.81e-01 (8.72e-01)\nINFO 2025-01-26 15:07:46,900 train_utils.py: 271: Train Epoch: [0][1300/4156] | Batch Time: 0.70 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 2.98e-01 (8.73e-01)\nINFO 2025-01-26 15:07:53,803 train_utils.py: 271: Train Epoch: [0][1310/4156] | Batch Time: 0.68 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 2.99e-01 (8.77e-01)\nINFO 2025-01-26 15:08:00,817 train_utils.py: 271: Train Epoch: [0][1320/4156] | Batch Time: 0.71 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 1.38e-01 (8.76e-01)\nINFO 2025-01-26 15:08:07,900 train_utils.py: 271: Train Epoch: [0][1330/4156] | Batch Time: 0.72 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 5.86e-01 (8.74e-01)\nINFO 2025-01-26 15:08:14,893 train_utils.py: 271: Train Epoch: [0][1340/4156] | Batch Time: 0.69 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 1.18e+00 (8.73e-01)\nINFO 2025-01-26 15:08:21,930 train_utils.py: 271: Train Epoch: [0][1350/4156] | Batch Time: 0.68 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 4.85e-01 (8.73e-01)\nINFO 2025-01-26 15:08:29,015 train_utils.py: 271: Train Epoch: [0][1360/4156] | Batch Time: 0.72 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 4.27e-01 (8.71e-01)\nINFO 2025-01-26 15:08:35,998 train_utils.py: 271: Train Epoch: [0][1370/4156] | Batch Time: 0.68 (0.82) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 1.82e-01 (8.68e-01)\nINFO 2025-01-26 15:08:43,086 train_utils.py: 271: Train Epoch: [0][1380/4156] | Batch Time: 0.72 (0.82) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 9.68e-01 (8.66e-01)\nINFO 2025-01-26 15:08:50,121 train_utils.py: 271: Train Epoch: [0][1390/4156] | Batch Time: 0.71 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 3.77e-01 (8.64e-01)\nINFO 2025-01-26 15:08:57,146 train_utils.py: 271: Train Epoch: [0][1400/4156] | Batch Time: 0.70 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 3.10e+00 (8.66e-01)\nINFO 2025-01-26 15:09:04,182 train_utils.py: 271: Train Epoch: [0][1410/4156] | Batch Time: 0.69 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 4.88e-01 (8.65e-01)\nINFO 2025-01-26 15:09:11,152 train_utils.py: 271: Train Epoch: [0][1420/4156] | Batch Time: 0.70 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 4.66e+00 (8.67e-01)\nINFO 2025-01-26 15:09:18,223 train_utils.py: 271: Train Epoch: [0][1430/4156] | Batch Time: 0.72 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 1.33e-01 (8.64e-01)\nINFO 2025-01-26 15:09:25,251 train_utils.py: 271: Train Epoch: [0][1440/4156] | Batch Time: 0.71 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 4.75e-01 (8.63e-01)\nINFO 2025-01-26 15:09:32,507 train_utils.py: 271: Train Epoch: [0][1450/4156] | Batch Time: 0.71 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.78e-01 (8.61e-01)\nINFO 2025-01-26 15:09:39,608 train_utils.py: 271: Train Epoch: [0][1460/4156] | Batch Time: 0.71 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.11e+00 (8.60e-01)\nINFO 2025-01-26 15:09:46,714 train_utils.py: 271: Train Epoch: [0][1470/4156] | Batch Time: 0.71 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.62e-01 (8.59e-01)\nINFO 2025-01-26 15:09:53,795 train_utils.py: 271: Train Epoch: [0][1480/4156] | Batch Time: 0.72 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 2.08e-01 (8.57e-01)\nINFO 2025-01-26 15:10:00,925 train_utils.py: 271: Train Epoch: [0][1490/4156] | Batch Time: 0.71 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 3.42e-01 (8.55e-01)\nINFO 2025-01-26 15:10:08,087 train_utils.py: 271: Train Epoch: [0][1500/4156] | Batch Time: 0.73 (0.81) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 4.10e-01 (8.55e-01)\nINFO 2025-01-26 15:10:15,313 train_utils.py: 271: Train Epoch: [0][1510/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 2.96e-01 (8.61e-01)\nINFO 2025-01-26 15:10:22,451 train_utils.py: 271: Train Epoch: [0][1520/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.97e-01 (8.59e-01)\nINFO 2025-01-26 15:10:29,572 train_utils.py: 271: Train Epoch: [0][1530/4156] | Batch Time: 0.73 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.68e-01 (8.57e-01)\nINFO 2025-01-26 15:10:36,636 train_utils.py: 271: Train Epoch: [0][1540/4156] | Batch Time: 0.70 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.41e-01 (8.56e-01)\nINFO 2025-01-26 15:10:43,719 train_utils.py: 271: Train Epoch: [0][1550/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.47e+00 (8.65e-01)\nINFO 2025-01-26 15:10:50,777 train_utils.py: 271: Train Epoch: [0][1560/4156] | Batch Time: 0.70 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 2.76e-01 (8.62e-01)\nINFO 2025-01-26 15:10:57,887 train_utils.py: 271: Train Epoch: [0][1570/4156] | Batch Time: 0.68 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 4.57e-01 (8.63e-01)\nINFO 2025-01-26 15:11:04,834 train_utils.py: 271: Train Epoch: [0][1580/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.52e-01 (8.61e-01)\nINFO 2025-01-26 15:11:11,851 train_utils.py: 271: Train Epoch: [0][1590/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.35e+00 (8.61e-01)\nINFO 2025-01-26 15:11:18,999 train_utils.py: 271: Train Epoch: [0][1600/4156] | Batch Time: 0.73 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 8.19e-01 (8.59e-01)\nINFO 2025-01-26 15:11:26,099 train_utils.py: 271: Train Epoch: [0][1610/4156] | Batch Time: 0.69 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 2.73e+00 (8.61e-01)\nINFO 2025-01-26 15:11:33,160 train_utils.py: 271: Train Epoch: [0][1620/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 5.95e-01 (8.62e-01)\nINFO 2025-01-26 15:11:40,176 train_utils.py: 271: Train Epoch: [0][1630/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 1.50e-01 (8.59e-01)\nINFO 2025-01-26 15:11:47,227 train_utils.py: 271: Train Epoch: [0][1640/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 7.60e-01 (8.60e-01)\nINFO 2025-01-26 15:11:54,406 train_utils.py: 271: Train Epoch: [0][1650/4156] | Batch Time: 0.69 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 3.44e-01 (8.58e-01)\nINFO 2025-01-26 15:12:01,440 train_utils.py: 271: Train Epoch: [0][1660/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 9.62e-01 (8.58e-01)\nINFO 2025-01-26 15:12:08,450 train_utils.py: 271: Train Epoch: [0][1670/4156] | Batch Time: 0.71 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 4.61e-01 (8.56e-01)\nINFO 2025-01-26 15:12:15,420 train_utils.py: 271: Train Epoch: [0][1680/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 5.17e+00 (8.57e-01)\nINFO 2025-01-26 15:12:22,499 train_utils.py: 271: Train Epoch: [0][1690/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 4.01e-01 (8.54e-01)\nINFO 2025-01-26 15:12:29,529 train_utils.py: 271: Train Epoch: [0][1700/4156] | Batch Time: 0.74 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 9.25e-01 (8.54e-01)\nINFO 2025-01-26 15:12:36,617 train_utils.py: 271: Train Epoch: [0][1710/4156] | Batch Time: 0.73 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 4.21e-01 (8.52e-01)\nINFO 2025-01-26 15:12:43,765 train_utils.py: 271: Train Epoch: [0][1720/4156] | Batch Time: 0.73 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 5.35e-01 (8.50e-01)\nINFO 2025-01-26 15:12:50,761 train_utils.py: 271: Train Epoch: [0][1730/4156] | Batch Time: 0.69 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 1.81e+00 (8.49e-01)\nINFO 2025-01-26 15:12:57,708 train_utils.py: 271: Train Epoch: [0][1740/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 3.28e-01 (8.48e-01)\nINFO 2025-01-26 15:13:05,000 train_utils.py: 271: Train Epoch: [0][1750/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 2.82e-01 (8.51e-01)\nINFO 2025-01-26 15:13:12,007 train_utils.py: 271: Train Epoch: [0][1760/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 2.50e+00 (8.54e-01)\nINFO 2025-01-26 15:13:19,033 train_utils.py: 271: Train Epoch: [0][1770/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 12.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 1.17e+00 (8.52e-01)\nINFO 2025-01-26 15:13:26,169 train_utils.py: 271: Train Epoch: [0][1780/4156] | Batch Time: 0.70 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 5.83e-01 (8.51e-01)\nINFO 2025-01-26 15:13:33,203 train_utils.py: 271: Train Epoch: [0][1790/4156] | Batch Time: 0.69 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.42e-01 (8.49e-01)\nINFO 2025-01-26 15:13:40,560 train_utils.py: 271: Train Epoch: [0][1800/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 12.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 6.90e-01 (8.48e-01)\nINFO 2025-01-26 15:13:47,596 train_utils.py: 271: Train Epoch: [0][1810/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.61e-01 (8.47e-01)\nINFO 2025-01-26 15:13:54,682 train_utils.py: 271: Train Epoch: [0][1820/4156] | Batch Time: 0.71 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 4.27e-01 (8.44e-01)\nINFO 2025-01-26 15:14:01,774 train_utils.py: 271: Train Epoch: [0][1830/4156] | Batch Time: 0.70 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.23e+00 (8.42e-01)\nINFO 2025-01-26 15:14:08,822 train_utils.py: 271: Train Epoch: [0][1840/4156] | Batch Time: 0.73 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 2.25e-01 (8.41e-01)\nINFO 2025-01-26 15:14:15,883 train_utils.py: 271: Train Epoch: [0][1850/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.15e-01 (8.42e-01)\nINFO 2025-01-26 15:14:22,938 train_utils.py: 271: Train Epoch: [0][1860/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 5.80e-01 (8.41e-01)\nINFO 2025-01-26 15:14:29,974 train_utils.py: 271: Train Epoch: [0][1870/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 9.91e-01 (8.41e-01)\nINFO 2025-01-26 15:14:37,088 train_utils.py: 271: Train Epoch: [0][1880/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 2.75e-01 (8.43e-01)\nINFO 2025-01-26 15:14:44,173 train_utils.py: 271: Train Epoch: [0][1890/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 3.48e-01 (8.42e-01)\nINFO 2025-01-26 15:14:51,199 train_utils.py: 271: Train Epoch: [0][1900/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 8.15e-01 (8.42e-01)\nINFO 2025-01-26 15:14:58,189 train_utils.py: 271: Train Epoch: [0][1910/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 2.46e-01 (8.44e-01)\nINFO 2025-01-26 15:15:05,198 train_utils.py: 271: Train Epoch: [0][1920/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 5.08e-01 (8.42e-01)\nINFO 2025-01-26 15:15:12,496 train_utils.py: 271: Train Epoch: [0][1930/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 2.37e-01 (8.40e-01)\nINFO 2025-01-26 15:15:19,509 train_utils.py: 271: Train Epoch: [0][1940/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 8.54e-01 (8.40e-01)\nINFO 2025-01-26 15:15:26,522 train_utils.py: 271: Train Epoch: [0][1950/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 4.07e-01 (8.39e-01)\nINFO 2025-01-26 15:15:33,590 train_utils.py: 271: Train Epoch: [0][1960/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 6.73e-01 (8.38e-01)\nINFO 2025-01-26 15:15:40,653 train_utils.py: 271: Train Epoch: [0][1970/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 5.67e-01 (8.36e-01)\nINFO 2025-01-26 15:15:47,738 train_utils.py: 271: Train Epoch: [0][1980/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.80e-01 (8.35e-01)\nINFO 2025-01-26 15:15:54,763 train_utils.py: 271: Train Epoch: [0][1990/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 4.11e+00 (8.35e-01)\nINFO 2025-01-26 15:16:01,769 train_utils.py: 271: Train Epoch: [0][2000/4156] | Batch Time: 0.70 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.41e-01 (8.32e-01)\nINFO 2025-01-26 15:16:08,764 train_utils.py: 271: Train Epoch: [0][2010/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 5.91e-01 (8.32e-01)\nINFO 2025-01-26 15:16:15,865 train_utils.py: 271: Train Epoch: [0][2020/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.02e+00 (8.32e-01)\nINFO 2025-01-26 15:16:22,909 train_utils.py: 271: Train Epoch: [0][2030/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.20e-01 (8.31e-01)\nINFO 2025-01-26 15:16:29,915 train_utils.py: 271: Train Epoch: [0][2040/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 5.03e-01 (8.31e-01)\nINFO 2025-01-26 15:16:36,867 train_utils.py: 271: Train Epoch: [0][2050/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 7.63e-01 (8.29e-01)\nINFO 2025-01-26 15:16:43,974 train_utils.py: 271: Train Epoch: [0][2060/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 5.34e-01 (8.29e-01)\nINFO 2025-01-26 15:16:51,035 train_utils.py: 271: Train Epoch: [0][2070/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 6.47e-02 (8.28e-01)\nINFO 2025-01-26 15:16:58,113 train_utils.py: 271: Train Epoch: [0][2080/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.37e+00 (8.30e-01)\nINFO 2025-01-26 15:17:05,229 train_utils.py: 271: Train Epoch: [0][2090/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 2.85e-01 (8.30e-01)\nINFO 2025-01-26 15:17:12,351 train_utils.py: 271: Train Epoch: [0][2100/4156] | Batch Time: 0.74 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.01e-01 (8.29e-01)\nINFO 2025-01-26 15:17:19,413 train_utils.py: 271: Train Epoch: [0][2110/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 4.23e-01 (8.26e-01)\nINFO 2025-01-26 15:17:26,552 train_utils.py: 271: Train Epoch: [0][2120/4156] | Batch Time: 0.73 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.69e+00 (8.26e-01)\nINFO 2025-01-26 15:17:33,568 train_utils.py: 271: Train Epoch: [0][2130/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 7.31e-01 (8.25e-01)\nINFO 2025-01-26 15:17:40,477 train_utils.py: 271: Train Epoch: [0][2140/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 2.80e-01 (8.24e-01)\nINFO 2025-01-26 15:17:47,578 train_utils.py: 271: Train Epoch: [0][2150/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 2.94e-01 (8.23e-01)\nINFO 2025-01-26 15:17:54,578 train_utils.py: 271: Train Epoch: [0][2160/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 5.81e-01 (8.23e-01)\nINFO 2025-01-26 15:18:01,601 train_utils.py: 271: Train Epoch: [0][2170/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 1.84e-01 (8.23e-01)\nINFO 2025-01-26 15:18:08,617 train_utils.py: 271: Train Epoch: [0][2180/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 2.11e-01 (8.21e-01)\nINFO 2025-01-26 15:18:15,673 train_utils.py: 271: Train Epoch: [0][2190/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 2.14e+00 (8.21e-01)\nINFO 2025-01-26 15:18:22,753 train_utils.py: 271: Train Epoch: [0][2200/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 1.29e+00 (8.21e-01)\nINFO 2025-01-26 15:18:29,856 train_utils.py: 271: Train Epoch: [0][2210/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 3.76e-01 (8.20e-01)\nINFO 2025-01-26 15:18:36,896 train_utils.py: 271: Train Epoch: [0][2220/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 5.03e-01 (8.21e-01)\nINFO 2025-01-26 15:18:44,001 train_utils.py: 271: Train Epoch: [0][2230/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 2.48e-01 (8.20e-01)\nINFO 2025-01-26 15:18:51,004 train_utils.py: 271: Train Epoch: [0][2240/4156] | Batch Time: 0.73 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 3.23e-01 (8.19e-01)\nINFO 2025-01-26 15:18:57,937 train_utils.py: 271: Train Epoch: [0][2250/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 4.89e-01 (8.17e-01)\nINFO 2025-01-26 15:19:04,972 train_utils.py: 271: Train Epoch: [0][2260/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 1.51e-01 (8.16e-01)\nINFO 2025-01-26 15:19:11,944 train_utils.py: 271: Train Epoch: [0][2270/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 5.97e-01 (8.15e-01)\nINFO 2025-01-26 15:19:19,027 train_utils.py: 271: Train Epoch: [0][2280/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 1.85e-01 (8.14e-01)\nINFO 2025-01-26 15:19:25,993 train_utils.py: 271: Train Epoch: [0][2290/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 1.43e+00 (8.13e-01)\nINFO 2025-01-26 15:19:33,023 train_utils.py: 271: Train Epoch: [0][2300/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 3.40e-01 (8.13e-01)\nINFO 2025-01-26 15:19:40,096 train_utils.py: 271: Train Epoch: [0][2310/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 2.78e-01 (8.12e-01)\nINFO 2025-01-26 15:19:47,160 train_utils.py: 271: Train Epoch: [0][2320/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 1.39e+00 (8.11e-01)\nINFO 2025-01-26 15:19:54,249 train_utils.py: 271: Train Epoch: [0][2330/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 3.24e-01 (8.09e-01)\nINFO 2025-01-26 15:20:01,530 train_utils.py: 271: Train Epoch: [0][2340/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 7.48e-01 (8.08e-01)\nINFO 2025-01-26 15:20:08,508 train_utils.py: 271: Train Epoch: [0][2350/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 1.64e-01 (8.08e-01)\nINFO 2025-01-26 15:20:15,526 train_utils.py: 271: Train Epoch: [0][2360/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 1.91e+00 (8.08e-01)\nINFO 2025-01-26 15:20:22,553 train_utils.py: 271: Train Epoch: [0][2370/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 2.45e-01 (8.06e-01)\nINFO 2025-01-26 15:20:29,585 train_utils.py: 271: Train Epoch: [0][2380/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 4.94e-01 (8.08e-01)\nINFO 2025-01-26 15:20:36,592 train_utils.py: 271: Train Epoch: [0][2390/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 3.14e-01 (8.09e-01)\nINFO 2025-01-26 15:20:43,647 train_utils.py: 271: Train Epoch: [0][2400/4156] | Batch Time: 0.74 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 6.28e-01 (8.09e-01)\nINFO 2025-01-26 15:20:50,784 train_utils.py: 271: Train Epoch: [0][2410/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 2.31e-01 (8.09e-01)\nINFO 2025-01-26 15:20:57,894 train_utils.py: 271: Train Epoch: [0][2420/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 9.30e-02 (8.08e-01)\nINFO 2025-01-26 15:21:05,015 train_utils.py: 271: Train Epoch: [0][2430/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 3.16e+00 (8.10e-01)\nINFO 2025-01-26 15:21:12,032 train_utils.py: 271: Train Epoch: [0][2440/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 3.07e-01 (8.10e-01)\nINFO 2025-01-26 15:21:19,258 train_utils.py: 271: Train Epoch: [0][2450/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 2.84e-01 (8.09e-01)\nINFO 2025-01-26 15:21:26,348 train_utils.py: 271: Train Epoch: [0][2460/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 3.34e-01 (8.07e-01)\nINFO 2025-01-26 15:21:33,427 train_utils.py: 271: Train Epoch: [0][2470/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 8.87e-01 (8.07e-01)\nINFO 2025-01-26 15:21:40,532 train_utils.py: 271: Train Epoch: [0][2480/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 6.61e-01 (8.06e-01)\nINFO 2025-01-26 15:21:47,576 train_utils.py: 271: Train Epoch: [0][2490/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 7.92e-01 (8.05e-01)\nINFO 2025-01-26 15:21:54,821 train_utils.py: 271: Train Epoch: [0][2500/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 5.75e-01 (8.06e-01)\nINFO 2025-01-26 15:22:01,918 train_utils.py: 271: Train Epoch: [0][2510/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 6.47e-01 (8.04e-01)\nINFO 2025-01-26 15:22:09,031 train_utils.py: 271: Train Epoch: [0][2520/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 7.35e-01 (8.04e-01)\nINFO 2025-01-26 15:22:16,075 train_utils.py: 271: Train Epoch: [0][2530/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 1.33e+00 (8.06e-01)\nINFO 2025-01-26 15:22:23,105 train_utils.py: 271: Train Epoch: [0][2540/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 1.13e-01 (8.07e-01)\nINFO 2025-01-26 15:22:30,364 train_utils.py: 271: Train Epoch: [0][2550/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 2.51e-01 (8.06e-01)\nINFO 2025-01-26 15:22:37,417 train_utils.py: 271: Train Epoch: [0][2560/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 3.75e-01 (8.06e-01)\nINFO 2025-01-26 15:22:44,455 train_utils.py: 271: Train Epoch: [0][2570/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 5.32e-01 (8.06e-01)\nINFO 2025-01-26 15:22:51,478 train_utils.py: 271: Train Epoch: [0][2580/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 6.37e-01 (8.07e-01)\nINFO 2025-01-26 15:22:58,538 train_utils.py: 271: Train Epoch: [0][2590/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 8.32e-01 (8.06e-01)\nINFO 2025-01-26 15:23:05,454 train_utils.py: 271: Train Epoch: [0][2600/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 6.59e-01 (8.06e-01)\nINFO 2025-01-26 15:23:12,476 train_utils.py: 271: Train Epoch: [0][2610/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 1.18e+00 (8.05e-01)\nINFO 2025-01-26 15:23:19,513 train_utils.py: 271: Train Epoch: [0][2620/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 1.28e+00 (8.07e-01)\nINFO 2025-01-26 15:23:26,507 train_utils.py: 271: Train Epoch: [0][2630/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 8.34e-01 (8.07e-01)\nINFO 2025-01-26 15:23:33,547 train_utils.py: 271: Train Epoch: [0][2640/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 4.92e-01 (8.07e-01)\nINFO 2025-01-26 15:23:40,647 train_utils.py: 271: Train Epoch: [0][2650/4156] | Batch Time: 0.74 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.54e-01 (8.06e-01)\nINFO 2025-01-26 15:23:47,654 train_utils.py: 271: Train Epoch: [0][2660/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 5.26e-01 (8.07e-01)\nINFO 2025-01-26 15:23:54,749 train_utils.py: 271: Train Epoch: [0][2670/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 3.43e-01 (8.05e-01)\nINFO 2025-01-26 15:24:01,818 train_utils.py: 271: Train Epoch: [0][2680/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.75e-01 (8.05e-01)\nINFO 2025-01-26 15:24:08,802 train_utils.py: 271: Train Epoch: [0][2690/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 3.27e-01 (8.05e-01)\nINFO 2025-01-26 15:24:15,869 train_utils.py: 271: Train Epoch: [0][2700/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 4.76e-01 (8.04e-01)\nINFO 2025-01-26 15:24:22,890 train_utils.py: 271: Train Epoch: [0][2710/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 5.11e-01 (8.07e-01)\nINFO 2025-01-26 15:24:29,979 train_utils.py: 271: Train Epoch: [0][2720/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 1.53e+00 (8.07e-01)\nINFO 2025-01-26 15:24:37,020 train_utils.py: 271: Train Epoch: [0][2730/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.59e-01 (8.07e-01)\nINFO 2025-01-26 15:24:44,058 train_utils.py: 271: Train Epoch: [0][2740/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 1.83e+00 (8.07e-01)\nINFO 2025-01-26 15:24:51,067 train_utils.py: 271: Train Epoch: [0][2750/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 4.40e-01 (8.07e-01)\nINFO 2025-01-26 15:24:58,407 train_utils.py: 271: Train Epoch: [0][2760/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 6.91e-01 (8.05e-01)\nINFO 2025-01-26 15:25:05,426 train_utils.py: 271: Train Epoch: [0][2770/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 1.51e-01 (8.05e-01)\nINFO 2025-01-26 15:25:12,411 train_utils.py: 271: Train Epoch: [0][2780/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 7.31e-01 (8.04e-01)\nINFO 2025-01-26 15:25:19,392 train_utils.py: 271: Train Epoch: [0][2790/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 1.37e-01 (8.03e-01)\nINFO 2025-01-26 15:25:26,512 train_utils.py: 271: Train Epoch: [0][2800/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 5.12e-01 (8.01e-01)\nINFO 2025-01-26 15:25:33,599 train_utils.py: 271: Train Epoch: [0][2810/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 6.97e-01 (8.01e-01)\nINFO 2025-01-26 15:25:40,647 train_utils.py: 271: Train Epoch: [0][2820/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.81e-01 (8.01e-01)\nINFO 2025-01-26 15:25:47,630 train_utils.py: 271: Train Epoch: [0][2830/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 2.26e-01 (8.04e-01)\nINFO 2025-01-26 15:25:54,626 train_utils.py: 271: Train Epoch: [0][2840/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.50e-01 (8.04e-01)\nINFO 2025-01-26 15:26:01,574 train_utils.py: 271: Train Epoch: [0][2850/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 4.48e-01 (8.03e-01)\nINFO 2025-01-26 15:26:08,554 train_utils.py: 271: Train Epoch: [0][2860/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.69e-01 (8.04e-01)\nINFO 2025-01-26 15:26:15,468 train_utils.py: 271: Train Epoch: [0][2870/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.81e-01 (8.04e-01)\nINFO 2025-01-26 15:26:22,476 train_utils.py: 271: Train Epoch: [0][2880/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.07e-01 (8.03e-01)\nINFO 2025-01-26 15:26:29,523 train_utils.py: 271: Train Epoch: [0][2890/4156] | Batch Time: 0.75 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 2.76e-01 (8.02e-01)\nINFO 2025-01-26 15:26:36,527 train_utils.py: 271: Train Epoch: [0][2900/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 9.96e-01 (8.01e-01)\nINFO 2025-01-26 15:26:43,567 train_utils.py: 271: Train Epoch: [0][2910/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 6.39e+00 (8.02e-01)\nINFO 2025-01-26 15:26:50,880 train_utils.py: 271: Train Epoch: [0][2920/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 1.57e-01 (8.04e-01)\nINFO 2025-01-26 15:26:58,004 train_utils.py: 271: Train Epoch: [0][2930/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 4.48e-01 (8.03e-01)\nINFO 2025-01-26 15:27:05,070 train_utils.py: 271: Train Epoch: [0][2940/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 2.41e-01 (8.02e-01)\nINFO 2025-01-26 15:27:12,248 train_utils.py: 271: Train Epoch: [0][2950/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 8.86e-01 (8.01e-01)\nINFO 2025-01-26 15:27:19,283 train_utils.py: 271: Train Epoch: [0][2960/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 2.32e+00 (8.01e-01)\nINFO 2025-01-26 15:27:26,545 train_utils.py: 271: Train Epoch: [0][2970/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 7.37e-01 (8.00e-01)\nINFO 2025-01-26 15:27:33,600 train_utils.py: 271: Train Epoch: [0][2980/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 6.59e-01 (8.02e-01)\nINFO 2025-01-26 15:27:40,656 train_utils.py: 271: Train Epoch: [0][2990/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 5.36e-01 (8.03e-01)\nINFO 2025-01-26 15:27:47,730 train_utils.py: 271: Train Epoch: [0][3000/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 1.41e+00 (8.03e-01)\nINFO 2025-01-26 15:27:54,704 train_utils.py: 271: Train Epoch: [0][3010/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 4.29e-01 (8.02e-01)\nINFO 2025-01-26 15:28:01,756 train_utils.py: 271: Train Epoch: [0][3020/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 4.10e-01 (8.02e-01)\nINFO 2025-01-26 15:28:08,794 train_utils.py: 271: Train Epoch: [0][3030/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 5.54e-01 (8.01e-01)\nINFO 2025-01-26 15:28:15,884 train_utils.py: 271: Train Epoch: [0][3040/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 7.63e-01 (8.00e-01)\nINFO 2025-01-26 15:28:22,885 train_utils.py: 271: Train Epoch: [0][3050/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 4.69e-01 (7.98e-01)\nINFO 2025-01-26 15:28:29,918 train_utils.py: 271: Train Epoch: [0][3060/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 1.30e+00 (7.98e-01)\nINFO 2025-01-26 15:28:37,095 train_utils.py: 271: Train Epoch: [0][3070/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 1.33e-01 (7.98e-01)\nINFO 2025-01-26 15:28:44,017 train_utils.py: 271: Train Epoch: [0][3080/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 1.39e-01 (7.99e-01)\nINFO 2025-01-26 15:28:51,059 train_utils.py: 271: Train Epoch: [0][3090/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 1.15e+00 (7.97e-01)\nINFO 2025-01-26 15:28:58,099 train_utils.py: 271: Train Epoch: [0][3100/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 8.48e-01 (7.96e-01)\nINFO 2025-01-26 15:29:05,146 train_utils.py: 271: Train Epoch: [0][3110/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 8.44e-01 (7.96e-01)\nINFO 2025-01-26 15:29:12,195 train_utils.py: 271: Train Epoch: [0][3120/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 8.28e-01 (7.96e-01)\nINFO 2025-01-26 15:29:19,178 train_utils.py: 271: Train Epoch: [0][3130/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 3.69e-01 (7.95e-01)\nINFO 2025-01-26 15:29:26,222 train_utils.py: 271: Train Epoch: [0][3140/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 6.57e-01 (7.95e-01)\nINFO 2025-01-26 15:29:33,198 train_utils.py: 271: Train Epoch: [0][3150/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 2.02e-01 (7.93e-01)\nINFO 2025-01-26 15:29:40,285 train_utils.py: 271: Train Epoch: [0][3160/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 4.82e-01 (7.93e-01)\nINFO 2025-01-26 15:29:47,414 train_utils.py: 271: Train Epoch: [0][3170/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 2.04e+00 (7.92e-01)\nINFO 2025-01-26 15:29:54,773 train_utils.py: 271: Train Epoch: [0][3180/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 1.85e-01 (7.95e-01)\nINFO 2025-01-26 15:30:01,869 train_utils.py: 271: Train Epoch: [0][3190/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 2.03e+00 (7.95e-01)\nINFO 2025-01-26 15:30:08,985 train_utils.py: 271: Train Epoch: [0][3200/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 6.71e-01 (7.94e-01)\nINFO 2025-01-26 15:30:16,074 train_utils.py: 271: Train Epoch: [0][3210/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 2.13e-01 (7.95e-01)\nINFO 2025-01-26 15:30:23,075 train_utils.py: 271: Train Epoch: [0][3220/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 7.57e-01 (7.94e-01)\nINFO 2025-01-26 15:30:30,091 train_utils.py: 271: Train Epoch: [0][3230/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 1.18e+00 (7.95e-01)\nINFO 2025-01-26 15:30:37,077 train_utils.py: 271: Train Epoch: [0][3240/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 4.49e-01 (7.95e-01)\nINFO 2025-01-26 15:30:44,121 train_utils.py: 271: Train Epoch: [0][3250/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 1.26e+00 (7.94e-01)\nINFO 2025-01-26 15:30:51,137 train_utils.py: 271: Train Epoch: [0][3260/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 8.81e-02 (7.93e-01)\nINFO 2025-01-26 15:30:58,177 train_utils.py: 271: Train Epoch: [0][3270/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 8.47e-01 (7.92e-01)\nINFO 2025-01-26 15:31:05,234 train_utils.py: 271: Train Epoch: [0][3280/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 4.44e-01 (7.92e-01)\nINFO 2025-01-26 15:31:12,266 train_utils.py: 271: Train Epoch: [0][3290/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 4.38e-01 (7.92e-01)\nINFO 2025-01-26 15:31:19,336 train_utils.py: 271: Train Epoch: [0][3300/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 1.73e+00 (7.91e-01)\nINFO 2025-01-26 15:31:26,291 train_utils.py: 271: Train Epoch: [0][3310/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 1.89e-01 (7.91e-01)\nINFO 2025-01-26 15:31:33,347 train_utils.py: 271: Train Epoch: [0][3320/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 1.95e-01 (7.92e-01)\nINFO 2025-01-26 15:31:40,345 train_utils.py: 271: Train Epoch: [0][3330/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 3.57e-01 (7.91e-01)\nINFO 2025-01-26 15:31:47,606 train_utils.py: 271: Train Epoch: [0][3340/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 5.22e-01 (7.92e-01)\nINFO 2025-01-26 15:31:54,707 train_utils.py: 271: Train Epoch: [0][3350/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 7.98e-01 (7.92e-01)\nINFO 2025-01-26 15:32:01,817 train_utils.py: 271: Train Epoch: [0][3360/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 4.27e-01 (7.91e-01)\nINFO 2025-01-26 15:32:08,903 train_utils.py: 271: Train Epoch: [0][3370/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 1.46e+00 (7.91e-01)\nINFO 2025-01-26 15:32:16,155 train_utils.py: 271: Train Epoch: [0][3380/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 9.78e-01 (7.92e-01)\nINFO 2025-01-26 15:32:23,199 train_utils.py: 271: Train Epoch: [0][3390/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 6.28e-01 (7.91e-01)\nINFO 2025-01-26 15:32:30,275 train_utils.py: 271: Train Epoch: [0][3400/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 1.56e+00 (7.90e-01)\nINFO 2025-01-26 15:32:37,371 train_utils.py: 271: Train Epoch: [0][3410/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 4.28e-01 (7.90e-01)\nINFO 2025-01-26 15:32:44,534 train_utils.py: 271: Train Epoch: [0][3420/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.86e-01 (7.89e-01)\nINFO 2025-01-26 15:32:51,629 train_utils.py: 271: Train Epoch: [0][3430/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 1.70e-01 (7.89e-01)\nINFO 2025-01-26 15:32:58,703 train_utils.py: 271: Train Epoch: [0][3440/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 3.21e-01 (7.89e-01)\nINFO 2025-01-26 15:33:05,767 train_utils.py: 271: Train Epoch: [0][3450/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 1.85e-01 (7.91e-01)\nINFO 2025-01-26 15:33:12,864 train_utils.py: 271: Train Epoch: [0][3460/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.65e-01 (7.91e-01)\nINFO 2025-01-26 15:33:19,894 train_utils.py: 271: Train Epoch: [0][3470/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 2.72e-01 (7.92e-01)\nINFO 2025-01-26 15:33:26,916 train_utils.py: 271: Train Epoch: [0][3480/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.97e-01 (7.91e-01)\nINFO 2025-01-26 15:33:33,942 train_utils.py: 271: Train Epoch: [0][3490/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.75e-01 (7.91e-01)\nINFO 2025-01-26 15:33:40,962 train_utils.py: 271: Train Epoch: [0][3500/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 1.07e+00 (7.91e-01)\nINFO 2025-01-26 15:33:48,010 train_utils.py: 271: Train Epoch: [0][3510/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 8.82e-01 (7.91e-01)\nINFO 2025-01-26 15:33:55,176 train_utils.py: 271: Train Epoch: [0][3520/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 8.86e-01 (7.91e-01)\nINFO 2025-01-26 15:34:02,274 train_utils.py: 271: Train Epoch: [0][3530/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 2.07e+00 (7.91e-01)\nINFO 2025-01-26 15:34:09,236 train_utils.py: 271: Train Epoch: [0][3540/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 4.72e-01 (7.91e-01)\nINFO 2025-01-26 15:34:16,277 train_utils.py: 271: Train Epoch: [0][3550/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 4.39e-01 (7.91e-01)\nINFO 2025-01-26 15:34:23,425 train_utils.py: 271: Train Epoch: [0][3560/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 4.67e-01 (7.90e-01)\nINFO 2025-01-26 15:34:30,557 train_utils.py: 271: Train Epoch: [0][3570/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 1.48e-01 (7.89e-01)\nINFO 2025-01-26 15:34:37,538 train_utils.py: 271: Train Epoch: [0][3580/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 1.38e+00 (7.88e-01)\nINFO 2025-01-26 15:34:44,810 train_utils.py: 271: Train Epoch: [0][3590/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.40e+00 (7.88e-01)\nINFO 2025-01-26 15:34:51,901 train_utils.py: 271: Train Epoch: [0][3600/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 6.20e-01 (7.89e-01)\nINFO 2025-01-26 15:34:58,864 train_utils.py: 271: Train Epoch: [0][3610/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.77e-01 (7.88e-01)\nINFO 2025-01-26 15:35:05,855 train_utils.py: 271: Train Epoch: [0][3620/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 5.13e-01 (7.88e-01)\nINFO 2025-01-26 15:35:12,904 train_utils.py: 271: Train Epoch: [0][3630/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.17e-01 (7.87e-01)\nINFO 2025-01-26 15:35:19,918 train_utils.py: 271: Train Epoch: [0][3640/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 6.64e-01 (7.87e-01)\nINFO 2025-01-26 15:35:26,909 train_utils.py: 271: Train Epoch: [0][3650/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 3.82e-01 (7.88e-01)\nINFO 2025-01-26 15:35:33,893 train_utils.py: 271: Train Epoch: [0][3660/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 6.57e-01 (7.86e-01)\nINFO 2025-01-26 15:35:40,977 train_utils.py: 271: Train Epoch: [0][3670/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.65e-01 (7.87e-01)\nINFO 2025-01-26 15:35:48,024 train_utils.py: 271: Train Epoch: [0][3680/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 1.36e-01 (7.86e-01)\nINFO 2025-01-26 15:35:55,217 train_utils.py: 271: Train Epoch: [0][3690/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.39e-01 (7.86e-01)\nINFO 2025-01-26 15:36:02,270 train_utils.py: 271: Train Epoch: [0][3700/4156] | Batch Time: 0.75 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 1.17e+00 (7.86e-01)\nINFO 2025-01-26 15:36:09,312 train_utils.py: 271: Train Epoch: [0][3710/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.32e-01 (7.86e-01)\nINFO 2025-01-26 15:36:16,380 train_utils.py: 271: Train Epoch: [0][3720/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 4.34e-01 (7.86e-01)\nINFO 2025-01-26 15:36:23,399 train_utils.py: 271: Train Epoch: [0][3730/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.03e-01 (7.85e-01)\nINFO 2025-01-26 15:36:30,442 train_utils.py: 271: Train Epoch: [0][3740/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.90e-01 (7.85e-01)\nINFO 2025-01-26 15:36:37,657 train_utils.py: 271: Train Epoch: [0][3750/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 5.39e-01 (7.85e-01)\nINFO 2025-01-26 15:36:44,676 train_utils.py: 271: Train Epoch: [0][3760/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 2.79e-01 (7.84e-01)\nINFO 2025-01-26 15:36:51,704 train_utils.py: 271: Train Epoch: [0][3770/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 1.63e-01 (7.85e-01)\nINFO 2025-01-26 15:36:58,703 train_utils.py: 271: Train Epoch: [0][3780/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 1.02e+00 (7.84e-01)\nINFO 2025-01-26 15:37:05,754 train_utils.py: 271: Train Epoch: [0][3790/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 5.88e-01 (7.85e-01)\nINFO 2025-01-26 15:37:13,007 train_utils.py: 271: Train Epoch: [0][3800/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 4.99e-01 (7.85e-01)\nINFO 2025-01-26 15:37:20,103 train_utils.py: 271: Train Epoch: [0][3810/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 3.82e-01 (7.84e-01)\nINFO 2025-01-26 15:37:27,123 train_utils.py: 271: Train Epoch: [0][3820/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 2.80e-01 (7.84e-01)\nINFO 2025-01-26 15:37:34,087 train_utils.py: 271: Train Epoch: [0][3830/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 7.37e-01 (7.84e-01)\nINFO 2025-01-26 15:37:41,149 train_utils.py: 271: Train Epoch: [0][3840/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 2.68e-01 (7.84e-01)\nINFO 2025-01-26 15:37:48,181 train_utils.py: 271: Train Epoch: [0][3850/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 1.30e+00 (7.84e-01)\nINFO 2025-01-26 15:37:55,328 train_utils.py: 271: Train Epoch: [0][3860/4156] | Batch Time: 0.75 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 3.76e-01 (7.84e-01)\nINFO 2025-01-26 15:38:02,308 train_utils.py: 271: Train Epoch: [0][3870/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 2.57e-01 (7.84e-01)\nINFO 2025-01-26 15:38:09,364 train_utils.py: 271: Train Epoch: [0][3880/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 5.86e-01 (7.83e-01)\nINFO 2025-01-26 15:38:16,394 train_utils.py: 271: Train Epoch: [0][3890/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 7.78e-01 (7.83e-01)\nINFO 2025-01-26 15:38:23,436 train_utils.py: 271: Train Epoch: [0][3900/4156] | Batch Time: 0.75 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 3.09e-01 (7.82e-01)\nINFO 2025-01-26 15:38:30,500 train_utils.py: 271: Train Epoch: [0][3910/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 3.40e-01 (7.82e-01)\nINFO 2025-01-26 15:38:37,665 train_utils.py: 271: Train Epoch: [0][3920/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 6.33e-01 (7.83e-01)\nINFO 2025-01-26 15:38:44,668 train_utils.py: 271: Train Epoch: [0][3930/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 8.37e-01 (7.83e-01)\nINFO 2025-01-26 15:38:51,702 train_utils.py: 271: Train Epoch: [0][3940/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.55e-01 (7.83e-01)\nINFO 2025-01-26 15:38:58,676 train_utils.py: 271: Train Epoch: [0][3950/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 1.91e-01 (7.82e-01)\nINFO 2025-01-26 15:39:05,648 train_utils.py: 271: Train Epoch: [0][3960/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.88e-01 (7.83e-01)\nINFO 2025-01-26 15:39:12,659 train_utils.py: 271: Train Epoch: [0][3970/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.60e-01 (7.82e-01)\nINFO 2025-01-26 15:39:19,665 train_utils.py: 271: Train Epoch: [0][3980/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 3.47e-01 (7.81e-01)\nINFO 2025-01-26 15:39:26,725 train_utils.py: 271: Train Epoch: [0][3990/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 3.13e-01 (7.81e-01)\nINFO 2025-01-26 15:39:33,843 train_utils.py: 271: Train Epoch: [0][4000/4156] | Batch Time: 0.77 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 1.11e-01 (7.82e-01)\nINFO 2025-01-26 15:39:41,131 train_utils.py: 271: Train Epoch: [0][4010/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 2.83e-01 (7.81e-01)\nINFO 2025-01-26 15:39:48,102 train_utils.py: 271: Train Epoch: [0][4020/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 2.32e-01 (7.80e-01)\nINFO 2025-01-26 15:39:55,191 train_utils.py: 271: Train Epoch: [0][4030/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 4.53e-01 (7.80e-01)\nINFO 2025-01-26 15:40:02,211 train_utils.py: 271: Train Epoch: [0][4040/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 1.49e+00 (7.79e-01)\nINFO 2025-01-26 15:40:09,156 train_utils.py: 271: Train Epoch: [0][4050/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 1.13e+00 (7.78e-01)\nINFO 2025-01-26 15:40:16,062 train_utils.py: 271: Train Epoch: [0][4060/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 1.31e-01 (7.78e-01)\nINFO 2025-01-26 15:40:23,018 train_utils.py: 271: Train Epoch: [0][4070/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 1.09e+00 (7.77e-01)\nINFO 2025-01-26 15:40:29,887 train_utils.py: 271: Train Epoch: [0][4080/4156] | Batch Time: 0.67 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 5.44e-01 (7.77e-01)\nINFO 2025-01-26 15:40:36,743 train_utils.py: 271: Train Epoch: [0][4090/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 2.05e-01 (7.77e-01)\nINFO 2025-01-26 15:40:43,693 train_utils.py: 271: Train Epoch: [0][4100/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 5.68e-01 (7.76e-01)\nINFO 2025-01-26 15:40:50,469 train_utils.py: 271: Train Epoch: [0][4110/4156] | Batch Time: 0.67 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 1.78e-01 (7.77e-01)\nINFO 2025-01-26 15:40:57,429 train_utils.py: 271: Train Epoch: [0][4120/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 3.49e-01 (7.76e-01)\nINFO 2025-01-26 15:41:04,336 train_utils.py: 271: Train Epoch: [0][4130/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 5.87e-01 (7.76e-01)\nINFO 2025-01-26 15:41:11,226 train_utils.py: 271: Train Epoch: [0][4140/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 4.47e-01 (7.76e-01)\nINFO 2025-01-26 15:41:18,038 train_utils.py: 271: Train Epoch: [0][4150/4156] | Batch Time: 0.67 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 1.04e+00 (7.75e-01)\nINFO 2025-01-26 15:41:28,329 trainer.py: 950: Estimated time remaining: 00d 03h 26m\nINFO 2025-01-26 15:41:28,338 trainer.py: 892: Synchronizing meters\nINFO 2025-01-26 15:41:28,339 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.7751096974102792, 'Losses/train_all_loss_mask': 0.011568029662910448, 'Losses/train_all_loss_dice': 0.39861571684675123, 'Losses/train_all_loss_iou': 0.1451333675380479, 'Losses/train_all_loss_class': 2.4522014071647834e-08, 'Losses/train_all_core_loss': 0.7751096974102792, 'Trainer/where': 0.19995187680461984, 'Trainer/epoch': 0, 'Trainer/steps_train': 4156}\nINFO 2025-01-26 15:44:09,737 train_utils.py: 271: Train Epoch: [1][   0/4156] | Batch Time: 158.12 (158.12) | Data Time: 154.92 (154.92) | Mem (GB): 10.00 (10.00/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 9.07e-01 (9.07e-01)\nINFO 2025-01-26 15:44:16,750 train_utils.py: 271: Train Epoch: [1][  10/4156] | Batch Time: 0.69 (15.01) | Data Time: 0.00 (14.08) | Mem (GB): 10.00 (10.18/11.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 7.14e-01 (6.04e-01)\nINFO 2025-01-26 15:44:23,862 train_utils.py: 271: Train Epoch: [1][  20/4156] | Batch Time: 0.71 (8.20) | Data Time: 0.00 (7.38) | Mem (GB): 10.00 (10.10/11.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 3.43e-01 (6.00e-01)\nINFO 2025-01-26 15:44:30,946 train_utils.py: 271: Train Epoch: [1][  30/4156] | Batch Time: 0.69 (5.78) | Data Time: 0.00 (5.00) | Mem (GB): 10.00 (10.06/11.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 8.92e-01 (6.52e-01)\nINFO 2025-01-26 15:44:38,032 train_utils.py: 271: Train Epoch: [1][  40/4156] | Batch Time: 0.69 (4.55) | Data Time: 0.00 (3.78) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 4.62e-01 (6.35e-01)\nINFO 2025-01-26 15:44:45,147 train_utils.py: 271: Train Epoch: [1][  50/4156] | Batch Time: 0.70 (3.79) | Data Time: 0.00 (3.04) | Mem (GB): 10.00 (10.06/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.89e-01 (6.70e-01)\nINFO 2025-01-26 15:44:52,267 train_utils.py: 271: Train Epoch: [1][  60/4156] | Batch Time: 0.71 (3.29) | Data Time: 0.00 (2.54) | Mem (GB): 10.00 (10.07/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.94e-01 (6.72e-01)\nINFO 2025-01-26 15:44:59,285 train_utils.py: 271: Train Epoch: [1][  70/4156] | Batch Time: 0.69 (2.92) | Data Time: 0.00 (2.18) | Mem (GB): 11.00 (10.07/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 7.50e-01 (6.50e-01)\nINFO 2025-01-26 15:45:06,379 train_utils.py: 271: Train Epoch: [1][  80/4156] | Batch Time: 0.69 (2.65) | Data Time: 0.00 (1.91) | Mem (GB): 10.00 (10.06/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.04e-01 (6.69e-01)\nINFO 2025-01-26 15:45:13,475 train_utils.py: 271: Train Epoch: [1][  90/4156] | Batch Time: 0.72 (2.44) | Data Time: 0.00 (1.70) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 4.17e-01 (6.43e-01)\nINFO 2025-01-26 15:45:20,552 train_utils.py: 271: Train Epoch: [1][ 100/4156] | Batch Time: 0.73 (2.27) | Data Time: 0.00 (1.53) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 6.54e-01 (6.46e-01)\nINFO 2025-01-26 15:45:27,668 train_utils.py: 271: Train Epoch: [1][ 110/4156] | Batch Time: 0.70 (2.13) | Data Time: 0.00 (1.40) | Mem (GB): 10.00 (10.05/11.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 5.89e-01 (6.48e-01)\nINFO 2025-01-26 15:45:34,798 train_utils.py: 271: Train Epoch: [1][ 120/4156] | Batch Time: 0.72 (2.01) | Data Time: 0.00 (1.28) | Mem (GB): 10.00 (10.07/12.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 4.63e-01 (6.60e-01)\nINFO 2025-01-26 15:45:41,845 train_utils.py: 271: Train Epoch: [1][ 130/4156] | Batch Time: 0.69 (1.91) | Data Time: 0.00 (1.18) | Mem (GB): 10.00 (10.09/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.03e+00 (6.70e-01)\nINFO 2025-01-26 15:45:48,896 train_utils.py: 271: Train Epoch: [1][ 140/4156] | Batch Time: 0.72 (1.82) | Data Time: 0.00 (1.10) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.19e+00 (6.77e-01)\nINFO 2025-01-26 15:45:56,024 train_utils.py: 271: Train Epoch: [1][ 150/4156] | Batch Time: 0.72 (1.75) | Data Time: 0.00 (1.03) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 4.52e-01 (6.62e-01)\nINFO 2025-01-26 15:46:03,024 train_utils.py: 271: Train Epoch: [1][ 160/4156] | Batch Time: 0.70 (1.69) | Data Time: 0.00 (0.96) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 3.56e-01 (6.52e-01)\nINFO 2025-01-26 15:46:10,120 train_utils.py: 271: Train Epoch: [1][ 170/4156] | Batch Time: 0.69 (1.63) | Data Time: 0.00 (0.91) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 2.10e-01 (6.35e-01)\nINFO 2025-01-26 15:46:17,262 train_utils.py: 271: Train Epoch: [1][ 180/4156] | Batch Time: 0.69 (1.58) | Data Time: 0.00 (0.86) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 2.88e-01 (6.39e-01)\nINFO 2025-01-26 15:46:24,392 train_utils.py: 271: Train Epoch: [1][ 190/4156] | Batch Time: 0.70 (1.53) | Data Time: 0.00 (0.81) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 8.41e-01 (6.54e-01)\nINFO 2025-01-26 15:46:31,428 train_utils.py: 271: Train Epoch: [1][ 200/4156] | Batch Time: 0.74 (1.49) | Data Time: 0.00 (0.77) | Mem (GB): 11.00 (10.12/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.31e+01 (7.17e-01)\nINFO 2025-01-26 15:46:38,402 train_utils.py: 271: Train Epoch: [1][ 210/4156] | Batch Time: 0.69 (1.45) | Data Time: 0.00 (0.74) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 5.32e-01 (7.29e-01)\nINFO 2025-01-26 15:46:45,479 train_utils.py: 271: Train Epoch: [1][ 220/4156] | Batch Time: 0.68 (1.42) | Data Time: 0.00 (0.70) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 3.17e-01 (7.23e-01)\nINFO 2025-01-26 15:46:52,547 train_utils.py: 271: Train Epoch: [1][ 230/4156] | Batch Time: 0.69 (1.39) | Data Time: 0.00 (0.67) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 1.80e+00 (7.65e-01)\nINFO 2025-01-26 15:46:59,680 train_utils.py: 271: Train Epoch: [1][ 240/4156] | Batch Time: 0.72 (1.36) | Data Time: 0.00 (0.64) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 5.38e-01 (7.70e-01)\nINFO 2025-01-26 15:47:06,743 train_utils.py: 271: Train Epoch: [1][ 250/4156] | Batch Time: 0.72 (1.34) | Data Time: 0.00 (0.62) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 7.32e-01 (7.73e-01)\nINFO 2025-01-26 15:47:13,776 train_utils.py: 271: Train Epoch: [1][ 260/4156] | Batch Time: 0.71 (1.31) | Data Time: 0.00 (0.59) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 7.11e-01 (7.60e-01)\nINFO 2025-01-26 15:47:20,727 train_utils.py: 271: Train Epoch: [1][ 270/4156] | Batch Time: 0.68 (1.29) | Data Time: 0.00 (0.57) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 5.17e-01 (7.68e-01)\nINFO 2025-01-26 15:47:27,810 train_utils.py: 271: Train Epoch: [1][ 280/4156] | Batch Time: 0.70 (1.27) | Data Time: 0.00 (0.55) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 2.56e-01 (7.69e-01)\nINFO 2025-01-26 15:47:34,826 train_utils.py: 271: Train Epoch: [1][ 290/4156] | Batch Time: 0.69 (1.25) | Data Time: 0.00 (0.53) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 2.58e-01 (7.61e-01)\nINFO 2025-01-26 15:47:41,956 train_utils.py: 271: Train Epoch: [1][ 300/4156] | Batch Time: 0.74 (1.23) | Data Time: 0.00 (0.52) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 5.29e-01 (7.54e-01)\nINFO 2025-01-26 15:47:49,139 train_utils.py: 271: Train Epoch: [1][ 310/4156] | Batch Time: 0.74 (1.21) | Data Time: 0.00 (0.50) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 1.25e+00 (7.57e-01)\nINFO 2025-01-26 15:47:56,106 train_utils.py: 271: Train Epoch: [1][ 320/4156] | Batch Time: 0.69 (1.20) | Data Time: 0.00 (0.48) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 4.94e-01 (7.50e-01)\nINFO 2025-01-26 15:48:03,211 train_utils.py: 271: Train Epoch: [1][ 330/4156] | Batch Time: 0.71 (1.18) | Data Time: 0.00 (0.47) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 7.42e-01 (7.78e-01)\nINFO 2025-01-26 15:48:10,331 train_utils.py: 271: Train Epoch: [1][ 340/4156] | Batch Time: 0.69 (1.17) | Data Time: 0.00 (0.46) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 5.01e-01 (7.68e-01)\nINFO 2025-01-26 15:48:17,331 train_utils.py: 271: Train Epoch: [1][ 350/4156] | Batch Time: 0.71 (1.16) | Data Time: 0.00 (0.44) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 6.47e-01 (7.67e-01)\nINFO 2025-01-26 15:48:24,335 train_utils.py: 271: Train Epoch: [1][ 360/4156] | Batch Time: 0.69 (1.14) | Data Time: 0.00 (0.43) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 2.96e-01 (7.66e-01)\nINFO 2025-01-26 15:48:31,287 train_utils.py: 271: Train Epoch: [1][ 370/4156] | Batch Time: 0.68 (1.13) | Data Time: 0.00 (0.42) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 9.98e-01 (7.64e-01)\nINFO 2025-01-26 15:48:38,287 train_utils.py: 271: Train Epoch: [1][ 380/4156] | Batch Time: 0.68 (1.12) | Data Time: 0.00 (0.41) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 1.35e+00 (7.67e-01)\nINFO 2025-01-26 15:48:45,414 train_utils.py: 271: Train Epoch: [1][ 390/4156] | Batch Time: 0.72 (1.11) | Data Time: 0.00 (0.40) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 8.76e-02 (7.60e-01)\nINFO 2025-01-26 15:48:52,714 train_utils.py: 271: Train Epoch: [1][ 400/4156] | Batch Time: 0.73 (1.10) | Data Time: 0.00 (0.39) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.59e-01 (7.61e-01)\nINFO 2025-01-26 15:48:59,697 train_utils.py: 271: Train Epoch: [1][ 410/4156] | Batch Time: 0.72 (1.09) | Data Time: 0.00 (0.38) | Mem (GB): 11.00 (10.12/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 1.20e+00 (7.65e-01)\nINFO 2025-01-26 15:49:06,776 train_utils.py: 271: Train Epoch: [1][ 420/4156] | Batch Time: 0.72 (1.08) | Data Time: 0.00 (0.37) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 1.78e-01 (7.64e-01)\nINFO 2025-01-26 15:49:13,739 train_utils.py: 271: Train Epoch: [1][ 430/4156] | Batch Time: 0.72 (1.07) | Data Time: 0.00 (0.36) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.28e-01 (7.64e-01)\nINFO 2025-01-26 15:49:20,757 train_utils.py: 271: Train Epoch: [1][ 440/4156] | Batch Time: 0.73 (1.06) | Data Time: 0.00 (0.35) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 3.51e-01 (7.66e-01)\nINFO 2025-01-26 15:49:27,821 train_utils.py: 271: Train Epoch: [1][ 450/4156] | Batch Time: 0.72 (1.06) | Data Time: 0.00 (0.34) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 1.71e-01 (7.60e-01)\nINFO 2025-01-26 15:49:34,871 train_utils.py: 271: Train Epoch: [1][ 460/4156] | Batch Time: 0.69 (1.05) | Data Time: 0.00 (0.34) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.02e-01 (7.52e-01)\nINFO 2025-01-26 15:49:41,916 train_utils.py: 271: Train Epoch: [1][ 470/4156] | Batch Time: 0.69 (1.04) | Data Time: 0.00 (0.33) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 8.74e-01 (7.52e-01)\nINFO 2025-01-26 15:49:48,937 train_utils.py: 271: Train Epoch: [1][ 480/4156] | Batch Time: 0.71 (1.03) | Data Time: 0.00 (0.32) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 2.49e-01 (7.61e-01)\nINFO 2025-01-26 15:49:55,986 train_utils.py: 271: Train Epoch: [1][ 490/4156] | Batch Time: 0.69 (1.03) | Data Time: 0.00 (0.32) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 2.77e-01 (7.52e-01)\nINFO 2025-01-26 15:50:03,003 train_utils.py: 271: Train Epoch: [1][ 500/4156] | Batch Time: 0.74 (1.02) | Data Time: 0.00 (0.31) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 2.48e-01 (7.48e-01)\nINFO 2025-01-26 15:50:09,957 train_utils.py: 271: Train Epoch: [1][ 510/4156] | Batch Time: 0.69 (1.01) | Data Time: 0.00 (0.30) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 2.46e-01 (7.47e-01)\nINFO 2025-01-26 15:50:16,984 train_utils.py: 271: Train Epoch: [1][ 520/4156] | Batch Time: 0.68 (1.01) | Data Time: 0.00 (0.30) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 1.21e+00 (7.47e-01)\nINFO 2025-01-26 15:50:24,044 train_utils.py: 271: Train Epoch: [1][ 530/4156] | Batch Time: 0.71 (1.00) | Data Time: 0.00 (0.29) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 1.87e-01 (7.58e-01)\nINFO 2025-01-26 15:50:31,025 train_utils.py: 271: Train Epoch: [1][ 540/4156] | Batch Time: 0.71 (1.00) | Data Time: 0.00 (0.29) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 5.38e-01 (7.54e-01)\nINFO 2025-01-26 15:50:38,058 train_utils.py: 271: Train Epoch: [1][ 550/4156] | Batch Time: 0.72 (0.99) | Data Time: 0.00 (0.28) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 8.53e-01 (7.51e-01)\nINFO 2025-01-26 15:50:45,114 train_utils.py: 271: Train Epoch: [1][ 560/4156] | Batch Time: 0.68 (0.99) | Data Time: 0.00 (0.28) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 7.66e-01 (7.55e-01)\nINFO 2025-01-26 15:50:52,218 train_utils.py: 271: Train Epoch: [1][ 570/4156] | Batch Time: 0.71 (0.98) | Data Time: 0.00 (0.27) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.51e-01 (7.55e-01)\nINFO 2025-01-26 15:50:59,337 train_utils.py: 271: Train Epoch: [1][ 580/4156] | Batch Time: 0.73 (0.98) | Data Time: 0.00 (0.27) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.56e-01 (7.51e-01)\nINFO 2025-01-26 15:51:06,452 train_utils.py: 271: Train Epoch: [1][ 590/4156] | Batch Time: 0.75 (0.97) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.34e-01 (7.47e-01)\nINFO 2025-01-26 15:51:13,484 train_utils.py: 271: Train Epoch: [1][ 600/4156] | Batch Time: 0.73 (0.97) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 3.25e-01 (7.47e-01)\nINFO 2025-01-26 15:51:20,496 train_utils.py: 271: Train Epoch: [1][ 610/4156] | Batch Time: 0.71 (0.96) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 4.68e-01 (7.53e-01)\nINFO 2025-01-26 15:51:27,498 train_utils.py: 271: Train Epoch: [1][ 620/4156] | Batch Time: 0.71 (0.96) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.18e+00 (7.52e-01)\nINFO 2025-01-26 15:51:34,507 train_utils.py: 271: Train Epoch: [1][ 630/4156] | Batch Time: 0.69 (0.96) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 7.89e-01 (7.48e-01)\nINFO 2025-01-26 15:51:41,477 train_utils.py: 271: Train Epoch: [1][ 640/4156] | Batch Time: 0.72 (0.95) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 5.64e-01 (7.43e-01)\nINFO 2025-01-26 15:51:48,483 train_utils.py: 271: Train Epoch: [1][ 650/4156] | Batch Time: 0.68 (0.95) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 5.10e-01 (7.44e-01)\nINFO 2025-01-26 15:51:55,501 train_utils.py: 271: Train Epoch: [1][ 660/4156] | Batch Time: 0.68 (0.94) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 1.18e+00 (7.50e-01)\nINFO 2025-01-26 15:52:02,655 train_utils.py: 271: Train Epoch: [1][ 670/4156] | Batch Time: 0.75 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.32e-01 (7.52e-01)\nINFO 2025-01-26 15:52:10,064 train_utils.py: 271: Train Epoch: [1][ 680/4156] | Batch Time: 0.72 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 4.34e-01 (7.48e-01)\nINFO 2025-01-26 15:52:17,133 train_utils.py: 271: Train Epoch: [1][ 690/4156] | Batch Time: 0.72 (0.93) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 2.55e-01 (7.51e-01)\nINFO 2025-01-26 15:52:24,231 train_utils.py: 271: Train Epoch: [1][ 700/4156] | Batch Time: 0.73 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 4.34e-01 (7.49e-01)\nINFO 2025-01-26 15:52:31,331 train_utils.py: 271: Train Epoch: [1][ 710/4156] | Batch Time: 0.68 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 2.04e+00 (7.44e-01)\nINFO 2025-01-26 15:52:38,417 train_utils.py: 271: Train Epoch: [1][ 720/4156] | Batch Time: 0.72 (0.92) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.71e-01 (7.45e-01)\nINFO 2025-01-26 15:52:45,463 train_utils.py: 271: Train Epoch: [1][ 730/4156] | Batch Time: 0.69 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 12.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 5.28e+00 (7.56e-01)\nINFO 2025-01-26 15:52:52,556 train_utils.py: 271: Train Epoch: [1][ 740/4156] | Batch Time: 0.72 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 5.39e-01 (7.53e-01)\nINFO 2025-01-26 15:52:59,650 train_utils.py: 271: Train Epoch: [1][ 750/4156] | Batch Time: 0.72 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 2.25e-01 (7.52e-01)\nINFO 2025-01-26 15:53:06,672 train_utils.py: 271: Train Epoch: [1][ 760/4156] | Batch Time: 0.71 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 3.17e-01 (7.50e-01)\nINFO 2025-01-26 15:53:13,823 train_utils.py: 271: Train Epoch: [1][ 770/4156] | Batch Time: 0.68 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 3.27e-01 (7.48e-01)\nINFO 2025-01-26 15:53:20,912 train_utils.py: 271: Train Epoch: [1][ 780/4156] | Batch Time: 0.71 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 6.92e-01 (7.47e-01)\nINFO 2025-01-26 15:53:27,993 train_utils.py: 271: Train Epoch: [1][ 790/4156] | Batch Time: 0.73 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 4.83e-01 (7.42e-01)\nINFO 2025-01-26 15:53:34,972 train_utils.py: 271: Train Epoch: [1][ 800/4156] | Batch Time: 0.70 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 4.13e-01 (7.39e-01)\nINFO 2025-01-26 15:53:42,134 train_utils.py: 271: Train Epoch: [1][ 810/4156] | Batch Time: 0.71 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 4.79e-01 (7.37e-01)\nINFO 2025-01-26 15:53:49,490 train_utils.py: 271: Train Epoch: [1][ 820/4156] | Batch Time: 0.70 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 2.20e-01 (7.38e-01)\nINFO 2025-01-26 15:53:56,548 train_utils.py: 271: Train Epoch: [1][ 830/4156] | Batch Time: 0.72 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 3.04e-01 (7.42e-01)\nINFO 2025-01-26 15:54:03,536 train_utils.py: 271: Train Epoch: [1][ 840/4156] | Batch Time: 0.68 (0.89) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 7.28e-01 (7.40e-01)\nINFO 2025-01-26 15:54:10,533 train_utils.py: 271: Train Epoch: [1][ 850/4156] | Batch Time: 0.72 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 1.65e+00 (7.37e-01)\nINFO 2025-01-26 15:54:17,565 train_utils.py: 271: Train Epoch: [1][ 860/4156] | Batch Time: 0.72 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 1.23e-01 (7.36e-01)\nINFO 2025-01-26 15:54:24,605 train_utils.py: 271: Train Epoch: [1][ 870/4156] | Batch Time: 0.68 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 1.24e+00 (7.36e-01)\nINFO 2025-01-26 15:54:31,661 train_utils.py: 271: Train Epoch: [1][ 880/4156] | Batch Time: 0.71 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 2.21e-01 (7.31e-01)\nINFO 2025-01-26 15:54:38,692 train_utils.py: 271: Train Epoch: [1][ 890/4156] | Batch Time: 0.68 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 4.54e-01 (7.28e-01)\nINFO 2025-01-26 15:54:45,712 train_utils.py: 271: Train Epoch: [1][ 900/4156] | Batch Time: 0.74 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 6.18e-01 (7.26e-01)\nINFO 2025-01-26 15:54:52,672 train_utils.py: 271: Train Epoch: [1][ 910/4156] | Batch Time: 0.69 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 2.12e-01 (7.24e-01)\nINFO 2025-01-26 15:54:59,739 train_utils.py: 271: Train Epoch: [1][ 920/4156] | Batch Time: 0.71 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 3.83e-01 (7.24e-01)\nINFO 2025-01-26 15:55:06,749 train_utils.py: 271: Train Epoch: [1][ 930/4156] | Batch Time: 0.69 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 5.16e-01 (7.22e-01)\nINFO 2025-01-26 15:55:13,748 train_utils.py: 271: Train Epoch: [1][ 940/4156] | Batch Time: 0.72 (0.87) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 1.73e-01 (7.22e-01)\nINFO 2025-01-26 15:55:20,716 train_utils.py: 271: Train Epoch: [1][ 950/4156] | Batch Time: 0.68 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 4.39e-01 (7.22e-01)\nINFO 2025-01-26 15:55:27,610 train_utils.py: 271: Train Epoch: [1][ 960/4156] | Batch Time: 0.68 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 9.10e-01 (7.22e-01)\nINFO 2025-01-26 15:55:34,625 train_utils.py: 271: Train Epoch: [1][ 970/4156] | Batch Time: 0.68 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 3.72e-01 (7.23e-01)\nINFO 2025-01-26 15:55:41,801 train_utils.py: 271: Train Epoch: [1][ 980/4156] | Batch Time: 0.71 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 3.27e-01 (7.22e-01)\nINFO 2025-01-26 15:55:48,894 train_utils.py: 271: Train Epoch: [1][ 990/4156] | Batch Time: 0.70 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 4.89e-01 (7.21e-01)\nINFO 2025-01-26 15:55:56,026 train_utils.py: 271: Train Epoch: [1][1000/4156] | Batch Time: 0.74 (0.86) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 8.30e-01 (7.19e-01)\nINFO 2025-01-26 15:56:03,121 train_utils.py: 271: Train Epoch: [1][1010/4156] | Batch Time: 0.72 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 1.05e+00 (7.18e-01)\nINFO 2025-01-26 15:56:10,190 train_utils.py: 271: Train Epoch: [1][1020/4156] | Batch Time: 0.71 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 4.30e-01 (7.20e-01)\nINFO 2025-01-26 15:56:17,237 train_utils.py: 271: Train Epoch: [1][1030/4156] | Batch Time: 0.72 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 11.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 1.26e+00 (7.20e-01)\nINFO 2025-01-26 15:56:24,277 train_utils.py: 271: Train Epoch: [1][1040/4156] | Batch Time: 0.69 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 4.22e-01 (7.17e-01)\nINFO 2025-01-26 15:56:31,448 train_utils.py: 271: Train Epoch: [1][1050/4156] | Batch Time: 0.72 (0.86) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 2.10e-01 (7.17e-01)\nINFO 2025-01-26 15:56:38,437 train_utils.py: 271: Train Epoch: [1][1060/4156] | Batch Time: 0.69 (0.85) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 4.12e-01 (7.18e-01)\nINFO 2025-01-26 15:56:45,485 train_utils.py: 271: Train Epoch: [1][1070/4156] | Batch Time: 0.68 (0.85) | Data Time: 0.00 (0.15) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 5.33e-01 (7.16e-01)\nINFO 2025-01-26 15:56:52,546 train_utils.py: 271: Train Epoch: [1][1080/4156] | Batch Time: 0.70 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 1.51e-01 (7.12e-01)\nINFO 2025-01-26 15:56:59,553 train_utils.py: 271: Train Epoch: [1][1090/4156] | Batch Time: 0.69 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 1.15e-01 (7.10e-01)\nINFO 2025-01-26 15:57:06,687 train_utils.py: 271: Train Epoch: [1][1100/4156] | Batch Time: 0.73 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 4.73e-01 (7.09e-01)\nINFO 2025-01-26 15:57:13,718 train_utils.py: 271: Train Epoch: [1][1110/4156] | Batch Time: 0.68 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 7.24e-01 (7.14e-01)\nINFO 2025-01-26 15:57:20,895 train_utils.py: 271: Train Epoch: [1][1120/4156] | Batch Time: 0.72 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 2.40e-01 (7.12e-01)\nINFO 2025-01-26 15:57:28,042 train_utils.py: 271: Train Epoch: [1][1130/4156] | Batch Time: 0.71 (0.85) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 3.66e-01 (7.11e-01)\nINFO 2025-01-26 15:57:35,102 train_utils.py: 271: Train Epoch: [1][1140/4156] | Batch Time: 0.73 (0.84) | Data Time: 0.00 (0.14) | Mem (GB): 12.00 (10.12/12.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 3.32e+00 (7.13e-01)\nINFO 2025-01-26 15:57:42,164 train_utils.py: 271: Train Epoch: [1][1150/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.14) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 8.11e-01 (7.13e-01)\nINFO 2025-01-26 15:57:49,221 train_utils.py: 271: Train Epoch: [1][1160/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 4.37e-01 (7.14e-01)\nINFO 2025-01-26 15:57:56,159 train_utils.py: 271: Train Epoch: [1][1170/4156] | Batch Time: 0.68 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 3.91e-01 (7.12e-01)\nINFO 2025-01-26 15:58:03,261 train_utils.py: 271: Train Epoch: [1][1180/4156] | Batch Time: 0.70 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 7.49e-01 (7.15e-01)\nINFO 2025-01-26 15:58:10,343 train_utils.py: 271: Train Epoch: [1][1190/4156] | Batch Time: 0.72 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 1.58e-01 (7.12e-01)\nINFO 2025-01-26 15:58:17,408 train_utils.py: 271: Train Epoch: [1][1200/4156] | Batch Time: 0.73 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 6.25e-01 (7.09e-01)\nINFO 2025-01-26 15:58:24,381 train_utils.py: 271: Train Epoch: [1][1210/4156] | Batch Time: 0.68 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 7.35e-01 (7.08e-01)\nINFO 2025-01-26 15:58:31,353 train_utils.py: 271: Train Epoch: [1][1220/4156] | Batch Time: 0.69 (0.84) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 8.81e-01 (7.08e-01)\nINFO 2025-01-26 15:58:38,686 train_utils.py: 271: Train Epoch: [1][1230/4156] | Batch Time: 0.69 (0.83) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 4.12e-01 (7.05e-01)\nINFO 2025-01-26 15:58:45,688 train_utils.py: 271: Train Epoch: [1][1240/4156] | Batch Time: 0.72 (0.83) | Data Time: 0.00 (0.13) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 9.86e-01 (7.04e-01)\nINFO 2025-01-26 15:58:52,760 train_utils.py: 271: Train Epoch: [1][1250/4156] | Batch Time: 0.73 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 1.18e-01 (7.02e-01)\nINFO 2025-01-26 15:58:59,801 train_utils.py: 271: Train Epoch: [1][1260/4156] | Batch Time: 0.72 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 1.63e-01 (7.00e-01)\nINFO 2025-01-26 15:59:06,830 train_utils.py: 271: Train Epoch: [1][1270/4156] | Batch Time: 0.69 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 4.97e-01 (7.00e-01)\nINFO 2025-01-26 15:59:13,811 train_utils.py: 271: Train Epoch: [1][1280/4156] | Batch Time: 0.68 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 2.93e-01 (6.99e-01)\nINFO 2025-01-26 15:59:20,892 train_utils.py: 271: Train Epoch: [1][1290/4156] | Batch Time: 0.72 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.36e-01 (6.97e-01)\nINFO 2025-01-26 15:59:28,039 train_utils.py: 271: Train Epoch: [1][1300/4156] | Batch Time: 0.74 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.38e-01 (6.99e-01)\nINFO 2025-01-26 15:59:34,993 train_utils.py: 271: Train Epoch: [1][1310/4156] | Batch Time: 0.73 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.43e-01 (6.97e-01)\nINFO 2025-01-26 15:59:42,086 train_utils.py: 271: Train Epoch: [1][1320/4156] | Batch Time: 0.68 (0.83) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.25e-01 (6.95e-01)\nINFO 2025-01-26 15:59:49,075 train_utils.py: 271: Train Epoch: [1][1330/4156] | Batch Time: 0.68 (0.82) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 1.70e+00 (6.95e-01)\nINFO 2025-01-26 15:59:56,051 train_utils.py: 271: Train Epoch: [1][1340/4156] | Batch Time: 0.68 (0.82) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 2.29e-01 (6.94e-01)\nINFO 2025-01-26 16:00:03,093 train_utils.py: 271: Train Epoch: [1][1350/4156] | Batch Time: 0.69 (0.82) | Data Time: 0.00 (0.12) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.72e-01 (6.92e-01)\nINFO 2025-01-26 16:00:10,083 train_utils.py: 271: Train Epoch: [1][1360/4156] | Batch Time: 0.69 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 5.56e-01 (6.91e-01)\nINFO 2025-01-26 16:00:17,066 train_utils.py: 271: Train Epoch: [1][1370/4156] | Batch Time: 0.70 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.61e-01 (6.89e-01)\nINFO 2025-01-26 16:00:24,344 train_utils.py: 271: Train Epoch: [1][1380/4156] | Batch Time: 0.96 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 1.45e+00 (6.92e-01)\nINFO 2025-01-26 16:00:31,304 train_utils.py: 271: Train Epoch: [1][1390/4156] | Batch Time: 0.69 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 1.21e+00 (6.94e-01)\nINFO 2025-01-26 16:00:38,389 train_utils.py: 271: Train Epoch: [1][1400/4156] | Batch Time: 0.73 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 8.78e-01 (6.93e-01)\nINFO 2025-01-26 16:00:45,471 train_utils.py: 271: Train Epoch: [1][1410/4156] | Batch Time: 0.73 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 9.79e-01 (6.93e-01)\nINFO 2025-01-26 16:00:52,618 train_utils.py: 271: Train Epoch: [1][1420/4156] | Batch Time: 0.72 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 7.52e-01 (6.92e-01)\nINFO 2025-01-26 16:00:59,682 train_utils.py: 271: Train Epoch: [1][1430/4156] | Batch Time: 0.72 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 2.20e-01 (6.91e-01)\nINFO 2025-01-26 16:01:06,780 train_utils.py: 271: Train Epoch: [1][1440/4156] | Batch Time: 0.68 (0.82) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 1.62e-01 (6.89e-01)\nINFO 2025-01-26 16:01:13,891 train_utils.py: 271: Train Epoch: [1][1450/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 5.62e-01 (6.88e-01)\nINFO 2025-01-26 16:01:20,878 train_utils.py: 271: Train Epoch: [1][1460/4156] | Batch Time: 0.68 (0.81) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 2.79e-01 (6.88e-01)\nINFO 2025-01-26 16:01:27,986 train_utils.py: 271: Train Epoch: [1][1470/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 7.94e-01 (6.89e-01)\nINFO 2025-01-26 16:01:35,076 train_utils.py: 271: Train Epoch: [1][1480/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.11) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 2.52e-01 (6.88e-01)\nINFO 2025-01-26 16:01:42,100 train_utils.py: 271: Train Epoch: [1][1490/4156] | Batch Time: 0.69 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 2.38e-01 (6.87e-01)\nINFO 2025-01-26 16:01:49,078 train_utils.py: 271: Train Epoch: [1][1500/4156] | Batch Time: 0.71 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 1.49e-01 (6.87e-01)\nINFO 2025-01-26 16:01:56,108 train_utils.py: 271: Train Epoch: [1][1510/4156] | Batch Time: 0.68 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 2.14e-01 (6.90e-01)\nINFO 2025-01-26 16:02:03,086 train_utils.py: 271: Train Epoch: [1][1520/4156] | Batch Time: 0.69 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 2.58e-01 (6.88e-01)\nINFO 2025-01-26 16:02:10,118 train_utils.py: 271: Train Epoch: [1][1530/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 3.70e-01 (6.87e-01)\nINFO 2025-01-26 16:02:17,135 train_utils.py: 271: Train Epoch: [1][1540/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 2.63e-01 (6.87e-01)\nINFO 2025-01-26 16:02:24,144 train_utils.py: 271: Train Epoch: [1][1550/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 1.20e+00 (6.86e-01)\nINFO 2025-01-26 16:02:31,174 train_utils.py: 271: Train Epoch: [1][1560/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 4.56e-01 (6.88e-01)\nINFO 2025-01-26 16:02:38,204 train_utils.py: 271: Train Epoch: [1][1570/4156] | Batch Time: 0.68 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 4.36e-01 (6.87e-01)\nINFO 2025-01-26 16:02:45,385 train_utils.py: 271: Train Epoch: [1][1580/4156] | Batch Time: 0.75 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 1.49e+00 (6.88e-01)\nINFO 2025-01-26 16:02:52,527 train_utils.py: 271: Train Epoch: [1][1590/4156] | Batch Time: 0.72 (0.81) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 1.93e-01 (6.86e-01)\nINFO 2025-01-26 16:02:59,529 train_utils.py: 271: Train Epoch: [1][1600/4156] | Batch Time: 0.70 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 1.16e+00 (6.88e-01)\nINFO 2025-01-26 16:03:06,608 train_utils.py: 271: Train Epoch: [1][1610/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 2.30e-01 (6.89e-01)\nINFO 2025-01-26 16:03:13,542 train_utils.py: 271: Train Epoch: [1][1620/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 3.15e-01 (6.88e-01)\nINFO 2025-01-26 16:03:20,630 train_utils.py: 271: Train Epoch: [1][1630/4156] | Batch Time: 0.71 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 2.66e+00 (6.91e-01)\nINFO 2025-01-26 16:03:27,682 train_utils.py: 271: Train Epoch: [1][1640/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.10) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 2.85e-01 (6.89e-01)\nINFO 2025-01-26 16:03:34,929 train_utils.py: 271: Train Epoch: [1][1650/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 1.15e+00 (6.92e-01)\nINFO 2025-01-26 16:03:41,965 train_utils.py: 271: Train Epoch: [1][1660/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 9.35e-01 (6.92e-01)\nINFO 2025-01-26 16:03:48,935 train_utils.py: 271: Train Epoch: [1][1670/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 2.63e-01 (6.91e-01)\nINFO 2025-01-26 16:03:55,938 train_utils.py: 271: Train Epoch: [1][1680/4156] | Batch Time: 0.75 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 6.94e-01 (6.91e-01)\nINFO 2025-01-26 16:04:03,018 train_utils.py: 271: Train Epoch: [1][1690/4156] | Batch Time: 0.71 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 3.20e-01 (6.90e-01)\nINFO 2025-01-26 16:04:10,097 train_utils.py: 271: Train Epoch: [1][1700/4156] | Batch Time: 0.69 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 5.20e-01 (6.91e-01)\nINFO 2025-01-26 16:04:17,114 train_utils.py: 271: Train Epoch: [1][1710/4156] | Batch Time: 0.70 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 1.23e-01 (6.91e-01)\nINFO 2025-01-26 16:04:24,103 train_utils.py: 271: Train Epoch: [1][1720/4156] | Batch Time: 0.71 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 1.45e+00 (6.92e-01)\nINFO 2025-01-26 16:04:31,035 train_utils.py: 271: Train Epoch: [1][1730/4156] | Batch Time: 0.68 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 4.77e-01 (6.91e-01)\nINFO 2025-01-26 16:04:38,088 train_utils.py: 271: Train Epoch: [1][1740/4156] | Batch Time: 0.69 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 7.95e-01 (6.90e-01)\nINFO 2025-01-26 16:04:45,098 train_utils.py: 271: Train Epoch: [1][1750/4156] | Batch Time: 0.70 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 2.98e+00 (7.06e-01)\nINFO 2025-01-26 16:04:52,212 train_utils.py: 271: Train Epoch: [1][1760/4156] | Batch Time: 0.72 (0.80) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 2.80e-01 (7.05e-01)\nINFO 2025-01-26 16:04:59,215 train_utils.py: 271: Train Epoch: [1][1770/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 5.14e-01 (7.08e-01)\nINFO 2025-01-26 16:05:06,244 train_utils.py: 271: Train Epoch: [1][1780/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 2.04e-01 (7.08e-01)\nINFO 2025-01-26 16:05:13,295 train_utils.py: 271: Train Epoch: [1][1790/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 2.46e+00 (7.07e-01)\nINFO 2025-01-26 16:05:20,330 train_utils.py: 271: Train Epoch: [1][1800/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 1.28e-01 (7.11e-01)\nINFO 2025-01-26 16:05:27,307 train_utils.py: 271: Train Epoch: [1][1810/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 5.06e-01 (7.11e-01)\nINFO 2025-01-26 16:05:34,296 train_utils.py: 271: Train Epoch: [1][1820/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 6.00e-01 (7.10e-01)\nINFO 2025-01-26 16:05:41,264 train_utils.py: 271: Train Epoch: [1][1830/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.09) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 2.65e-01 (7.11e-01)\nINFO 2025-01-26 16:05:48,304 train_utils.py: 271: Train Epoch: [1][1840/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 8.49e-01 (7.10e-01)\nINFO 2025-01-26 16:05:55,387 train_utils.py: 271: Train Epoch: [1][1850/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 2.45e-01 (7.10e-01)\nINFO 2025-01-26 16:06:02,472 train_utils.py: 271: Train Epoch: [1][1860/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 1.57e+00 (7.11e-01)\nINFO 2025-01-26 16:06:09,497 train_utils.py: 271: Train Epoch: [1][1870/4156] | Batch Time: 0.72 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 3.55e-01 (7.11e-01)\nINFO 2025-01-26 16:06:16,514 train_utils.py: 271: Train Epoch: [1][1880/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 3.91e-01 (7.09e-01)\nINFO 2025-01-26 16:06:23,535 train_utils.py: 271: Train Epoch: [1][1890/4156] | Batch Time: 0.70 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 2.00e-01 (7.11e-01)\nINFO 2025-01-26 16:06:30,479 train_utils.py: 271: Train Epoch: [1][1900/4156] | Batch Time: 0.70 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 3.96e-01 (7.10e-01)\nINFO 2025-01-26 16:06:37,476 train_utils.py: 271: Train Epoch: [1][1910/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 7.01e-01 (7.11e-01)\nINFO 2025-01-26 16:06:44,400 train_utils.py: 271: Train Epoch: [1][1920/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 9.28e-01 (7.09e-01)\nINFO 2025-01-26 16:06:51,436 train_utils.py: 271: Train Epoch: [1][1930/4156] | Batch Time: 0.69 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 5.40e-01 (7.08e-01)\nINFO 2025-01-26 16:06:58,437 train_utils.py: 271: Train Epoch: [1][1940/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 2.73e-01 (7.06e-01)\nINFO 2025-01-26 16:07:05,472 train_utils.py: 271: Train Epoch: [1][1950/4156] | Batch Time: 0.71 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 3.66e-01 (7.08e-01)\nINFO 2025-01-26 16:07:12,529 train_utils.py: 271: Train Epoch: [1][1960/4156] | Batch Time: 0.68 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 2.75e+00 (7.10e-01)\nINFO 2025-01-26 16:07:19,603 train_utils.py: 271: Train Epoch: [1][1970/4156] | Batch Time: 0.70 (0.79) | Data Time: 0.00 (0.08) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 8.57e-01 (7.09e-01)\nINFO 2025-01-26 16:07:26,684 train_utils.py: 271: Train Epoch: [1][1980/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 1.66e-01 (7.08e-01)\nINFO 2025-01-26 16:07:33,738 train_utils.py: 271: Train Epoch: [1][1990/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 5.38e-01 (7.08e-01)\nINFO 2025-01-26 16:07:40,976 train_utils.py: 271: Train Epoch: [1][2000/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 3.18e-01 (7.07e-01)\nINFO 2025-01-26 16:07:48,008 train_utils.py: 271: Train Epoch: [1][2010/4156] | Batch Time: 0.75 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 2.22e+00 (7.07e-01)\nINFO 2025-01-26 16:07:55,034 train_utils.py: 271: Train Epoch: [1][2020/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 1.02e+00 (7.06e-01)\nINFO 2025-01-26 16:08:02,131 train_utils.py: 271: Train Epoch: [1][2030/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 8.72e-01 (7.05e-01)\nINFO 2025-01-26 16:08:09,167 train_utils.py: 271: Train Epoch: [1][2040/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 9.86e-01 (7.04e-01)\nINFO 2025-01-26 16:08:16,168 train_utils.py: 271: Train Epoch: [1][2050/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 6.39e-01 (7.04e-01)\nINFO 2025-01-26 16:08:23,462 train_utils.py: 271: Train Epoch: [1][2060/4156] | Batch Time: 0.97 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 4.07e-01 (7.03e-01)\nINFO 2025-01-26 16:08:30,481 train_utils.py: 271: Train Epoch: [1][2070/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 1.31e-01 (7.03e-01)\nINFO 2025-01-26 16:08:37,516 train_utils.py: 271: Train Epoch: [1][2080/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.08) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 5.53e-01 (7.03e-01)\nINFO 2025-01-26 16:08:44,892 train_utils.py: 271: Train Epoch: [1][2090/4156] | Batch Time: 0.73 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 5.77e-01 (7.02e-01)\nINFO 2025-01-26 16:08:52,011 train_utils.py: 271: Train Epoch: [1][2100/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 8.48e-01 (7.02e-01)\nINFO 2025-01-26 16:08:59,139 train_utils.py: 271: Train Epoch: [1][2110/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 2.85e-01 (7.02e-01)\nINFO 2025-01-26 16:09:06,219 train_utils.py: 271: Train Epoch: [1][2120/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 5.82e-01 (7.03e-01)\nINFO 2025-01-26 16:09:13,339 train_utils.py: 271: Train Epoch: [1][2130/4156] | Batch Time: 0.70 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 2.53e-01 (7.02e-01)\nINFO 2025-01-26 16:09:20,455 train_utils.py: 271: Train Epoch: [1][2140/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 3.03e-01 (7.02e-01)\nINFO 2025-01-26 16:09:27,455 train_utils.py: 271: Train Epoch: [1][2150/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 5.51e-01 (7.03e-01)\nINFO 2025-01-26 16:09:34,521 train_utils.py: 271: Train Epoch: [1][2160/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 1.53e+00 (7.02e-01)\nINFO 2025-01-26 16:09:41,546 train_utils.py: 271: Train Epoch: [1][2170/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 1.90e-01 (7.02e-01)\nINFO 2025-01-26 16:09:48,627 train_utils.py: 271: Train Epoch: [1][2180/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 5.13e-01 (7.06e-01)\nINFO 2025-01-26 16:09:55,672 train_utils.py: 271: Train Epoch: [1][2190/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 6.59e-01 (7.07e-01)\nINFO 2025-01-26 16:10:02,738 train_utils.py: 271: Train Epoch: [1][2200/4156] | Batch Time: 0.74 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 7.68e-01 (7.05e-01)\nINFO 2025-01-26 16:10:09,794 train_utils.py: 271: Train Epoch: [1][2210/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 7.95e-01 (7.04e-01)\nINFO 2025-01-26 16:10:16,843 train_utils.py: 271: Train Epoch: [1][2220/4156] | Batch Time: 0.72 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 9.71e-02 (7.04e-01)\nINFO 2025-01-26 16:10:23,970 train_utils.py: 271: Train Epoch: [1][2230/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 1.57e-01 (7.03e-01)\nINFO 2025-01-26 16:10:30,956 train_utils.py: 271: Train Epoch: [1][2240/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 8.61e-01 (7.07e-01)\nINFO 2025-01-26 16:10:37,973 train_utils.py: 271: Train Epoch: [1][2250/4156] | Batch Time: 0.71 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 4.30e-01 (7.06e-01)\nINFO 2025-01-26 16:10:44,967 train_utils.py: 271: Train Epoch: [1][2260/4156] | Batch Time: 0.68 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.70e-01 (7.07e-01)\nINFO 2025-01-26 16:10:52,043 train_utils.py: 271: Train Epoch: [1][2270/4156] | Batch Time: 0.69 (0.78) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.12e-01 (7.06e-01)\nINFO 2025-01-26 16:10:59,091 train_utils.py: 271: Train Epoch: [1][2280/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.17e+00 (7.06e-01)\nINFO 2025-01-26 16:11:06,222 train_utils.py: 271: Train Epoch: [1][2290/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 5.83e-01 (7.06e-01)\nINFO 2025-01-26 16:11:13,137 train_utils.py: 271: Train Epoch: [1][2300/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.71e-01 (7.05e-01)\nINFO 2025-01-26 16:11:20,149 train_utils.py: 271: Train Epoch: [1][2310/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 3.31e-01 (7.04e-01)\nINFO 2025-01-26 16:11:27,163 train_utils.py: 271: Train Epoch: [1][2320/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 1.89e-01 (7.04e-01)\nINFO 2025-01-26 16:11:34,186 train_utils.py: 271: Train Epoch: [1][2330/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.49e+00 (7.03e-01)\nINFO 2025-01-26 16:11:41,200 train_utils.py: 271: Train Epoch: [1][2340/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 5.97e-01 (7.03e-01)\nINFO 2025-01-26 16:11:48,303 train_utils.py: 271: Train Epoch: [1][2350/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 2.32e-01 (7.03e-01)\nINFO 2025-01-26 16:11:55,426 train_utils.py: 271: Train Epoch: [1][2360/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 1.55e-01 (7.03e-01)\nINFO 2025-01-26 16:12:02,576 train_utils.py: 271: Train Epoch: [1][2370/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 4.47e-01 (7.03e-01)\nINFO 2025-01-26 16:12:09,667 train_utils.py: 271: Train Epoch: [1][2380/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 5.97e-01 (7.02e-01)\nINFO 2025-01-26 16:12:16,718 train_utils.py: 271: Train Epoch: [1][2390/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 3.50e-01 (7.03e-01)\nINFO 2025-01-26 16:12:23,802 train_utils.py: 271: Train Epoch: [1][2400/4156] | Batch Time: 0.74 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 2.32e-01 (7.02e-01)\nINFO 2025-01-26 16:12:30,864 train_utils.py: 271: Train Epoch: [1][2410/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.07) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 4.31e-01 (7.02e-01)\nINFO 2025-01-26 16:12:37,859 train_utils.py: 271: Train Epoch: [1][2420/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 3.65e-01 (7.01e-01)\nINFO 2025-01-26 16:12:44,996 train_utils.py: 271: Train Epoch: [1][2430/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 2.66e-01 (7.03e-01)\nINFO 2025-01-26 16:12:52,129 train_utils.py: 271: Train Epoch: [1][2440/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 5.35e-01 (7.03e-01)\nINFO 2025-01-26 16:12:59,189 train_utils.py: 271: Train Epoch: [1][2450/4156] | Batch Time: 0.68 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 6.01e-01 (7.04e-01)\nINFO 2025-01-26 16:13:06,236 train_utils.py: 271: Train Epoch: [1][2460/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 1.70e-01 (7.04e-01)\nINFO 2025-01-26 16:13:13,335 train_utils.py: 271: Train Epoch: [1][2470/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 3.48e-01 (7.02e-01)\nINFO 2025-01-26 16:13:20,680 train_utils.py: 271: Train Epoch: [1][2480/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 1.68e-01 (7.02e-01)\nINFO 2025-01-26 16:13:27,701 train_utils.py: 271: Train Epoch: [1][2490/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 4.79e-01 (7.03e-01)\nINFO 2025-01-26 16:13:34,819 train_utils.py: 271: Train Epoch: [1][2500/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 9.37e-01 (7.02e-01)\nINFO 2025-01-26 16:13:41,865 train_utils.py: 271: Train Epoch: [1][2510/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 5.47e-01 (7.02e-01)\nINFO 2025-01-26 16:13:48,878 train_utils.py: 271: Train Epoch: [1][2520/4156] | Batch Time: 0.73 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 2.35e-01 (7.01e-01)\nINFO 2025-01-26 16:13:55,977 train_utils.py: 271: Train Epoch: [1][2530/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 9.71e-01 (7.01e-01)\nINFO 2025-01-26 16:14:03,050 train_utils.py: 271: Train Epoch: [1][2540/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 4.25e-01 (7.01e-01)\nINFO 2025-01-26 16:14:10,122 train_utils.py: 271: Train Epoch: [1][2550/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 3.93e-01 (7.01e-01)\nINFO 2025-01-26 16:14:17,234 train_utils.py: 271: Train Epoch: [1][2560/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 1.29e+00 (7.01e-01)\nINFO 2025-01-26 16:14:24,276 train_utils.py: 271: Train Epoch: [1][2570/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 7.69e-01 (7.01e-01)\nINFO 2025-01-26 16:14:31,240 train_utils.py: 271: Train Epoch: [1][2580/4156] | Batch Time: 0.69 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 1.89e-01 (7.03e-01)\nINFO 2025-01-26 16:14:38,277 train_utils.py: 271: Train Epoch: [1][2590/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 2.62e-01 (7.03e-01)\nINFO 2025-01-26 16:14:45,363 train_utils.py: 271: Train Epoch: [1][2600/4156] | Batch Time: 0.74 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 5.22e-01 (7.03e-01)\nINFO 2025-01-26 16:14:52,468 train_utils.py: 271: Train Epoch: [1][2610/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 3.65e-01 (7.02e-01)\nINFO 2025-01-26 16:14:59,511 train_utils.py: 271: Train Epoch: [1][2620/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 7.20e-01 (7.02e-01)\nINFO 2025-01-26 16:15:06,680 train_utils.py: 271: Train Epoch: [1][2630/4156] | Batch Time: 0.71 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 2.17e+00 (7.02e-01)\nINFO 2025-01-26 16:15:13,761 train_utils.py: 271: Train Epoch: [1][2640/4156] | Batch Time: 0.70 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 12.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 1.96e+00 (7.02e-01)\nINFO 2025-01-26 16:15:20,808 train_utils.py: 271: Train Epoch: [1][2650/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 1.97e-01 (7.03e-01)\nINFO 2025-01-26 16:15:27,822 train_utils.py: 271: Train Epoch: [1][2660/4156] | Batch Time: 0.72 (0.77) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 3.03e-01 (7.03e-01)\nINFO 2025-01-26 16:15:34,910 train_utils.py: 271: Train Epoch: [1][2670/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 5.62e-01 (7.04e-01)\nINFO 2025-01-26 16:15:41,959 train_utils.py: 271: Train Epoch: [1][2680/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 7.78e-01 (7.05e-01)\nINFO 2025-01-26 16:15:49,022 train_utils.py: 271: Train Epoch: [1][2690/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 1.56e+00 (7.05e-01)\nINFO 2025-01-26 16:15:56,083 train_utils.py: 271: Train Epoch: [1][2700/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 1.49e-01 (7.04e-01)\nINFO 2025-01-26 16:16:03,221 train_utils.py: 271: Train Epoch: [1][2710/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 3.62e-01 (7.04e-01)\nINFO 2025-01-26 16:16:10,343 train_utils.py: 271: Train Epoch: [1][2720/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 3.06e-01 (7.03e-01)\nINFO 2025-01-26 16:16:17,484 train_utils.py: 271: Train Epoch: [1][2730/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 2.07e-01 (7.02e-01)\nINFO 2025-01-26 16:16:24,494 train_utils.py: 271: Train Epoch: [1][2740/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 5.82e-01 (7.02e-01)\nINFO 2025-01-26 16:16:31,483 train_utils.py: 271: Train Epoch: [1][2750/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 8.76e-01 (7.01e-01)\nINFO 2025-01-26 16:16:38,558 train_utils.py: 271: Train Epoch: [1][2760/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 4.06e-01 (7.02e-01)\nINFO 2025-01-26 16:16:45,532 train_utils.py: 271: Train Epoch: [1][2770/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 3.57e-01 (7.01e-01)\nINFO 2025-01-26 16:16:52,529 train_utils.py: 271: Train Epoch: [1][2780/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 5.72e-01 (7.02e-01)\nINFO 2025-01-26 16:16:59,520 train_utils.py: 271: Train Epoch: [1][2790/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 5.14e-01 (7.02e-01)\nINFO 2025-01-26 16:17:06,500 train_utils.py: 271: Train Epoch: [1][2800/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 4.99e-01 (7.02e-01)\nINFO 2025-01-26 16:17:13,601 train_utils.py: 271: Train Epoch: [1][2810/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 4.79e-01 (7.02e-01)\nINFO 2025-01-26 16:17:20,606 train_utils.py: 271: Train Epoch: [1][2820/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 7.98e-01 (7.03e-01)\nINFO 2025-01-26 16:17:27,635 train_utils.py: 271: Train Epoch: [1][2830/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 8.86e-01 (7.03e-01)\nINFO 2025-01-26 16:17:34,942 train_utils.py: 271: Train Epoch: [1][2840/4156] | Batch Time: 0.99 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 1.11e+00 (7.02e-01)\nINFO 2025-01-26 16:17:42,043 train_utils.py: 271: Train Epoch: [1][2850/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.06) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 4.34e-01 (7.03e-01)\nINFO 2025-01-26 16:17:49,088 train_utils.py: 271: Train Epoch: [1][2860/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 1.94e-01 (7.06e-01)\nINFO 2025-01-26 16:17:56,136 train_utils.py: 271: Train Epoch: [1][2870/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 3.90e-01 (7.05e-01)\nINFO 2025-01-26 16:18:03,235 train_utils.py: 271: Train Epoch: [1][2880/4156] | Batch Time: 0.70 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 3.58e-01 (7.04e-01)\nINFO 2025-01-26 16:18:10,377 train_utils.py: 271: Train Epoch: [1][2890/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 1.10e-01 (7.03e-01)\nINFO 2025-01-26 16:18:17,741 train_utils.py: 271: Train Epoch: [1][2900/4156] | Batch Time: 0.74 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 6.06e-01 (7.02e-01)\nINFO 2025-01-26 16:18:24,824 train_utils.py: 271: Train Epoch: [1][2910/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 4.79e-01 (7.01e-01)\nINFO 2025-01-26 16:18:31,774 train_utils.py: 271: Train Epoch: [1][2920/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 3.03e+00 (7.03e-01)\nINFO 2025-01-26 16:18:38,752 train_utils.py: 271: Train Epoch: [1][2930/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 3.12e-01 (7.01e-01)\nINFO 2025-01-26 16:18:45,747 train_utils.py: 271: Train Epoch: [1][2940/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 1.85e-01 (7.01e-01)\nINFO 2025-01-26 16:18:52,778 train_utils.py: 271: Train Epoch: [1][2950/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 3.00e-01 (7.00e-01)\nINFO 2025-01-26 16:18:59,740 train_utils.py: 271: Train Epoch: [1][2960/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 1.47e+00 (7.01e-01)\nINFO 2025-01-26 16:19:06,722 train_utils.py: 271: Train Epoch: [1][2970/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 4.40e-01 (7.03e-01)\nINFO 2025-01-26 16:19:13,707 train_utils.py: 271: Train Epoch: [1][2980/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 7.02e-01 (7.02e-01)\nINFO 2025-01-26 16:19:20,795 train_utils.py: 271: Train Epoch: [1][2990/4156] | Batch Time: 0.74 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 3.84e-01 (7.01e-01)\nINFO 2025-01-26 16:19:27,885 train_utils.py: 271: Train Epoch: [1][3000/4156] | Batch Time: 0.74 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 1.98e-01 (7.00e-01)\nINFO 2025-01-26 16:19:34,879 train_utils.py: 271: Train Epoch: [1][3010/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 5.95e+00 (7.02e-01)\nINFO 2025-01-26 16:19:41,908 train_utils.py: 271: Train Epoch: [1][3020/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 1.07e-01 (7.06e-01)\nINFO 2025-01-26 16:19:48,946 train_utils.py: 271: Train Epoch: [1][3030/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.11/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 2.06e+00 (7.06e-01)\nINFO 2025-01-26 16:19:55,871 train_utils.py: 271: Train Epoch: [1][3040/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 1.48e+00 (7.06e-01)\nINFO 2025-01-26 16:20:02,894 train_utils.py: 271: Train Epoch: [1][3050/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 3.60e-01 (7.05e-01)\nINFO 2025-01-26 16:20:09,875 train_utils.py: 271: Train Epoch: [1][3060/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 2.66e-01 (7.05e-01)\nINFO 2025-01-26 16:20:16,881 train_utils.py: 271: Train Epoch: [1][3070/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 2.70e-01 (7.05e-01)\nINFO 2025-01-26 16:20:23,855 train_utils.py: 271: Train Epoch: [1][3080/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 3.77e-01 (7.06e-01)\nINFO 2025-01-26 16:20:30,901 train_utils.py: 271: Train Epoch: [1][3090/4156] | Batch Time: 0.69 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 6.57e-01 (7.06e-01)\nINFO 2025-01-26 16:20:37,976 train_utils.py: 271: Train Epoch: [1][3100/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 1.01e+00 (7.06e-01)\nINFO 2025-01-26 16:20:45,058 train_utils.py: 271: Train Epoch: [1][3110/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 2.00e+00 (7.06e-01)\nINFO 2025-01-26 16:20:52,133 train_utils.py: 271: Train Epoch: [1][3120/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 1.84e-01 (7.05e-01)\nINFO 2025-01-26 16:20:59,154 train_utils.py: 271: Train Epoch: [1][3130/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 3.31e-01 (7.05e-01)\nINFO 2025-01-26 16:21:06,202 train_utils.py: 271: Train Epoch: [1][3140/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 6.20e-01 (7.05e-01)\nINFO 2025-01-26 16:21:13,185 train_utils.py: 271: Train Epoch: [1][3150/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 8.80e-01 (7.05e-01)\nINFO 2025-01-26 16:21:20,216 train_utils.py: 271: Train Epoch: [1][3160/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 1.54e-01 (7.04e-01)\nINFO 2025-01-26 16:21:27,247 train_utils.py: 271: Train Epoch: [1][3170/4156] | Batch Time: 0.72 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 2.69e-01 (7.04e-01)\nINFO 2025-01-26 16:21:34,291 train_utils.py: 271: Train Epoch: [1][3180/4156] | Batch Time: 0.68 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 2.84e-01 (7.05e-01)\nINFO 2025-01-26 16:21:41,365 train_utils.py: 271: Train Epoch: [1][3190/4156] | Batch Time: 0.71 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 3.17e-01 (7.04e-01)\nINFO 2025-01-26 16:21:48,448 train_utils.py: 271: Train Epoch: [1][3200/4156] | Batch Time: 0.73 (0.76) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 3.89e-01 (7.04e-01)\nINFO 2025-01-26 16:21:55,593 train_utils.py: 271: Train Epoch: [1][3210/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 3.21e-01 (7.03e-01)\nINFO 2025-01-26 16:22:02,649 train_utils.py: 271: Train Epoch: [1][3220/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 7.22e-01 (7.02e-01)\nINFO 2025-01-26 16:22:09,625 train_utils.py: 271: Train Epoch: [1][3230/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 4.26e-01 (7.02e-01)\nINFO 2025-01-26 16:22:16,648 train_utils.py: 271: Train Epoch: [1][3240/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 1.41e-01 (7.01e-01)\nINFO 2025-01-26 16:22:23,769 train_utils.py: 271: Train Epoch: [1][3250/4156] | Batch Time: 0.73 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 1.02e+00 (7.01e-01)\nINFO 2025-01-26 16:22:30,791 train_utils.py: 271: Train Epoch: [1][3260/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 6.28e-01 (7.01e-01)\nINFO 2025-01-26 16:22:37,765 train_utils.py: 271: Train Epoch: [1][3270/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 6.50e-01 (7.00e-01)\nINFO 2025-01-26 16:22:44,773 train_utils.py: 271: Train Epoch: [1][3280/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 1.08e-01 (7.01e-01)\nINFO 2025-01-26 16:22:51,854 train_utils.py: 271: Train Epoch: [1][3290/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 1.04e+00 (7.00e-01)\nINFO 2025-01-26 16:22:58,915 train_utils.py: 271: Train Epoch: [1][3300/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 1.71e-01 (6.99e-01)\nINFO 2025-01-26 16:23:06,159 train_utils.py: 271: Train Epoch: [1][3310/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 2.44e-01 (6.99e-01)\nINFO 2025-01-26 16:23:13,211 train_utils.py: 271: Train Epoch: [1][3320/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 2.97e+00 (6.99e-01)\nINFO 2025-01-26 16:23:20,281 train_utils.py: 271: Train Epoch: [1][3330/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 4.54e-01 (6.98e-01)\nINFO 2025-01-26 16:23:27,241 train_utils.py: 271: Train Epoch: [1][3340/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 2.82e-01 (7.00e-01)\nINFO 2025-01-26 16:23:34,255 train_utils.py: 271: Train Epoch: [1][3350/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 1.16e+00 (7.01e-01)\nINFO 2025-01-26 16:23:41,328 train_utils.py: 271: Train Epoch: [1][3360/4156] | Batch Time: 0.73 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 8.03e-01 (7.01e-01)\nINFO 2025-01-26 16:23:48,379 train_utils.py: 271: Train Epoch: [1][3370/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.93e-01 (7.00e-01)\nINFO 2025-01-26 16:23:55,411 train_utils.py: 271: Train Epoch: [1][3380/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.63e-01 (7.00e-01)\nINFO 2025-01-26 16:24:02,511 train_utils.py: 271: Train Epoch: [1][3390/4156] | Batch Time: 0.76 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 5.98e-01 (6.99e-01)\nINFO 2025-01-26 16:24:09,610 train_utils.py: 271: Train Epoch: [1][3400/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 2.55e-01 (6.99e-01)\nINFO 2025-01-26 16:24:16,763 train_utils.py: 271: Train Epoch: [1][3410/4156] | Batch Time: 0.73 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 2.65e-01 (7.00e-01)\nINFO 2025-01-26 16:24:23,785 train_utils.py: 271: Train Epoch: [1][3420/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.07e-01 (6.99e-01)\nINFO 2025-01-26 16:24:30,867 train_utils.py: 271: Train Epoch: [1][3430/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.74e-01 (6.98e-01)\nINFO 2025-01-26 16:24:37,835 train_utils.py: 271: Train Epoch: [1][3440/4156] | Batch Time: 0.73 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 2.87e-01 (6.98e-01)\nINFO 2025-01-26 16:24:44,761 train_utils.py: 271: Train Epoch: [1][3450/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 2.92e-01 (6.97e-01)\nINFO 2025-01-26 16:24:51,800 train_utils.py: 271: Train Epoch: [1][3460/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 6.15e-01 (6.96e-01)\nINFO 2025-01-26 16:24:58,894 train_utils.py: 271: Train Epoch: [1][3470/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 1.62e-01 (6.98e-01)\nINFO 2025-01-26 16:25:05,978 train_utils.py: 271: Train Epoch: [1][3480/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 2.83e-01 (6.97e-01)\nINFO 2025-01-26 16:25:12,975 train_utils.py: 271: Train Epoch: [1][3490/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 1.58e-01 (6.97e-01)\nINFO 2025-01-26 16:25:20,005 train_utils.py: 271: Train Epoch: [1][3500/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.05) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 2.12e-01 (6.96e-01)\nINFO 2025-01-26 16:25:26,993 train_utils.py: 271: Train Epoch: [1][3510/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 3.30e-01 (6.95e-01)\nINFO 2025-01-26 16:25:34,023 train_utils.py: 271: Train Epoch: [1][3520/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 7.70e-01 (6.94e-01)\nINFO 2025-01-26 16:25:41,058 train_utils.py: 271: Train Epoch: [1][3530/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 3.18e-01 (6.97e-01)\nINFO 2025-01-26 16:25:48,224 train_utils.py: 271: Train Epoch: [1][3540/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 7.83e-01 (6.97e-01)\nINFO 2025-01-26 16:25:55,276 train_utils.py: 271: Train Epoch: [1][3550/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 1.42e-01 (6.96e-01)\nINFO 2025-01-26 16:26:02,239 train_utils.py: 271: Train Epoch: [1][3560/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 3.97e-01 (6.96e-01)\nINFO 2025-01-26 16:26:09,179 train_utils.py: 271: Train Epoch: [1][3570/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 1.62e+00 (6.97e-01)\nINFO 2025-01-26 16:26:16,223 train_utils.py: 271: Train Epoch: [1][3580/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 3.95e-01 (6.97e-01)\nINFO 2025-01-26 16:26:23,235 train_utils.py: 271: Train Epoch: [1][3590/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 1.44e+00 (6.97e-01)\nINFO 2025-01-26 16:26:30,546 train_utils.py: 271: Train Epoch: [1][3600/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 1.68e-01 (6.97e-01)\nINFO 2025-01-26 16:26:37,512 train_utils.py: 271: Train Epoch: [1][3610/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 2.30e-01 (6.97e-01)\nINFO 2025-01-26 16:26:44,606 train_utils.py: 271: Train Epoch: [1][3620/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 1.15e+00 (6.96e-01)\nINFO 2025-01-26 16:26:51,654 train_utils.py: 271: Train Epoch: [1][3630/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 1.01e-01 (6.96e-01)\nINFO 2025-01-26 16:26:58,669 train_utils.py: 271: Train Epoch: [1][3640/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 3.36e-01 (6.96e-01)\nINFO 2025-01-26 16:27:05,750 train_utils.py: 271: Train Epoch: [1][3650/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 3.19e-01 (6.98e-01)\nINFO 2025-01-26 16:27:12,774 train_utils.py: 271: Train Epoch: [1][3660/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 6.44e-01 (6.98e-01)\nINFO 2025-01-26 16:27:19,756 train_utils.py: 271: Train Epoch: [1][3670/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 4.01e-01 (6.98e-01)\nINFO 2025-01-26 16:27:26,821 train_utils.py: 271: Train Epoch: [1][3680/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 4.59e-01 (6.98e-01)\nINFO 2025-01-26 16:27:33,978 train_utils.py: 271: Train Epoch: [1][3690/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 3.29e-01 (6.97e-01)\nINFO 2025-01-26 16:27:41,040 train_utils.py: 271: Train Epoch: [1][3700/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 1.90e-01 (6.97e-01)\nINFO 2025-01-26 16:27:48,092 train_utils.py: 271: Train Epoch: [1][3710/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 7.87e-01 (6.96e-01)\nINFO 2025-01-26 16:27:54,994 train_utils.py: 271: Train Epoch: [1][3720/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 1.83e-01 (6.97e-01)\nINFO 2025-01-26 16:28:02,224 train_utils.py: 271: Train Epoch: [1][3730/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 5.86e-01 (6.96e-01)\nINFO 2025-01-26 16:28:09,301 train_utils.py: 271: Train Epoch: [1][3740/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 5.29e-01 (6.96e-01)\nINFO 2025-01-26 16:28:16,288 train_utils.py: 271: Train Epoch: [1][3750/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 5.09e-01 (6.95e-01)\nINFO 2025-01-26 16:28:23,256 train_utils.py: 271: Train Epoch: [1][3760/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 5.75e-01 (6.95e-01)\nINFO 2025-01-26 16:28:30,260 train_utils.py: 271: Train Epoch: [1][3770/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 6.83e-01 (6.95e-01)\nINFO 2025-01-26 16:28:37,145 train_utils.py: 271: Train Epoch: [1][3780/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 2.08e+00 (6.95e-01)\nINFO 2025-01-26 16:28:44,187 train_utils.py: 271: Train Epoch: [1][3790/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 2.00e-01 (6.95e-01)\nINFO 2025-01-26 16:28:51,201 train_utils.py: 271: Train Epoch: [1][3800/4156] | Batch Time: 0.74 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 11.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 3.06e-01 (6.95e-01)\nINFO 2025-01-26 16:28:58,200 train_utils.py: 271: Train Epoch: [1][3810/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 1.81e-01 (6.94e-01)\nINFO 2025-01-26 16:29:05,237 train_utils.py: 271: Train Epoch: [1][3820/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 5.42e-01 (6.95e-01)\nINFO 2025-01-26 16:29:12,110 train_utils.py: 271: Train Epoch: [1][3830/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 2.62e-01 (6.94e-01)\nINFO 2025-01-26 16:29:19,234 train_utils.py: 271: Train Epoch: [1][3840/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 1.20e+00 (6.94e-01)\nINFO 2025-01-26 16:29:26,323 train_utils.py: 271: Train Epoch: [1][3850/4156] | Batch Time: 0.73 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 5.07e-01 (6.93e-01)\nINFO 2025-01-26 16:29:33,336 train_utils.py: 271: Train Epoch: [1][3860/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 3.11e-01 (6.93e-01)\nINFO 2025-01-26 16:29:40,246 train_utils.py: 271: Train Epoch: [1][3870/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 1.05e+00 (6.93e-01)\nINFO 2025-01-26 16:29:47,258 train_utils.py: 271: Train Epoch: [1][3880/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 4.06e-01 (6.93e-01)\nINFO 2025-01-26 16:29:54,338 train_utils.py: 271: Train Epoch: [1][3890/4156] | Batch Time: 0.68 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 4.41e-01 (6.94e-01)\nINFO 2025-01-26 16:30:01,419 train_utils.py: 271: Train Epoch: [1][3900/4156] | Batch Time: 0.74 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 2.65e+00 (6.94e-01)\nINFO 2025-01-26 16:30:08,482 train_utils.py: 271: Train Epoch: [1][3910/4156] | Batch Time: 0.71 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 5.97e-01 (6.95e-01)\nINFO 2025-01-26 16:30:15,524 train_utils.py: 271: Train Epoch: [1][3920/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 2.59e-01 (6.95e-01)\nINFO 2025-01-26 16:30:22,630 train_utils.py: 271: Train Epoch: [1][3930/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 12.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 3.55e+00 (6.97e-01)\nINFO 2025-01-26 16:30:29,653 train_utils.py: 271: Train Epoch: [1][3940/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 5.50e-01 (6.97e-01)\nINFO 2025-01-26 16:30:36,740 train_utils.py: 271: Train Epoch: [1][3950/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 1.50e+00 (6.98e-01)\nINFO 2025-01-26 16:30:43,809 train_utils.py: 271: Train Epoch: [1][3960/4156] | Batch Time: 0.72 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 2.94e-01 (6.97e-01)\nINFO 2025-01-26 16:30:50,916 train_utils.py: 271: Train Epoch: [1][3970/4156] | Batch Time: 0.70 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 4.90e-01 (6.97e-01)\nINFO 2025-01-26 16:30:57,879 train_utils.py: 271: Train Epoch: [1][3980/4156] | Batch Time: 0.73 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 1.98e-01 (6.98e-01)\nINFO 2025-01-26 16:31:04,963 train_utils.py: 271: Train Epoch: [1][3990/4156] | Batch Time: 0.69 (0.75) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 4.36e-01 (6.97e-01)\nINFO 2025-01-26 16:31:12,127 train_utils.py: 271: Train Epoch: [1][4000/4156] | Batch Time: 0.73 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 5.32e-01 (6.97e-01)\nINFO 2025-01-26 16:31:19,265 train_utils.py: 271: Train Epoch: [1][4010/4156] | Batch Time: 0.72 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 4.76e-01 (6.97e-01)\nINFO 2025-01-26 16:31:26,351 train_utils.py: 271: Train Epoch: [1][4020/4156] | Batch Time: 0.69 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 1.51e-01 (6.96e-01)\nINFO 2025-01-26 16:31:33,422 train_utils.py: 271: Train Epoch: [1][4030/4156] | Batch Time: 0.71 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 7.28e-01 (6.97e-01)\nINFO 2025-01-26 16:31:40,429 train_utils.py: 271: Train Epoch: [1][4040/4156] | Batch Time: 0.69 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 1.64e-01 (6.96e-01)\nINFO 2025-01-26 16:31:47,594 train_utils.py: 271: Train Epoch: [1][4050/4156] | Batch Time: 0.73 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 2.26e-01 (6.95e-01)\nINFO 2025-01-26 16:31:54,689 train_utils.py: 271: Train Epoch: [1][4060/4156] | Batch Time: 0.71 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 5.86e-01 (6.95e-01)\nINFO 2025-01-26 16:32:01,685 train_utils.py: 271: Train Epoch: [1][4070/4156] | Batch Time: 0.71 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 1.93e-01 (6.96e-01)\nINFO 2025-01-26 16:32:08,628 train_utils.py: 271: Train Epoch: [1][4080/4156] | Batch Time: 0.68 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 3.46e-01 (6.96e-01)\nINFO 2025-01-26 16:32:15,558 train_utils.py: 271: Train Epoch: [1][4090/4156] | Batch Time: 0.68 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 6.13e-01 (6.96e-01)\nINFO 2025-01-26 16:32:22,464 train_utils.py: 271: Train Epoch: [1][4100/4156] | Batch Time: 0.70 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 7.42e-01 (6.98e-01)\nINFO 2025-01-26 16:32:29,404 train_utils.py: 271: Train Epoch: [1][4110/4156] | Batch Time: 0.71 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 5.72e-01 (6.97e-01)\nINFO 2025-01-26 16:32:36,295 train_utils.py: 271: Train Epoch: [1][4120/4156] | Batch Time: 0.68 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 2.83e-01 (6.97e-01)\nINFO 2025-01-26 16:32:43,245 train_utils.py: 271: Train Epoch: [1][4130/4156] | Batch Time: 0.68 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 2.45e-01 (6.96e-01)\nINFO 2025-01-26 16:32:50,141 train_utils.py: 271: Train Epoch: [1][4140/4156] | Batch Time: 0.71 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 2.22e-01 (6.95e-01)\nINFO 2025-01-26 16:32:57,397 train_utils.py: 271: Train Epoch: [1][4150/4156] | Batch Time: 0.67 (0.74) | Data Time: 0.00 (0.04) | Mem (GB): 10.00 (10.10/12.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 1.84e-01 (6.97e-01)\nINFO 2025-01-26 16:33:07,780 trainer.py: 950: Estimated time remaining: 00d 02h 34m\nINFO 2025-01-26 16:33:07,786 trainer.py: 892: Synchronizing meters\nINFO 2025-01-26 16:33:07,786 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 0.6969918606128755, 'Losses/train_all_loss_mask': 0.010319281246041794, 'Losses/train_all_loss_dice': 0.36251049207503744, 'Losses/train_all_loss_iou': 0.12809565615309385, 'Losses/train_all_loss_class': 9.093785089801032e-08, 'Losses/train_all_core_loss': 0.6969918606128755, 'Trainer/where': 0.39995187680461985, 'Trainer/epoch': 1, 'Trainer/steps_train': 8312}\nINFO 2025-01-26 16:35:50,256 train_utils.py: 271: Train Epoch: [2][   0/4156] | Batch Time: 158.79 (158.79) | Data Time: 158.05 (158.05) | Mem (GB): 12.00 (12.00/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 3.48e+00 (3.48e+00)\nINFO 2025-01-26 16:35:57,351 train_utils.py: 271: Train Epoch: [2][  10/4156] | Batch Time: 0.69 (15.08) | Data Time: 0.00 (14.37) | Mem (GB): 10.00 (10.27/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 3.61e-01 (9.06e-01)\nINFO 2025-01-26 16:36:04,504 train_utils.py: 271: Train Epoch: [2][  20/4156] | Batch Time: 0.69 (8.24) | Data Time: 0.00 (7.53) | Mem (GB): 10.00 (10.19/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 2.18e-01 (8.61e-01)\nINFO 2025-01-26 16:36:11,695 train_utils.py: 271: Train Epoch: [2][  30/4156] | Batch Time: 0.71 (5.81) | Data Time: 0.00 (5.10) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 3.09e-01 (8.00e-01)\nINFO 2025-01-26 16:36:18,747 train_utils.py: 271: Train Epoch: [2][  40/4156] | Batch Time: 0.70 (4.57) | Data Time: 0.00 (3.86) | Mem (GB): 10.00 (10.15/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 2.38e-01 (7.91e-01)\nINFO 2025-01-26 16:36:25,838 train_utils.py: 271: Train Epoch: [2][  50/4156] | Batch Time: 0.71 (3.81) | Data Time: 0.00 (3.10) | Mem (GB): 10.00 (10.20/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 9.14e-01 (8.99e-01)\nINFO 2025-01-26 16:36:32,951 train_utils.py: 271: Train Epoch: [2][  60/4156] | Batch Time: 0.71 (3.30) | Data Time: 0.00 (2.59) | Mem (GB): 10.00 (10.20/12.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 4.67e-01 (8.75e-01)\nINFO 2025-01-26 16:36:39,997 train_utils.py: 271: Train Epoch: [2][  70/4156] | Batch Time: 0.72 (2.94) | Data Time: 0.00 (2.23) | Mem (GB): 11.00 (10.18/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 3.03e+00 (8.47e-01)\nINFO 2025-01-26 16:36:46,973 train_utils.py: 271: Train Epoch: [2][  80/4156] | Batch Time: 0.72 (2.66) | Data Time: 0.00 (1.95) | Mem (GB): 10.00 (10.19/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 2.54e-01 (8.23e-01)\nINFO 2025-01-26 16:36:54,023 train_utils.py: 271: Train Epoch: [2][  90/4156] | Batch Time: 0.68 (2.45) | Data Time: 0.00 (1.74) | Mem (GB): 10.00 (10.16/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 1.35e+00 (7.73e-01)\nINFO 2025-01-26 16:37:01,037 train_utils.py: 271: Train Epoch: [2][ 100/4156] | Batch Time: 0.73 (2.27) | Data Time: 0.00 (1.57) | Mem (GB): 10.00 (10.15/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 6.78e-01 (7.76e-01)\nINFO 2025-01-26 16:37:08,054 train_utils.py: 271: Train Epoch: [2][ 110/4156] | Batch Time: 0.72 (2.13) | Data Time: 0.00 (1.42) | Mem (GB): 10.00 (10.15/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 3.44e-01 (7.93e-01)\nINFO 2025-01-26 16:37:15,195 train_utils.py: 271: Train Epoch: [2][ 120/4156] | Batch Time: 0.71 (2.01) | Data Time: 0.00 (1.31) | Mem (GB): 10.00 (10.16/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 2.00e-01 (8.01e-01)\nINFO 2025-01-26 16:37:22,355 train_utils.py: 271: Train Epoch: [2][ 130/4156] | Batch Time: 0.73 (1.92) | Data Time: 0.00 (1.21) | Mem (GB): 10.00 (10.15/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 2.53e-01 (7.86e-01)\nINFO 2025-01-26 16:37:29,520 train_utils.py: 271: Train Epoch: [2][ 140/4156] | Batch Time: 0.72 (1.83) | Data Time: 0.00 (1.12) | Mem (GB): 10.00 (10.16/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 2.56e-01 (7.66e-01)\nINFO 2025-01-26 16:37:36,593 train_utils.py: 271: Train Epoch: [2][ 150/4156] | Batch Time: 0.71 (1.76) | Data Time: 0.00 (1.05) | Mem (GB): 10.00 (10.15/12.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 1.94e+00 (7.67e-01)\nINFO 2025-01-26 16:37:43,611 train_utils.py: 271: Train Epoch: [2][ 160/4156] | Batch Time: 0.70 (1.69) | Data Time: 0.00 (0.98) | Mem (GB): 10.00 (10.14/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 2.36e-01 (7.55e-01)\nINFO 2025-01-26 16:37:50,678 train_utils.py: 271: Train Epoch: [2][ 170/4156] | Batch Time: 0.72 (1.63) | Data Time: 0.00 (0.93) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 5.56e-01 (7.41e-01)\nINFO 2025-01-26 16:37:57,721 train_utils.py: 271: Train Epoch: [2][ 180/4156] | Batch Time: 0.70 (1.58) | Data Time: 0.00 (0.87) | Mem (GB): 11.00 (10.14/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 7.02e-01 (7.35e-01)\nINFO 2025-01-26 16:38:04,904 train_utils.py: 271: Train Epoch: [2][ 190/4156] | Batch Time: 0.68 (1.54) | Data Time: 0.00 (0.83) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 7.89e-02 (7.42e-01)\nINFO 2025-01-26 16:38:11,990 train_utils.py: 271: Train Epoch: [2][ 200/4156] | Batch Time: 0.74 (1.50) | Data Time: 0.00 (0.79) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 2.81e-01 (7.34e-01)\nINFO 2025-01-26 16:38:19,007 train_utils.py: 271: Train Epoch: [2][ 210/4156] | Batch Time: 0.72 (1.46) | Data Time: 0.00 (0.75) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 7.49e-01 (7.25e-01)\nINFO 2025-01-26 16:38:26,085 train_utils.py: 271: Train Epoch: [2][ 220/4156] | Batch Time: 0.68 (1.42) | Data Time: 0.00 (0.72) | Mem (GB): 11.00 (10.13/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 1.67e+00 (7.16e-01)\nINFO 2025-01-26 16:38:33,112 train_utils.py: 271: Train Epoch: [2][ 230/4156] | Batch Time: 0.69 (1.39) | Data Time: 0.00 (0.69) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 7.64e-01 (7.15e-01)\nINFO 2025-01-26 16:38:40,169 train_utils.py: 271: Train Epoch: [2][ 240/4156] | Batch Time: 0.72 (1.36) | Data Time: 0.00 (0.66) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 5.67e-01 (7.05e-01)\nINFO 2025-01-26 16:38:47,251 train_utils.py: 271: Train Epoch: [2][ 250/4156] | Batch Time: 0.71 (1.34) | Data Time: 0.00 (0.63) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 1.45e-01 (6.99e-01)\nINFO 2025-01-26 16:38:54,269 train_utils.py: 271: Train Epoch: [2][ 260/4156] | Batch Time: 0.72 (1.31) | Data Time: 0.00 (0.61) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 2.51e-01 (6.91e-01)\nINFO 2025-01-26 16:39:01,270 train_utils.py: 271: Train Epoch: [2][ 270/4156] | Batch Time: 0.68 (1.29) | Data Time: 0.00 (0.58) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 3.30e-01 (6.96e-01)\nINFO 2025-01-26 16:39:08,362 train_utils.py: 271: Train Epoch: [2][ 280/4156] | Batch Time: 0.69 (1.27) | Data Time: 0.00 (0.56) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 1.01e+00 (7.07e-01)\nINFO 2025-01-26 16:39:15,366 train_utils.py: 271: Train Epoch: [2][ 290/4156] | Batch Time: 0.69 (1.25) | Data Time: 0.00 (0.54) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 3.63e-01 (8.19e-01)\nINFO 2025-01-26 16:39:22,463 train_utils.py: 271: Train Epoch: [2][ 300/4156] | Batch Time: 0.75 (1.23) | Data Time: 0.00 (0.53) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 3.44e-01 (8.47e-01)\nINFO 2025-01-26 16:39:29,593 train_utils.py: 271: Train Epoch: [2][ 310/4156] | Batch Time: 0.73 (1.22) | Data Time: 0.00 (0.51) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 7.91e-01 (8.40e-01)\nINFO 2025-01-26 16:39:36,733 train_utils.py: 271: Train Epoch: [2][ 320/4156] | Batch Time: 0.75 (1.20) | Data Time: 0.00 (0.49) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 7.48e-01 (8.56e-01)\nINFO 2025-01-26 16:39:43,698 train_utils.py: 271: Train Epoch: [2][ 330/4156] | Batch Time: 0.72 (1.18) | Data Time: 0.00 (0.48) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 1.88e-01 (8.41e-01)\nINFO 2025-01-26 16:39:50,755 train_utils.py: 271: Train Epoch: [2][ 340/4156] | Batch Time: 0.72 (1.17) | Data Time: 0.00 (0.46) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 1.28e-01 (8.30e-01)\nINFO 2025-01-26 16:39:57,772 train_utils.py: 271: Train Epoch: [2][ 350/4156] | Batch Time: 0.69 (1.16) | Data Time: 0.00 (0.45) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 2.78e-01 (8.31e-01)\nINFO 2025-01-26 16:40:04,805 train_utils.py: 271: Train Epoch: [2][ 360/4156] | Batch Time: 0.69 (1.14) | Data Time: 0.00 (0.44) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 4.43e-01 (8.26e-01)\nINFO 2025-01-26 16:40:11,945 train_utils.py: 271: Train Epoch: [2][ 370/4156] | Batch Time: 0.70 (1.13) | Data Time: 0.00 (0.43) | Mem (GB): 11.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 6.13e-01 (8.21e-01)\nINFO 2025-01-26 16:40:19,114 train_utils.py: 271: Train Epoch: [2][ 380/4156] | Batch Time: 0.73 (1.12) | Data Time: 0.00 (0.42) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 4.12e-01 (8.09e-01)\nINFO 2025-01-26 16:40:26,098 train_utils.py: 271: Train Epoch: [2][ 390/4156] | Batch Time: 0.69 (1.11) | Data Time: 0.00 (0.41) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 6.30e-01 (7.99e-01)\nINFO 2025-01-26 16:40:33,259 train_utils.py: 271: Train Epoch: [2][ 400/4156] | Batch Time: 0.69 (1.10) | Data Time: 0.00 (0.39) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 1.47e-01 (7.89e-01)\nINFO 2025-01-26 16:40:40,370 train_utils.py: 271: Train Epoch: [2][ 410/4156] | Batch Time: 0.71 (1.09) | Data Time: 0.00 (0.39) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 6.11e-01 (7.90e-01)\nINFO 2025-01-26 16:40:47,434 train_utils.py: 271: Train Epoch: [2][ 420/4156] | Batch Time: 0.73 (1.08) | Data Time: 0.00 (0.38) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 2.86e-01 (7.81e-01)\nINFO 2025-01-26 16:40:54,548 train_utils.py: 271: Train Epoch: [2][ 430/4156] | Batch Time: 0.71 (1.07) | Data Time: 0.00 (0.37) | Mem (GB): 10.00 (10.13/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 8.07e-01 (7.77e-01)\nINFO 2025-01-26 16:41:01,679 train_utils.py: 271: Train Epoch: [2][ 440/4156] | Batch Time: 0.69 (1.07) | Data Time: 0.00 (0.36) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 2.91e-01 (7.68e-01)\nINFO 2025-01-26 16:41:08,701 train_utils.py: 271: Train Epoch: [2][ 450/4156] | Batch Time: 0.69 (1.06) | Data Time: 0.00 (0.35) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 9.39e-01 (7.60e-01)\nINFO 2025-01-26 16:41:15,863 train_utils.py: 271: Train Epoch: [2][ 460/4156] | Batch Time: 0.70 (1.05) | Data Time: 0.00 (0.34) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 8.56e-01 (7.60e-01)\nINFO 2025-01-26 16:41:22,929 train_utils.py: 271: Train Epoch: [2][ 470/4156] | Batch Time: 0.72 (1.04) | Data Time: 0.00 (0.34) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 3.12e-01 (7.56e-01)\nINFO 2025-01-26 16:41:29,918 train_utils.py: 271: Train Epoch: [2][ 480/4156] | Batch Time: 0.71 (1.04) | Data Time: 0.00 (0.33) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 2.56e+00 (7.58e-01)\nINFO 2025-01-26 16:41:36,890 train_utils.py: 271: Train Epoch: [2][ 490/4156] | Batch Time: 0.68 (1.03) | Data Time: 0.00 (0.32) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 51m | Losses/train_all_loss: 2.09e-01 (7.55e-01)\nINFO 2025-01-26 16:41:43,887 train_utils.py: 271: Train Epoch: [2][ 500/4156] | Batch Time: 0.72 (1.02) | Data Time: 0.00 (0.32) | Mem (GB): 11.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 6.23e-01 (7.56e-01)\nINFO 2025-01-26 16:41:50,929 train_utils.py: 271: Train Epoch: [2][ 510/4156] | Batch Time: 0.70 (1.02) | Data Time: 0.00 (0.31) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 3.20e-01 (7.52e-01)\nINFO 2025-01-26 16:41:57,921 train_utils.py: 271: Train Epoch: [2][ 520/4156] | Batch Time: 0.70 (1.01) | Data Time: 0.00 (0.30) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 4.20e-01 (7.50e-01)\nINFO 2025-01-26 16:42:05,042 train_utils.py: 271: Train Epoch: [2][ 530/4156] | Batch Time: 0.72 (1.00) | Data Time: 0.00 (0.30) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 1.90e-01 (7.50e-01)\nINFO 2025-01-26 16:42:12,089 train_utils.py: 271: Train Epoch: [2][ 540/4156] | Batch Time: 0.71 (1.00) | Data Time: 0.00 (0.29) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 1.51e+00 (7.47e-01)\nINFO 2025-01-26 16:42:19,080 train_utils.py: 271: Train Epoch: [2][ 550/4156] | Batch Time: 0.69 (0.99) | Data Time: 0.00 (0.29) | Mem (GB): 11.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 5.76e-01 (7.40e-01)\nINFO 2025-01-26 16:42:26,101 train_utils.py: 271: Train Epoch: [2][ 560/4156] | Batch Time: 0.68 (0.99) | Data Time: 0.00 (0.28) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 3.26e-01 (7.37e-01)\nINFO 2025-01-26 16:42:33,165 train_utils.py: 271: Train Epoch: [2][ 570/4156] | Batch Time: 0.70 (0.98) | Data Time: 0.00 (0.28) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 52m | Losses/train_all_loss: 2.53e-01 (7.42e-01)\nINFO 2025-01-26 16:42:40,196 train_utils.py: 271: Train Epoch: [2][ 580/4156] | Batch Time: 0.69 (0.98) | Data Time: 0.00 (0.27) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 1.66e-01 (7.36e-01)\nINFO 2025-01-26 16:42:47,228 train_utils.py: 271: Train Epoch: [2][ 590/4156] | Batch Time: 0.71 (0.97) | Data Time: 0.00 (0.27) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 5.28e-01 (7.42e-01)\nINFO 2025-01-26 16:42:54,351 train_utils.py: 271: Train Epoch: [2][ 600/4156] | Batch Time: 0.73 (0.97) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 2.05e+00 (7.44e-01)\nINFO 2025-01-26 16:43:01,361 train_utils.py: 271: Train Epoch: [2][ 610/4156] | Batch Time: 0.71 (0.97) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 7.69e-01 (7.47e-01)\nINFO 2025-01-26 16:43:08,325 train_utils.py: 271: Train Epoch: [2][ 620/4156] | Batch Time: 0.69 (0.96) | Data Time: 0.00 (0.26) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 2.56e-01 (7.42e-01)\nINFO 2025-01-26 16:43:15,343 train_utils.py: 271: Train Epoch: [2][ 630/4156] | Batch Time: 0.72 (0.96) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 3.47e-01 (7.42e-01)\nINFO 2025-01-26 16:43:22,361 train_utils.py: 271: Train Epoch: [2][ 640/4156] | Batch Time: 0.72 (0.95) | Data Time: 0.00 (0.25) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 5.36e-01 (7.42e-01)\nINFO 2025-01-26 16:43:29,376 train_utils.py: 271: Train Epoch: [2][ 650/4156] | Batch Time: 0.72 (0.95) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 4.00e-01 (7.39e-01)\nINFO 2025-01-26 16:43:36,389 train_utils.py: 271: Train Epoch: [2][ 660/4156] | Batch Time: 0.72 (0.95) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 53m | Losses/train_all_loss: 3.17e-01 (7.37e-01)\nINFO 2025-01-26 16:43:44,059 train_utils.py: 271: Train Epoch: [2][ 670/4156] | Batch Time: 0.69 (0.94) | Data Time: 0.00 (0.24) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 3.99e-01 (7.41e-01)\nINFO 2025-01-26 16:43:51,211 train_utils.py: 271: Train Epoch: [2][ 680/4156] | Batch Time: 0.72 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 5.16e-01 (7.41e-01)\nINFO 2025-01-26 16:43:58,204 train_utils.py: 271: Train Epoch: [2][ 690/4156] | Batch Time: 0.72 (0.94) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 2.28e-01 (7.42e-01)\nINFO 2025-01-26 16:44:05,184 train_utils.py: 271: Train Epoch: [2][ 700/4156] | Batch Time: 0.70 (0.93) | Data Time: 0.00 (0.23) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 4.18e-01 (7.53e-01)\nINFO 2025-01-26 16:44:12,290 train_utils.py: 271: Train Epoch: [2][ 710/4156] | Batch Time: 0.72 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 1.73e+00 (7.49e-01)\nINFO 2025-01-26 16:44:19,422 train_utils.py: 271: Train Epoch: [2][ 720/4156] | Batch Time: 0.72 (0.93) | Data Time: 0.00 (0.22) | Mem (GB): 12.00 (10.12/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 1.57e+00 (7.48e-01)\nINFO 2025-01-26 16:44:26,568 train_utils.py: 271: Train Epoch: [2][ 730/4156] | Batch Time: 0.74 (0.92) | Data Time: 0.00 (0.22) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 1.60e+00 (7.46e-01)\nINFO 2025-01-26 16:44:33,622 train_utils.py: 271: Train Epoch: [2][ 740/4156] | Batch Time: 0.70 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 12.00 (10.12/12.00) | Time Elapsed: 00d 01h 54m | Losses/train_all_loss: 6.21e-01 (7.44e-01)\nINFO 2025-01-26 16:44:40,750 train_utils.py: 271: Train Epoch: [2][ 750/4156] | Batch Time: 0.69 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 7.02e-01 (7.40e-01)\nINFO 2025-01-26 16:44:47,854 train_utils.py: 271: Train Epoch: [2][ 760/4156] | Batch Time: 0.72 (0.92) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 3.21e-01 (7.37e-01)\nINFO 2025-01-26 16:44:54,883 train_utils.py: 271: Train Epoch: [2][ 770/4156] | Batch Time: 0.69 (0.91) | Data Time: 0.00 (0.21) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 3.78e-01 (7.36e-01)\nINFO 2025-01-26 16:45:01,920 train_utils.py: 271: Train Epoch: [2][ 780/4156] | Batch Time: 0.68 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 5.14e-01 (7.46e-01)\nINFO 2025-01-26 16:45:08,963 train_utils.py: 271: Train Epoch: [2][ 790/4156] | Batch Time: 0.69 (0.91) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 3.73e-01 (7.41e-01)\nINFO 2025-01-26 16:45:16,121 train_utils.py: 271: Train Epoch: [2][ 800/4156] | Batch Time: 0.73 (0.90) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 3.13e-01 (7.43e-01)\nINFO 2025-01-26 16:45:23,402 train_utils.py: 271: Train Epoch: [2][ 810/4156] | Batch Time: 0.69 (0.90) | Data Time: 0.00 (0.20) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 2.06e-01 (7.38e-01)\nINFO 2025-01-26 16:45:30,548 train_utils.py: 271: Train Epoch: [2][ 820/4156] | Batch Time: 0.72 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 2.64e-01 (7.37e-01)\nINFO 2025-01-26 16:45:37,608 train_utils.py: 271: Train Epoch: [2][ 830/4156] | Batch Time: 0.69 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 55m | Losses/train_all_loss: 4.40e-01 (7.35e-01)\nINFO 2025-01-26 16:45:44,765 train_utils.py: 271: Train Epoch: [2][ 840/4156] | Batch Time: 0.71 (0.90) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 1.96e-01 (7.34e-01)\nINFO 2025-01-26 16:45:51,785 train_utils.py: 271: Train Epoch: [2][ 850/4156] | Batch Time: 0.72 (0.89) | Data Time: 0.00 (0.19) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 3.45e-01 (7.40e-01)\nINFO 2025-01-26 16:45:58,849 train_utils.py: 271: Train Epoch: [2][ 860/4156] | Batch Time: 0.68 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 2.89e-01 (7.36e-01)\nINFO 2025-01-26 16:46:05,921 train_utils.py: 271: Train Epoch: [2][ 870/4156] | Batch Time: 0.71 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 3.73e-01 (7.35e-01)\nINFO 2025-01-26 16:46:12,975 train_utils.py: 271: Train Epoch: [2][ 880/4156] | Batch Time: 0.72 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 2.15e-01 (7.33e-01)\nINFO 2025-01-26 16:46:20,078 train_utils.py: 271: Train Epoch: [2][ 890/4156] | Batch Time: 0.72 (0.89) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 2.56e-01 (7.32e-01)\nINFO 2025-01-26 16:46:27,152 train_utils.py: 271: Train Epoch: [2][ 900/4156] | Batch Time: 0.71 (0.88) | Data Time: 0.00 (0.18) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 7.79e-01 (7.30e-01)\nINFO 2025-01-26 16:46:34,149 train_utils.py: 271: Train Epoch: [2][ 910/4156] | Batch Time: 0.69 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 11.00 (10.12/12.00) | Time Elapsed: 00d 01h 56m | Losses/train_all_loss: 6.17e-01 (7.28e-01)\nINFO 2025-01-26 16:46:41,211 train_utils.py: 271: Train Epoch: [2][ 920/4156] | Batch Time: 0.72 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 2.20e-01 (7.26e-01)\nINFO 2025-01-26 16:46:48,247 train_utils.py: 271: Train Epoch: [2][ 930/4156] | Batch Time: 0.68 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 9.73e-01 (7.24e-01)\nINFO 2025-01-26 16:46:55,292 train_utils.py: 271: Train Epoch: [2][ 940/4156] | Batch Time: 0.69 (0.88) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.12/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 5.93e-01 (7.24e-01)\nINFO 2025-01-26 16:47:02,380 train_utils.py: 271: Train Epoch: [2][ 950/4156] | Batch Time: 0.71 (0.87) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 3.89e-01 (7.21e-01)\nINFO 2025-01-26 16:47:09,524 train_utils.py: 271: Train Epoch: [2][ 960/4156] | Batch Time: 0.69 (0.87) | Data Time: 0.00 (0.17) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 8.36e-01 (7.19e-01)\nINFO 2025-01-26 16:47:16,597 train_utils.py: 271: Train Epoch: [2][ 970/4156] | Batch Time: 0.72 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 1.62e-01 (7.19e-01)\nINFO 2025-01-26 16:47:23,652 train_utils.py: 271: Train Epoch: [2][ 980/4156] | Batch Time: 0.69 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 8.07e-01 (7.16e-01)\nINFO 2025-01-26 16:47:30,798 train_utils.py: 271: Train Epoch: [2][ 990/4156] | Batch Time: 0.72 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 9.73e-01 (7.13e-01)\nINFO 2025-01-26 16:47:37,922 train_utils.py: 271: Train Epoch: [2][1000/4156] | Batch Time: 0.70 (0.87) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 57m | Losses/train_all_loss: 2.23e-01 (7.11e-01)\nINFO 2025-01-26 16:47:44,923 train_utils.py: 271: Train Epoch: [2][1010/4156] | Batch Time: 0.68 (0.86) | Data Time: 0.00 (0.16) | Mem (GB): 10.00 (10.11/12.00) | Time Elapsed: 00d 01h 58m | Losses/train_all_loss: 3.22e-01 (7.12e-01)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nos.listdir('/kaggle/working/sam2/checkpoints')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_file(\n    path_or_fileobj='/kaggle/working/sam2/sam2_logs/configs/train.yaml/checkpoints/checkpoint.pt',\n    path_in_repo=\"sm_base_3ep.pth\",\n    repo_id=\"andrey200702/REC_SYS\",\n    repo_type=\"dataset\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd ../\nimport os\nfrom PIL import Image\nimport json\nmass = []\nfor path in os.listdir('/kaggle/working/sam2/hui-5/train'):\n    if path.endswith('.json'):\n        anns = json.load(open(f\"/kaggle/working/sam2/hui-5/train/{path}\"))\n        mass.append(anns['image']['image_id'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.tensor([6455450141600579512]).dtype","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"decode(ann['annotations'][0]['segmentation'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pycocotools.mask import decode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}