{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10552556,"sourceType":"datasetVersion","datasetId":6529178}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/segment-anything-2.git\n%cd /kaggle/working/segment-anything-2\n%pip install -e .\n%cd /kaggle/working/segment-anything-2/checkpoints/\n!bash /kaggle/working/segment-anything-2/checkpoints/download_ckpts.sh\n%cd /kaggle/working/segment-anything-2/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T19:26:05.886442Z","iopub.execute_input":"2025-01-27T19:26:05.886689Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'segment-anything-2'...\nremote: Enumerating objects: 1070, done.\u001b[K\nremote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\u001b[K\nReceiving objects: 100% (1070/1070), 134.70 MiB | 23.54 MiB/s, done.\nResolving deltas: 100% (376/376), done.\n/kaggle/working/segment-anything-2\nObtaining file:///kaggle/working/segment-anything-2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!wget https://storage.yandexcloud.net/ds-ods/files/files/2ad601fd/test.zip\n!wget https://storage.yandexcloud.net/ds-ods/files/submissions/21dd71fc-63e7-4419-8cc6-ec6af1214dee/62ab2c57/NTO_SUB0%202.zip\n!pip install supervision ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# if using Apple MPS, fall back to CPU for unsupported ops\nos.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:48:35.684802Z","iopub.execute_input":"2025-01-25T16:48:35.685137Z","iopub.status.idle":"2025-01-25T16:48:38.706283Z","shell.execute_reply.started":"2025-01-25T16:48:35.685109Z","shell.execute_reply":"2025-01-25T16:48:38.705589Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%capture\n!unzip /kaggle/working/segment-anything-2/test.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:48:38.706992Z","iopub.execute_input":"2025-01-25T16:48:38.707345Z","iopub.status.idle":"2025-01-25T16:48:51.253321Z","shell.execute_reply.started":"2025-01-25T16:48:38.707324Z","shell.execute_reply":"2025-01-25T16:48:51.252519Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# select the device for computation\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"using device: {device}\")\n\nif device.type == \"cuda\":\n    # use bfloat16 for the entire notebook\n    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n    if torch.cuda.get_device_properties(0).major >= 8:\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\nelif device.type == \"mps\":\n    print(\n        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:50:28.357180Z","iopub.execute_input":"2025-01-25T16:50:28.357538Z","iopub.status.idle":"2025-01-25T16:50:28.364466Z","shell.execute_reply.started":"2025-01-25T16:50:28.357514Z","shell.execute_reply":"2025-01-25T16:50:28.363574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://huggingface.co/datasets/andrey200702/REC_SYS/resolve/main/sm_base_6ep.pth?download=true\n!mv ./sm_base_6ep.pth?download=true ./sm_base_6ep.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:50:29.752864Z","iopub.execute_input":"2025-01-25T16:50:29.753213Z","iopub.status.idle":"2025-01-25T16:50:52.657181Z","shell.execute_reply.started":"2025-01-25T16:50:29.753182Z","shell.execute_reply":"2025-01-25T16:50:52.655956Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://huggingface.co/datasets/andrey200702/REC_SYS/resolve/main/sm_base_3ep.pth?download=true\n!mv ./sm_base_3ep.pth?download=true ./sm_base_3ep.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\nsam2_checkpoint = \"./sm_base_6ep.pth\"\nmodel_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n\nsam2_model_large = build_sam2(model_cfg, sam2_checkpoint, device=device)\n\npredictor_large = SAM2ImagePredictor(sam2_model_large)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:53:56.871319Z","iopub.execute_input":"2025-01-25T16:53:56.871806Z","iopub.status.idle":"2025-01-25T16:53:58.725318Z","shell.execute_reply.started":"2025-01-25T16:53:56.871777Z","shell.execute_reply":"2025-01-25T16:53:58.724385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%capture\n!pip install ultralytics supervision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:54:01.933277Z","iopub.execute_input":"2025-01-25T16:54:01.933617Z","iopub.status.idle":"2025-01-25T16:54:06.298557Z","shell.execute_reply.started":"2025-01-25T16:54:01.933586Z","shell.execute_reply":"2025-01-25T16:54:06.297665Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from  ultralytics import YOLO\nimport supervision as sv\nmodel = YOLO('/kaggle/input/best-yolo/best (1).pt')\n\ndef predict_yolo_image(image):\n    return sv.Detections.from_ultralytics(model(image,conf=0.4)[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:54:06.299883Z","iopub.execute_input":"2025-01-25T16:54:06.300197Z","iopub.status.idle":"2025-01-25T16:54:11.410399Z","shell.execute_reply.started":"2025-01-25T16:54:06.300153Z","shell.execute_reply":"2025-01-25T16:54:11.409620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_sam(image,box):\n    predictor.set_image(image)\n    masks, scores, _ = predictor.predict(\n        point_coords=None,\n        point_labels=None,\n        box=box,\n        multimask_output=False,\n    )\n    return masks, scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:54:17.692776Z","iopub.execute_input":"2025-01-25T16:54:17.693460Z","iopub.status.idle":"2025-01-25T16:54:17.697775Z","shell.execute_reply.started":"2025-01-25T16:54:17.693421Z","shell.execute_reply":"2025-01-25T16:54:17.696885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(3)\n\ndef show_mask(mask, ax, random_color=False, borders = True):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    if borders:\n        import cv2\n        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n        # Try to smooth contours\n        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n    ax.imshow(mask_image)\n\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n\ndef show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image)\n        show_mask(mask, plt.gca(), borders=borders)\n        if point_coords is not None:\n            assert input_labels is not None\n            show_points(point_coords, input_labels, plt.gca())\n        if box_coords is not None:\n            # boxes\n            show_box(box_coords, plt.gca())\n        if len(scores) > 1:\n            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:54:19.311584Z","iopub.execute_input":"2025-01-25T16:54:19.311880Z","iopub.status.idle":"2025-01-25T16:54:19.322915Z","shell.execute_reply.started":"2025-01-25T16:54:19.311858Z","shell.execute_reply":"2025-01-25T16:54:19.322056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport supervision as sv\nimport pycocotools\nimport json\nimport pycocotools.mask\nimport numpy as np\nfrom ultralytics import YOLO\nimport albumentations as A\nimport os\n\ntest_folder = \"test\"\noutput_folder = \"output_yolo\"  # Папка для сохранения результатов\nos.makedirs(output_folder, exist_ok=True)\npredictions = []\nfor file_name in tqdm(os.listdir(test_folder)):\n    image_path = os.path.join(test_folder, file_name)\n    image = Image.open(image_path)\n    predicts = predict_yolo_image(image)\n\n    if predicts.xyxy is None:\n        continue\n    pred_classes = predicts.class_id.tolist()  # Классы объектов\n    pred_boxes = predicts.xyxy.tolist()#instances.pred_boxes.tensor.tolist()  # Координаты bbox\n    pred_scores = predicts.confidence.tolist()  # Скоринг предсказаний\n        \n    for i in range(len(pred_boxes)):\n        mask , scores = predict_sam(image,pred_boxes[i])\n        if float(scores[0]) <= 0.5:\n            continue\n        rle = pycocotools.mask.encode(np.asfortranarray(mask.astype(np.uint8)[0]))\n        predictions.append({\n            \"image_name\": file_name,\n            \"category_id\": 0,  # ID категории\n            \"bbox\": pred_boxes[i],  # Координаты bounding box\n            \"score\": float(scores[0] * 0.5 + pred_scores[i] * 0.5),  # Оценка уверенности\n            \"segmentation\": {\n                \"size\" : rle[\"size\"],\n                \"counts\" : str(rle[\"counts\"])\n            }  # Сегментация в формате RLE\n        })\n\n\n# Сохранение предсказаний в JSON\npredictions_file = os.path.join(output_folder, \"submission.json\")\nwith open(predictions_file, \"w\") as f:\n    json.dump(predictions, f, indent=4)\n\nprint(f\"Обработка завершена. Результаты сохранены в папке: {output_folder}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T16:58:09.931607Z","iopub.execute_input":"2025-01-25T16:58:09.931972Z","iopub.status.idle":"2025-01-25T16:58:14.362676Z","shell.execute_reply.started":"2025-01-25T16:58:09.931941Z","shell.execute_reply":"2025-01-25T16:58:14.361487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}