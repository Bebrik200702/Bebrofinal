{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install roboflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\ndata = json.load(open(\n    '/hui-5/train/b2bb4be281e85937cffaa9181aeab665_JPG.rf.2d8a24818745bd4e7612bc79fbd944d6.json'\n))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"5m7KUUT814dfZC9t6Jy8\")\nproject = rf.workspace(\"huiiii\").project(\"hui-kuxax\")\nversion = project.version(5)\ndataset = version.download(\"sam2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd ./checkpoints && ./download_ckpts.sh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/sam2.git\n!wget -O /content/sam2/sam2/configs/train.yaml 'https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3'\n%cd ./sam2/\n!pip install -e .[dev] -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget -O /kaggle/working/sam2/sam2/configs/train.yaml 'https://drive.usercontent.google.com/download?id=11cmbxPPsYqFyWq87tmLgBAQ6OZgEhPG3'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/sam2/sam2/configs/train.yaml\n# @package _global_\n\nscratch:\n  resolution: 1024\n  train_batch_size: 1\n  num_train_workers: 4\n  num_frames: 1\n  max_num_objects: 3\n  base_lr: 5.0e-6\n  vision_lr: 3.0e-06\n  phases_per_epoch: 1\n  num_epochs: 40\n\ndataset:\n  # PATHS to Dataset\n  img_folder: /kaggle/working/sam2/hui-5/train # PATH to MOSE JPEGImages folder\n  gt_folder: /kaggle/working/sam2/hui-5/train  # PATH to MOSE Annotations folder\n  #file_list_txt: training/assets/MOSE_sample_train_list.txt # Optional PATH to filelist containing a subset of videos to be used for training\n  multiplier: 2\n\n# Video transforms\nvos:\n  train_transforms:\n    - _target_: training.dataset.transforms.ComposeAPI\n      transforms:\n        - _target_: training.dataset.transforms.RandomHorizontalFlip\n          consistent_transform: True\n        - _target_: training.dataset.transforms.RandomAffine\n          degrees: 25\n          shear: 20\n          image_interpolation: bilinear\n          consistent_transform: True\n        - _target_: training.dataset.transforms.RandomResizeAPI\n          sizes: ${scratch.resolution}\n          square: true\n          consistent_transform: True\n        - _target_: training.dataset.transforms.ColorJitter\n          consistent_transform: True\n          brightness: 0.1\n          contrast: 0.03\n          saturation: 0.03\n          hue: null\n        - _target_: training.dataset.transforms.RandomGrayscale\n          p: 0.05\n          consistent_transform: True\n        - _target_: training.dataset.transforms.ColorJitter\n          consistent_transform: False\n          brightness: 0.1\n          contrast: 0.05\n          saturation: 0.05\n          hue: null\n        - _target_: training.dataset.transforms.ToTensorAPI\n        - _target_: training.dataset.transforms.NormalizeAPI\n          mean: [0.485, 0.456, 0.406]\n          std: [0.229, 0.224, 0.225]\n\ntrainer:\n  _target_: training.trainer.Trainer\n  mode: train_only\n  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n  accelerator: cuda\n  seed_value: 123\n\n  model:\n    _target_: training.model.sam2.SAM2Train\n    image_encoder:\n      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n      scalp: 1\n      trunk:\n        _target_: sam2.modeling.backbones.hieradet.Hiera\n        embed_dim: 112\n        num_heads: 2\n        drop_path_rate: 0.1\n      neck:\n        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 256\n          normalize: true\n          scale: null\n          temperature: 10000\n        d_model: 256\n        backbone_channel_list: [896, 448, 224, 112]\n        fpn_top_down_levels: [2, 3]  # output level 0 and 1 directly use the backbone features\n        fpn_interp_model: nearest\n\n    memory_attention:\n      _target_: sam2.modeling.memory_attention.MemoryAttention\n      d_model: 256\n      pos_enc_at_input: true\n      layer:\n        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n        activation: relu\n        dim_feedforward: 2048\n        dropout: 0.1\n        pos_enc_at_attn: false\n        self_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes: [32, 32]\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n        d_model: 256\n        pos_enc_at_cross_attn_keys: true\n        pos_enc_at_cross_attn_queries: false\n        cross_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes: [32, 32]\n          rope_k_repeat: True\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n          kv_in_dim: 64\n      num_layers: 4\n\n    memory_encoder:\n        _target_: sam2.modeling.memory_encoder.MemoryEncoder\n        out_dim: 64\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 64\n          normalize: true\n          scale: null\n          temperature: 10000\n        mask_downsampler:\n          _target_: sam2.modeling.memory_encoder.MaskDownSampler\n          kernel_size: 3\n          stride: 2\n          padding: 1\n        fuser:\n          _target_: sam2.modeling.memory_encoder.Fuser\n          layer:\n            _target_: sam2.modeling.memory_encoder.CXBlock\n            dim: 256\n            kernel_size: 7\n            padding: 3\n            layer_scale_init_value: 1e-6\n            use_dwconv: True  # depth-wise convs\n          num_layers: 2\n\n    num_maskmem: 7\n    image_size: ${scratch.resolution}\n    # apply scaled sigmoid on mask logits for memory encoder, and directly feed input mask as output mask\n    sigmoid_scale_for_mem_enc: 20.0\n    sigmoid_bias_for_mem_enc: -10.0\n    use_mask_input_as_output_without_sam: true\n    # Memory\n    directly_add_no_mem_embed: true\n    no_obj_embed_spatial: true\n    # use high-resolution feature map in the SAM mask decoder\n    use_high_res_features_in_sam: true\n    # output 3 masks on the first click on initial conditioning frames\n    multimask_output_in_sam: true\n    # SAM heads\n    iou_prediction_use_sigmoid: True\n    # cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder\n    use_obj_ptrs_in_encoder: true\n    add_tpos_enc_to_obj_ptrs: true\n    proj_tpos_enc_in_obj_ptrs: true\n    use_signed_tpos_enc_to_obj_ptrs: true\n    only_obj_ptrs_in_the_past_for_eval: true\n    # object occlusion prediction\n    pred_obj_scores: true\n    pred_obj_scores_mlp: true\n    fixed_no_obj_ptr: true\n    # multimask tracking settings\n    multimask_output_for_tracking: true\n    use_multimask_token_for_obj_ptr: true\n    multimask_min_pt_num: 0\n    multimask_max_pt_num: 1\n    use_mlp_for_obj_ptr_proj: true\n    # Compilation flag\n    # compile_image_encoder: False\n\n    ####### Training specific params #######\n    # box/point input and corrections\n    prob_to_use_pt_input_for_train: 0.5\n    prob_to_use_pt_input_for_eval: 0.0\n    prob_to_use_box_input_for_train: 0.5  # 0.5*0.5 = 0.25 prob to use box instead of points\n    prob_to_use_box_input_for_eval: 0.0\n    prob_to_sample_from_gt_for_train: 0.1  # with a small prob, sampling correction points from GT mask instead of prediction errors\n    num_frames_to_correct_for_train: 2  # iteratively sample on random 1~2 frames (always include the first frame)\n    num_frames_to_correct_for_eval: 1  # only iteratively sample on first frame\n    rand_frames_to_correct_for_train: True  # random #init-cond-frame ~ 2\n    add_all_frames_to_correct_as_cond: True  # when a frame receives a correction click, it becomes a conditioning frame (even if it's not initially a conditioning frame)\n    # maximum 2 initial conditioning frames\n    num_init_cond_frames_for_train: 2\n    rand_init_cond_frames_for_train: True  # random 1~2\n    num_correction_pt_per_frame: 7\n    use_act_ckpt_iterative_pt_sampling: false\n    \n\n    \n    num_init_cond_frames_for_eval: 1  # only mask on the first frame\n    forward_backbone_per_frame_for_eval: True\n    \n\n  data:\n    train:\n      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n      phases_per_epoch: ${scratch.phases_per_epoch}\n      batch_sizes:\n        - ${scratch.train_batch_size}\n\n      datasets:\n        - _target_: training.dataset.vos_dataset.VOSDataset\n          transforms: ${vos.train_transforms}\n          training: true\n          video_dataset:\n            _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n            img_folder: ${dataset.img_folder}\n            gt_folder: ${dataset.gt_folder}\n            # file_list_txt: ${dataset.file_list_txt}\n          multiplier: ${dataset.multiplier}\n          sampler:\n            _target_: training.dataset.vos_sampler.RandomUniformSampler\n            num_frames: 1\n            max_num_objects: ${scratch.max_num_objects}\n      shuffle: True\n      num_workers: ${scratch.num_train_workers}\n      pin_memory: True\n      drop_last: True\n      collate_fn:\n        _target_: training.utils.data_utils.collate_fn\n        _partial_: true\n        dict_key: all\n\n  optim:\n    amp:\n      enabled: True\n      amp_dtype: bfloat16\n\n    optimizer:\n      _target_: torch.optim.AdamW\n\n    gradient_clip:\n      _target_: training.optimizer.GradientClipper\n      max_norm: 0.1\n      norm_type: 2\n\n    param_group_modifiers:\n      - _target_: training.optimizer.layer_decay_param_modifier\n        _partial_: True\n        layer_decay_value: 0.9\n        apply_to: 'image_encoder.trunk'\n        overrides:\n          - pattern: '*pos_embed*'\n            value: 1.0\n\n    options:\n      lr:\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.CosineParamScheduler\n            start_value: ${scratch.base_lr}\n            end_value: ${divide:${scratch.base_lr},10}\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.CosineParamScheduler\n            start_value: ${scratch.vision_lr}\n            end_value: ${divide:${scratch.vision_lr},10}\n          param_names:\n            - 'image_encoder.*'\n      weight_decay:\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n            value: 0.1\n        - scheduler:\n            _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n            value: 0.0\n          param_names:\n            - '*bias*'\n          module_cls_names: ['torch.nn.LayerNorm']\n\n  loss:\n    all:\n      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n      weight_dict:\n        loss_mask: 20\n        loss_dice: 1\n        loss_iou: 1\n        loss_class: 1\n      supervise_all_iou: true\n      iou_use_l1_loss: true\n      pred_obj_scores: true\n      focal_gamma_obj_score: 0.0\n      focal_alpha_obj_score: -1.0\n\n  distributed:\n    backend: nccl\n    find_unused_parameters: True\n\n  logging:\n    tensorboard_writer:\n      _target_: training.utils.logger.make_tensorboard_logger\n      log_dir:  ${launcher.experiment_log_dir}/tensorboard\n      flush_secs: 120\n      should_log: True\n    log_dir: ${launcher.experiment_log_dir}/logs\n    log_freq: 10\n\n  # initialize from a SAM 2 checkpoint\n  checkpoint:\n    save_dir: ${launcher.experiment_log_dir}/checkpoints\n    save_freq: 100 # 0 only last checkpoint is saved.\n    model_weight_initializer:\n      _partial_: True\n      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n      strict: True\n      ignore_unexpected_keys: null\n      ignore_missing_keys: null\n\n      state_dict:\n        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt # PATH to SAM 2.1 checkpoint\n        ckpt_state_dict_keys: ['model']\n\nlauncher:\n  num_nodes: 1\n  gpus_per_node: 8\n  experiment_log_dir: null # Path to log directory, defaults to ./sam2_logs/${config_name}\n\n# SLURM args if running on a cluster\nsubmitit:\n  partition: null\n  account: null\n  qos: null\n  cpus_per_task: 10\n  use_cluster: false\n  timeout_hour: 24\n  name: null\n  port_range: [10000, 65000]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T20:29:55.526210Z","iopub.execute_input":"2025-01-24T20:29:55.526606Z","iopub.status.idle":"2025-01-24T20:29:55.534735Z","shell.execute_reply.started":"2025-01-24T20:29:55.526570Z","shell.execute_reply":"2025-01-24T20:29:55.533506Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/sam2/sam2/configs/train.yaml\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd ./sam2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T20:30:00.752308Z","iopub.execute_input":"2025-01-24T20:30:00.752680Z","iopub.status.idle":"2025-01-24T20:30:00.759247Z","shell.execute_reply.started":"2025-01-24T20:30:00.752646Z","shell.execute_reply":"2025-01-24T20:30:00.758317Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/sam2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"os.listdir('/kaggle/working/sam2/hui-5/train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Script to rename roboflow filenames to something SAM 2.1 compatible.\n# Maybe it is possible to remove this step tweaking sam2/sam2/configs/train.yaml.\nimport os\nimport re\n\nFOLDER = \"/kaggle/working/sam2/hui-5/train\"\n\nfor filename in os.listdir(FOLDER):\n    # Replace all except last dot with underscore\n    new_filename = filename.replace(\".\", \"_\", filename.count(\".\") - 1)\n    if not re.search(r\"_\\d+\\.\\w+$\", new_filename):\n        # Add an int to the end of base name\n        new_filename = new_filename.replace(\".\", \"_1.\")\n    os.rename(os.path.join(FOLDER, filename), os.path.join(FOLDER, new_filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python training/train.py -c 'configs/train.yaml' --use-cluster 0 --num-gpus 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T20:30:02.034273Z","iopub.execute_input":"2025-01-24T20:30:02.034640Z"}},"outputs":[{"name":"stdout","text":"###################### Train App Config ####################\nscratch:\n  resolution: 1024\n  train_batch_size: 1\n  num_train_workers: 4\n  num_frames: 1\n  max_num_objects: 3\n  base_lr: 5.0e-06\n  vision_lr: 3.0e-06\n  phases_per_epoch: 1\n  num_epochs: 40\ndataset:\n  img_folder: /kaggle/working/sam2/hui-5/train\n  gt_folder: /kaggle/working/sam2/hui-5/train\n  multiplier: 2\nvos:\n  train_transforms:\n  - _target_: training.dataset.transforms.ComposeAPI\n    transforms:\n    - _target_: training.dataset.transforms.RandomHorizontalFlip\n      consistent_transform: true\n    - _target_: training.dataset.transforms.RandomAffine\n      degrees: 25\n      shear: 20\n      image_interpolation: bilinear\n      consistent_transform: true\n    - _target_: training.dataset.transforms.RandomResizeAPI\n      sizes: ${scratch.resolution}\n      square: true\n      consistent_transform: true\n    - _target_: training.dataset.transforms.ColorJitter\n      consistent_transform: true\n      brightness: 0.1\n      contrast: 0.03\n      saturation: 0.03\n      hue: null\n    - _target_: training.dataset.transforms.RandomGrayscale\n      p: 0.05\n      consistent_transform: true\n    - _target_: training.dataset.transforms.ColorJitter\n      consistent_transform: false\n      brightness: 0.1\n      contrast: 0.05\n      saturation: 0.05\n      hue: null\n    - _target_: training.dataset.transforms.ToTensorAPI\n    - _target_: training.dataset.transforms.NormalizeAPI\n      mean:\n      - 0.485\n      - 0.456\n      - 0.406\n      std:\n      - 0.229\n      - 0.224\n      - 0.225\ntrainer:\n  _target_: training.trainer.Trainer\n  mode: train_only\n  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}\n  accelerator: cuda\n  seed_value: 123\n  model:\n    _target_: training.model.sam2.SAM2Train\n    image_encoder:\n      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n      scalp: 1\n      trunk:\n        _target_: sam2.modeling.backbones.hieradet.Hiera\n        embed_dim: 112\n        num_heads: 2\n        drop_path_rate: 0.1\n      neck:\n        _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n        position_encoding:\n          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n          num_pos_feats: 256\n          normalize: true\n          scale: null\n          temperature: 10000\n        d_model: 256\n        backbone_channel_list:\n        - 896\n        - 448\n        - 224\n        - 112\n        fpn_top_down_levels:\n        - 2\n        - 3\n        fpn_interp_model: nearest\n    memory_attention:\n      _target_: sam2.modeling.memory_attention.MemoryAttention\n      d_model: 256\n      pos_enc_at_input: true\n      layer:\n        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n        activation: relu\n        dim_feedforward: 2048\n        dropout: 0.1\n        pos_enc_at_attn: false\n        self_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes:\n          - 32\n          - 32\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n        d_model: 256\n        pos_enc_at_cross_attn_keys: true\n        pos_enc_at_cross_attn_queries: false\n        cross_attention:\n          _target_: sam2.modeling.sam.transformer.RoPEAttention\n          rope_theta: 10000.0\n          feat_sizes:\n          - 32\n          - 32\n          rope_k_repeat: true\n          embedding_dim: 256\n          num_heads: 1\n          downsample_rate: 1\n          dropout: 0.1\n          kv_in_dim: 64\n      num_layers: 4\n    memory_encoder:\n      _target_: sam2.modeling.memory_encoder.MemoryEncoder\n      out_dim: 64\n      position_encoding:\n        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n        num_pos_feats: 64\n        normalize: true\n        scale: null\n        temperature: 10000\n      mask_downsampler:\n        _target_: sam2.modeling.memory_encoder.MaskDownSampler\n        kernel_size: 3\n        stride: 2\n        padding: 1\n      fuser:\n        _target_: sam2.modeling.memory_encoder.Fuser\n        layer:\n          _target_: sam2.modeling.memory_encoder.CXBlock\n          dim: 256\n          kernel_size: 7\n          padding: 3\n          layer_scale_init_value: 1.0e-06\n          use_dwconv: true\n        num_layers: 2\n    num_maskmem: 7\n    image_size: ${scratch.resolution}\n    sigmoid_scale_for_mem_enc: 20.0\n    sigmoid_bias_for_mem_enc: -10.0\n    use_mask_input_as_output_without_sam: true\n    directly_add_no_mem_embed: true\n    no_obj_embed_spatial: true\n    use_high_res_features_in_sam: true\n    multimask_output_in_sam: true\n    iou_prediction_use_sigmoid: true\n    use_obj_ptrs_in_encoder: true\n    add_tpos_enc_to_obj_ptrs: true\n    proj_tpos_enc_in_obj_ptrs: true\n    use_signed_tpos_enc_to_obj_ptrs: true\n    only_obj_ptrs_in_the_past_for_eval: true\n    pred_obj_scores: true\n    pred_obj_scores_mlp: true\n    fixed_no_obj_ptr: true\n    multimask_output_for_tracking: true\n    use_multimask_token_for_obj_ptr: true\n    multimask_min_pt_num: 0\n    multimask_max_pt_num: 1\n    use_mlp_for_obj_ptr_proj: true\n    prob_to_use_pt_input_for_train: 0.5\n    prob_to_use_pt_input_for_eval: 0.0\n    prob_to_use_box_input_for_train: 0.5\n    prob_to_use_box_input_for_eval: 0.0\n    prob_to_sample_from_gt_for_train: 0.1\n    num_frames_to_correct_for_train: 2\n    num_frames_to_correct_for_eval: 1\n    rand_frames_to_correct_for_train: true\n    add_all_frames_to_correct_as_cond: true\n    num_init_cond_frames_for_train: 2\n    rand_init_cond_frames_for_train: true\n    num_correction_pt_per_frame: 7\n    use_act_ckpt_iterative_pt_sampling: false\n    num_init_cond_frames_for_eval: 1\n    forward_backbone_per_frame_for_eval: true\n  data:\n    train:\n      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset\n      phases_per_epoch: ${scratch.phases_per_epoch}\n      batch_sizes:\n      - ${scratch.train_batch_size}\n      datasets:\n      - _target_: training.dataset.vos_dataset.VOSDataset\n        transforms: ${vos.train_transforms}\n        training: true\n        video_dataset:\n          _target_: training.dataset.vos_raw_dataset.SA1BRawDataset\n          img_folder: ${dataset.img_folder}\n          gt_folder: ${dataset.gt_folder}\n        multiplier: ${dataset.multiplier}\n        sampler:\n          _target_: training.dataset.vos_sampler.RandomUniformSampler\n          num_frames: 1\n          max_num_objects: ${scratch.max_num_objects}\n      shuffle: true\n      num_workers: ${scratch.num_train_workers}\n      pin_memory: true\n      drop_last: true\n      collate_fn:\n        _target_: training.utils.data_utils.collate_fn\n        _partial_: true\n        dict_key: all\n  optim:\n    amp:\n      enabled: true\n      amp_dtype: bfloat16\n    optimizer:\n      _target_: torch.optim.AdamW\n    gradient_clip:\n      _target_: training.optimizer.GradientClipper\n      max_norm: 0.1\n      norm_type: 2\n    param_group_modifiers:\n    - _target_: training.optimizer.layer_decay_param_modifier\n      _partial_: true\n      layer_decay_value: 0.9\n      apply_to: image_encoder.trunk\n      overrides:\n      - pattern: '*pos_embed*'\n        value: 1.0\n    options:\n      lr:\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n          start_value: ${scratch.base_lr}\n          end_value: ${divide:${scratch.base_lr},10}\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.CosineParamScheduler\n          start_value: ${scratch.vision_lr}\n          end_value: ${divide:${scratch.vision_lr},10}\n        param_names:\n        - image_encoder.*\n      weight_decay:\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n          value: 0.1\n      - scheduler:\n          _target_: fvcore.common.param_scheduler.ConstantParamScheduler\n          value: 0.0\n        param_names:\n        - '*bias*'\n        module_cls_names:\n        - torch.nn.LayerNorm\n  loss:\n    all:\n      _target_: training.loss_fns.MultiStepMultiMasksAndIous\n      weight_dict:\n        loss_mask: 20\n        loss_dice: 1\n        loss_iou: 1\n        loss_class: 1\n      supervise_all_iou: true\n      iou_use_l1_loss: true\n      pred_obj_scores: true\n      focal_gamma_obj_score: 0.0\n      focal_alpha_obj_score: -1.0\n  distributed:\n    backend: nccl\n    find_unused_parameters: true\n  logging:\n    tensorboard_writer:\n      _target_: training.utils.logger.make_tensorboard_logger\n      log_dir: ${launcher.experiment_log_dir}/tensorboard\n      flush_secs: 120\n      should_log: true\n    log_dir: ${launcher.experiment_log_dir}/logs\n    log_freq: 10\n  checkpoint:\n    save_dir: ${launcher.experiment_log_dir}/checkpoints\n    save_freq: 100\n    model_weight_initializer:\n      _partial_: true\n      _target_: training.utils.checkpoint_utils.load_state_dict_into_model\n      strict: true\n      ignore_unexpected_keys: null\n      ignore_missing_keys: null\n      state_dict:\n        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels\n        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt\n        ckpt_state_dict_keys:\n        - model\nlauncher:\n  num_nodes: 1\n  gpus_per_node: 8\n  experiment_log_dir: /kaggle/working/sam2/sam2_logs/configs/train.yaml\nsubmitit:\n  partition: null\n  account: null\n  qos: null\n  cpus_per_task: 10\n  use_cluster: false\n  timeout_hour: 24\n  name: null\n  port_range:\n  - 10000\n  - 65000\n\n############################################################\n2025-01-24 20:30:05.736694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-24 20:30:05.758531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-24 20:30:05.766441: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO 2025-01-24 20:30:08,355 train_utils.py: 108: MACHINE SEED: 4920\nINFO 2025-01-24 20:30:08,357 train_utils.py: 154: Logging ENV_VARIABLES\nINFO 2025-01-24 20:30:08,357 train_utils.py: 155: BUILD_DATE=20250109-122511\nCLICOLOR=1\nCLOUDSDK_CONFIG=/content/.config\nCLOUDSDK_PYTHON=python3\nCOLAB_DEBUG_ADAPTER_MUX_PATH=/usr/local/bin/dap_multiplexer\nCOLAB_FILE_HANDLER_ADDR=localhost:3453\nCOLAB_HUMAN_READABLE_NODE_LOGS=1\nCOLAB_JUPYTER_ALLOW_ORIGIN_PAT=https://colab\\.(sandbox|research)\\.google\\.com\nCOLAB_JUPYTER_IP=127.0.0.1\nCOLAB_KERNEL_MANAGER_PROXY_PORT=6000\nCOLAB_LANGUAGE_SERVER_PROXY=/usr/colab/bin/language_service\nCOLAB_RELEASE_TAG=release-colab_20241217-060132_RC00\nCOLAB_TPU_1VM=\nCOLAB_WARMUP_DEFAULTS=1\nCOLUMNS=100\nCUDA_HOME=/usr/local/cuda\nCUDA_MAJOR_VERSION=12\nCUDA_MINOR_VERSION=2\nCUDA_MODULE_LOADING=LAZY\nCUDA_VERSION=12.2.2\nDEBIAN_FRONTEND=noninteractive\nENV=/root/.bashrc\nGIT_COMMIT=55c18210abfa1f3e2b75e9ffd25950ebfbbb6694\nGIT_PAGER=cat\nHOME=/root\nHOSTNAME=9c98b326d5c2\nHYDRA_FULL_ERROR=1\nJPY_PARENT_PID=1\nKAGGLE_API_V1_TOKEN=/etc/secrets/kaggle/api-v1-token\nKAGGLE_CONTAINER_NAME=kaggle_P3ejfWfpOCGVjBNVVvs1rIa7Gi3ErbHVOqYW0TXkuMQ-219115988-webtier\nKAGGLE_DATA_PROXY_PROJECT=kaggle-161607\nKAGGLE_DATA_PROXY_TOKEN=eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.IWOHgnUOO9JpGNEKadpHhvJaCMJHDr20ur5-J7QJk2jQMBQPxVMVug.Sj7Wa4kbXLqvFAR_TDMsCQ.IY3jDy4-5JTGCOiKu0OGSxY-xOb_-3MQTXy2ZHWXy5Tj--TBiuJWgGk0mKaJit8rPJgGPhEhm4V_WhclQ2hS6LIZEv8RinJIOOoy8qQA_MlesTlf9_53l-yHG1j251MLNAkgTc_rPEuVh9N0kMVb_C3VNkPY_wYtUFXA6NxTggTL8G-KftrO3BC3N33q0ZStscLErh9V7Uip7Mw7gYPzXKWwOdMDkZX-7ZXx8e1kdsepKq--VSnWoXmSCgPxqdbBlYr_22drcHIYM8_mrMAxC2C_RAA1Nktn1pKXG5fA_3Pt1SKThlkue7t_zb3J7Lt9.P_DjwqX2WpPVvbkKarg3wQ\nKAGGLE_DATA_PROXY_URL=https://dp.kaggle.net\nKAGGLE_DOCKER_IMAGE=gcr.io/kaggle-gpu-images/python@sha256:57cb636a65386fd6c74fc9969211623034c487f7d483f9cd2c8456ebe2619345\nKAGGLE_GCP_ZONE=europe-west4-a\nKAGGLE_GRPC_DATA_PROXY_URL=dp.kaggle.net:443\nKAGGLE_KERNEL_INTEGRATIONS=\nKAGGLE_KERNEL_RUN_TYPE=Interactive\nKAGGLE_URL_BASE=https://www.kaggle.com\nKAGGLE_USER_SECRETS_TOKEN=eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Zn0Wb6nCWIfn01GD1DU4GA.xk7Dm97Vsu17T1D1ssofzo_5Ee-U86KqpNhujyNobnT9lmWtgipg3x74ph46gD9IY4Ytb5QOtk8yur8mjv7o3RvGhMalRFYeNgiYE6JzGcBjQy5KF3oi8G-DDOm2Z9FNnQ7cCUVOIZ8Y6cWNeayNiw.e7NDC-PViztfmudjY0IGYQ\nKMP_LISTEN_PORT=6000\nKMP_TARGET_PORT=9000\nLANG=en_US.UTF-8\nLANGUAGE=en_US\nLAST_FORCED_REBUILD=20241216\nLC_ALL=en_US.UTF-8\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\nLIBRARY_PATH=/usr/local/cuda/lib64/stubs\nLOCAL_RANK=0\nMASTER_ADDR=localhost\nMASTER_PORT=25475\nMKL_THREADING_LAYER=GNU\nMPLBACKEND=module://ipykernel.pylab.backend_inline\nNCCL_VERSION=2.19.3-1\nNVARCH=x86_64\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\nNVIDIA_PRODUCT_NAME=CUDA\nNVIDIA_REQUIRE_CUDA=cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526\nNVIDIA_VISIBLE_DEVICES=all\nNV_CUDA_COMPAT_PACKAGE=cuda-compat-12-2\nNV_CUDA_CUDART_DEV_VERSION=12.2.140-1\nNV_CUDA_CUDART_VERSION=12.2.140-1\nNV_CUDA_LIB_VERSION=12.2.2-1\nNV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-2=12.2.2-1\nNV_CUDA_NSIGHT_COMPUTE_VERSION=12.2.2-1\nNV_CUDNN_PACKAGE=libcudnn8=8.9.6.50-1+cuda12.2\nNV_CUDNN_PACKAGE_DEV=libcudnn8-dev=8.9.6.50-1+cuda12.2\nNV_CUDNN_PACKAGE_NAME=libcudnn8\nNV_CUDNN_VERSION=8.9.6.50\nNV_LIBCUBLAS_DEV_PACKAGE=libcublas-dev-12-2=12.2.5.6-1\nNV_LIBCUBLAS_DEV_PACKAGE_NAME=libcublas-dev-12-2\nNV_LIBCUBLAS_DEV_VERSION=12.2.5.6-1\nNV_LIBCUBLAS_PACKAGE=libcublas-12-2=12.2.5.6-1\nNV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-2\nNV_LIBCUBLAS_VERSION=12.2.5.6-1\nNV_LIBCUSPARSE_DEV_VERSION=12.1.2.141-1\nNV_LIBCUSPARSE_VERSION=12.1.2.141-1\nNV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\nNV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\nNV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\nNV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\nNV_LIBNCCL_PACKAGE_NAME=libnccl2\nNV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\nNV_LIBNPP_DEV_PACKAGE=libnpp-dev-12-2=12.2.1.4-1\nNV_LIBNPP_DEV_VERSION=12.2.1.4-1\nNV_LIBNPP_PACKAGE=libnpp-12-2=12.2.1.4-1\nNV_LIBNPP_VERSION=12.2.1.4-1\nNV_NVML_DEV_VERSION=12.2.140-1\nNV_NVPROF_DEV_PACKAGE=cuda-nvprof-12-2=12.2.142-1\nNV_NVPROF_VERSION=12.2.142-1\nNV_NVTX_VERSION=12.2.140-1\nPAGER=cat\nPATH=~/.local/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\nPWD=/kaggle/working/sam2\nPYTHONPATH=/kaggle/lib/kagglegym:/kaggle/lib\nPYTHONUSERBASE=/root/.local\nRANK=0\nSHELL=/bin/bash\nSHLVL=0\nTCLLIBPATH=/usr/share/tcltk/tcllib1.20\nTERM=xterm-color\nTESSERACT_PATH=/usr/bin/tesseract\nTF2_BEHAVIOR=1\nTF_CPP_MIN_LOG_LEVEL=2\nTF_FORCE_GPU_ALLOW_GROWTH=true\nTORCH_NCCL_ASYNC_ERROR_HANDLING=1\nTPU_ML_PLATFORM=Tensorflow\nTPU_ML_PLATFORM_VERSION=2.17.1\nVM_GCE_METADATA_HOST=169.254.169.254\nWORLD_SIZE=1\n_=/usr/local/bin/python\n\nINFO 2025-01-24 20:30:08,357 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.\nINFO 2025-01-24 20:30:08,359 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /kaggle/working/sam2/sam2_logs/configs/train.yaml/tensorboard\nINFO 2025-01-24 20:30:09,272 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5\nINFO 2025-01-24 20:30:09,276 trainer.py:1059: ====================\nINFO 2025-01-24 20:30:09,276 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>\nINFO 2025-01-24 20:30:09,280 trainer.py:1061: Model is SAM2Train(\n  (image_encoder): ImageEncoder(\n    (trunk): Hiera(\n      (patch_embed): PatchEmbed(\n        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n      )\n      (blocks): ModuleList(\n        (0): MultiScaleBlock(\n          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=112, out_features=336, bias=True)\n            (proj): Linear(in_features=112, out_features=112, bias=True)\n          )\n          (drop_path): Identity()\n          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=112, out_features=448, bias=True)\n              (1): Linear(in_features=448, out_features=112, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (1): MultiScaleBlock(\n          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=112, out_features=336, bias=True)\n            (proj): Linear(in_features=112, out_features=112, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=112, out_features=448, bias=True)\n              (1): Linear(in_features=448, out_features=112, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (2): MultiScaleBlock(\n          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)\n          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n          (attn): MultiScaleAttention(\n            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n            (qkv): Linear(in_features=112, out_features=672, bias=True)\n            (proj): Linear(in_features=224, out_features=224, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=224, out_features=896, bias=True)\n              (1): Linear(in_features=896, out_features=224, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n          (proj): Linear(in_features=112, out_features=224, bias=True)\n        )\n        (3-4): 2 x MultiScaleBlock(\n          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=224, out_features=672, bias=True)\n            (proj): Linear(in_features=224, out_features=224, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=224, out_features=896, bias=True)\n              (1): Linear(in_features=896, out_features=224, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (5): MultiScaleBlock(\n          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)\n          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n          (attn): MultiScaleAttention(\n            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n            (qkv): Linear(in_features=224, out_features=1344, bias=True)\n            (proj): Linear(in_features=448, out_features=448, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=448, out_features=1792, bias=True)\n              (1): Linear(in_features=1792, out_features=448, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n          (proj): Linear(in_features=224, out_features=448, bias=True)\n        )\n        (6-20): 15 x MultiScaleBlock(\n          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=448, out_features=1344, bias=True)\n            (proj): Linear(in_features=448, out_features=448, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=448, out_features=1792, bias=True)\n              (1): Linear(in_features=1792, out_features=448, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n        (21): MultiScaleBlock(\n          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)\n          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n          (attn): MultiScaleAttention(\n            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n            (qkv): Linear(in_features=448, out_features=2688, bias=True)\n            (proj): Linear(in_features=896, out_features=896, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=896, out_features=3584, bias=True)\n              (1): Linear(in_features=3584, out_features=896, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n          (proj): Linear(in_features=448, out_features=896, bias=True)\n        )\n        (22-23): 2 x MultiScaleBlock(\n          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n          (attn): MultiScaleAttention(\n            (qkv): Linear(in_features=896, out_features=2688, bias=True)\n            (proj): Linear(in_features=896, out_features=896, bias=True)\n          )\n          (drop_path): DropPath()\n          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=896, out_features=3584, bias=True)\n              (1): Linear(in_features=3584, out_features=896, bias=True)\n            )\n            (act): GELU(approximate='none')\n          )\n        )\n      )\n    )\n    (neck): FpnNeck(\n      (position_encoding): PositionEmbeddingSine()\n      (convs): ModuleList(\n        (0): Sequential(\n          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Sequential(\n          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Sequential(\n          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Sequential(\n          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n    )\n  )\n  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n  (memory_attention): MemoryAttention(\n    (layers): ModuleList(\n      (0-3): 4 x MemoryAttentionLayer(\n        (self_attn): RoPEAttention(\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n        )\n        (cross_attn_image): RoPEAttention(\n          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n        )\n        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (memory_encoder): MemoryEncoder(\n    (mask_downsampler): MaskDownSampler(\n      (encoder): Sequential(\n        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (1): LayerNorm2d()\n        (2): GELU(approximate='none')\n        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (4): LayerNorm2d()\n        (5): GELU(approximate='none')\n        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (7): LayerNorm2d()\n        (8): GELU(approximate='none')\n        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        (10): LayerNorm2d()\n        (11): GELU(approximate='none')\n        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      )\n    )\n    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fuser): Fuser(\n      (proj): Identity()\n      (layers): ModuleList(\n        (0-1): 2 x CXBlock(\n          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n          (norm): LayerNorm2d()\n          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n          (act): GELU(approximate='none')\n          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n          (drop_path): Identity()\n        )\n      )\n    )\n    (position_encoding): PositionEmbeddingSine()\n    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (sam_prompt_encoder): PromptEncoder(\n    (pe_layer): PositionEmbeddingRandom()\n    (point_embeddings): ModuleList(\n      (0-3): 4 x Embedding(1, 256)\n    )\n    (not_a_point_embed): Embedding(1, 256)\n    (mask_downscaling): Sequential(\n      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n      (4): LayerNorm2d()\n      (5): GELU(approximate='none')\n      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (no_mask_embed): Embedding(1, 256)\n  )\n  (sam_mask_decoder): MaskDecoder(\n    (transformer): TwoWayTransformer(\n      (layers): ModuleList(\n        (0-1): 2 x TwoWayAttentionBlock(\n          (self_attn): Attention(\n            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n          )\n          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_token_to_image): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (mlp): MLP(\n            (layers): ModuleList(\n              (0): Linear(in_features=256, out_features=2048, bias=True)\n              (1): Linear(in_features=2048, out_features=256, bias=True)\n            )\n            (act): ReLU()\n          )\n          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n          (cross_attn_image_to_token): Attention(\n            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n          )\n        )\n      )\n      (final_attn_token_to_image): Attention(\n        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n      )\n      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n    )\n    (iou_token): Embedding(1, 256)\n    (mask_tokens): Embedding(4, 256)\n    (obj_score_token): Embedding(1, 256)\n    (output_upscaling): Sequential(\n      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n      (1): LayerNorm2d()\n      (2): GELU(approximate='none')\n      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n      (4): GELU(approximate='none')\n    )\n    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n    (output_hypernetworks_mlps): ModuleList(\n      (0-3): 4 x MLP(\n        (layers): ModuleList(\n          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n          (2): Linear(in_features=256, out_features=32, bias=True)\n        )\n        (act): ReLU()\n      )\n    )\n    (iou_prediction_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=4, bias=True)\n      )\n      (act): ReLU()\n    )\n    (pred_obj_score_head): MLP(\n      (layers): ModuleList(\n        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n        (2): Linear(in_features=256, out_features=1, bias=True)\n      )\n      (act): ReLU()\n    )\n  )\n  (obj_ptr_proj): MLP(\n    (layers): ModuleList(\n      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n    )\n    (act): ReLU()\n  )\n  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n)\nINFO 2025-01-24 20:30:09,281 trainer.py:1062: \tTotal parameters 80.9 M\nINFO 2025-01-24 20:30:09,281 trainer.py:1063: \tTrainable parameters 80.9 M\nINFO 2025-01-24 20:30:09,281 trainer.py:1066: \tNon-Trainable parameters 0  \nINFO 2025-01-24 20:30:09,281 trainer.py:1069: ====================\nINFO 2025-01-24 20:30:09,285 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.\nINFO 2025-01-24 20:30:09,285 trainer.py: 314: Moving components to device cuda:0 and local rank 0.\nINFO 2025-01-24 20:30:09,403 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.\nINFO 2025-01-24 20:30:09,417 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.proj.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.norm2.bias'}\nINFO 2025-01-24 20:30:09,421 optimizer.py: 248: Matches for param_name [*bias*]: {'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'obj_ptr_tpos_proj.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'memory_attention.layers.2.norm1.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'memory_attention.layers.1.linear2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'memory_attention.layers.0.linear2.bias', 'memory_encoder.pix_feat_proj.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'memory_attention.layers.0.norm1.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'memory_attention.layers.0.linear1.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.neck.convs.0.conv.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'memory_attention.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'memory_encoder.out_proj.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.3.linear1.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'memory_attention.layers.2.norm3.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'mask_downsample.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'sam_mask_decoder.conv_s0.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'obj_ptr_proj.layers.0.bias', 'image_encoder.neck.convs.3.conv.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'memory_attention.layers.2.linear2.bias', 'sam_mask_decoder.conv_s1.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'sam_mask_decoder.output_upscaling.1.bias'}\nINFO 2025-01-24 20:30:09,422 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.2.norm3.weight', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'memory_attention.layers.0.norm2.weight', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'memory_attention.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.2.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'memory_attention.layers.3.norm1.weight', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.3.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight'} \nRaw dataset length = 16621\nINFO 2025-01-24 20:30:09,862 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]\n/kaggle/working/sam2/training/utils/checkpoint_utils.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f, map_location=map_location)\nINFO 2025-01-24 20:30:10,090 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': './checkpoints/sam2.1_hiera_base_plus.pt', 'ckpt_state_dict_keys': ['model']}}\n/kaggle/working/sam2/training/trainer.py:861: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(\nINFO 2025-01-24 20:30:25,366 train_utils.py: 271: Train Epoch: [0][    0/16621] | Batch Time: 14.94 (14.94) | Data Time: 13.20 (13.20) | Mem (GB): 7.00 (7.00/7.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 2.97e-01 (2.97e-01)\nINFO 2025-01-24 20:30:37,014 train_utils.py: 271: Train Epoch: [0][   10/16621] | Batch Time: 1.12 (2.42) | Data Time: 0.00 (1.20) | Mem (GB): 8.00 (8.00/9.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 9.90e-01 (5.54e-01)\nINFO 2025-01-24 20:30:48,330 train_utils.py: 271: Train Epoch: [0][   20/16621] | Batch Time: 1.10 (1.80) | Data Time: 0.00 (0.63) | Mem (GB): 8.00 (8.05/9.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 3.25e+00 (9.18e-01)\nINFO 2025-01-24 20:30:59,527 train_utils.py: 271: Train Epoch: [0][   30/16621] | Batch Time: 1.12 (1.58) | Data Time: 0.00 (0.43) | Mem (GB): 8.00 (8.03/9.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 4.58e-01 (1.32e+00)\nINFO 2025-01-24 20:31:10,955 train_utils.py: 271: Train Epoch: [0][   40/16621] | Batch Time: 1.13 (1.48) | Data Time: 0.00 (0.32) | Mem (GB): 8.00 (8.05/9.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 4.13e-01 (1.17e+00)\nINFO 2025-01-24 20:31:22,436 train_utils.py: 271: Train Epoch: [0][   50/16621] | Batch Time: 1.10 (1.41) | Data Time: 0.00 (0.26) | Mem (GB): 8.00 (8.08/9.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 7.31e-01 (1.08e+00)\nINFO 2025-01-24 20:31:33,749 train_utils.py: 271: Train Epoch: [0][   60/16621] | Batch Time: 1.10 (1.37) | Data Time: 0.00 (0.22) | Mem (GB): 8.00 (8.08/9.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 3.12e+00 (1.05e+00)\nINFO 2025-01-24 20:31:44,984 train_utils.py: 271: Train Epoch: [0][   70/16621] | Batch Time: 1.10 (1.33) | Data Time: 0.00 (0.19) | Mem (GB): 8.00 (8.07/9.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 3.51e+00 (1.04e+00)\nINFO 2025-01-24 20:31:56,234 train_utils.py: 271: Train Epoch: [0][   80/16621] | Batch Time: 1.13 (1.31) | Data Time: 0.00 (0.16) | Mem (GB): 8.00 (8.07/9.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 6.04e+00 (1.04e+00)\nINFO 2025-01-24 20:32:07,397 train_utils.py: 271: Train Epoch: [0][   90/16621] | Batch Time: 1.13 (1.29) | Data Time: 0.00 (0.15) | Mem (GB): 8.00 (8.07/9.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 8.47e-01 (1.01e+00)\nINFO 2025-01-24 20:32:18,802 train_utils.py: 271: Train Epoch: [0][  100/16621] | Batch Time: 1.14 (1.27) | Data Time: 0.00 (0.13) | Mem (GB): 8.00 (8.07/9.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 6.91e-01 (1.01e+00)\nINFO 2025-01-24 20:32:30,073 train_utils.py: 271: Train Epoch: [0][  110/16621] | Batch Time: 1.11 (1.26) | Data Time: 0.00 (0.12) | Mem (GB): 8.00 (8.07/9.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 2.74e+00 (1.05e+00)\nINFO 2025-01-24 20:32:41,486 train_utils.py: 271: Train Epoch: [0][  120/16621] | Batch Time: 1.11 (1.25) | Data Time: 0.00 (0.11) | Mem (GB): 8.00 (8.07/9.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 4.33e-01 (1.01e+00)\nINFO 2025-01-24 20:32:52,951 train_utils.py: 271: Train Epoch: [0][  130/16621] | Batch Time: 1.10 (1.24) | Data Time: 0.00 (0.10) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 4.72e-01 (1.03e+00)\nINFO 2025-01-24 20:33:04,149 train_utils.py: 271: Train Epoch: [0][  140/16621] | Batch Time: 1.12 (1.23) | Data Time: 0.00 (0.09) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 2.68e-01 (1.02e+00)\nINFO 2025-01-24 20:33:15,303 train_utils.py: 271: Train Epoch: [0][  150/16621] | Batch Time: 1.11 (1.22) | Data Time: 0.00 (0.09) | Mem (GB): 8.00 (8.07/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.02e+00 (1.01e+00)\nINFO 2025-01-24 20:33:26,740 train_utils.py: 271: Train Epoch: [0][  160/16621] | Batch Time: 1.12 (1.22) | Data Time: 0.00 (0.08) | Mem (GB): 8.00 (8.07/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.54e-01 (1.01e+00)\nINFO 2025-01-24 20:33:38,373 train_utils.py: 271: Train Epoch: [0][  170/16621] | Batch Time: 1.13 (1.22) | Data Time: 0.00 (0.08) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.81e-01 (9.94e-01)\nINFO 2025-01-24 20:33:49,464 train_utils.py: 271: Train Epoch: [0][  180/16621] | Batch Time: 1.10 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.82e+00 (9.86e-01)\nINFO 2025-01-24 20:34:00,754 train_utils.py: 271: Train Epoch: [0][  190/16621] | Batch Time: 1.13 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.70e-01 (9.79e-01)\nINFO 2025-01-24 20:34:11,950 train_utils.py: 271: Train Epoch: [0][  200/16621] | Batch Time: 1.11 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.69e-01 (9.63e-01)\nINFO 2025-01-24 20:34:23,178 train_utils.py: 271: Train Epoch: [0][  210/16621] | Batch Time: 1.10 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.91e-01 (9.81e-01)\nINFO 2025-01-24 20:34:35,156 train_utils.py: 271: Train Epoch: [0][  220/16621] | Batch Time: 1.13 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.19e+00 (9.62e-01)\nINFO 2025-01-24 20:34:46,297 train_utils.py: 271: Train Epoch: [0][  230/16621] | Batch Time: 1.14 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 2.84e-01 (9.48e-01)\nINFO 2025-01-24 20:34:58,142 train_utils.py: 271: Train Epoch: [0][  240/16621] | Batch Time: 1.28 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 6.51e-01 (9.72e-01)\nINFO 2025-01-24 20:35:09,289 train_utils.py: 271: Train Epoch: [0][  250/16621] | Batch Time: 1.11 (1.19) | Data Time: 0.00 (0.05) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 2.58e-01 (9.57e-01)\nINFO 2025-01-24 20:35:20,448 train_utils.py: 271: Train Epoch: [0][  260/16621] | Batch Time: 1.13 (1.19) | Data Time: 0.00 (0.05) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 7.09e-01 (9.56e-01)\nINFO 2025-01-24 20:35:31,571 train_utils.py: 271: Train Epoch: [0][  270/16621] | Batch Time: 1.11 (1.19) | Data Time: 0.00 (0.05) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 9.50e-01 (9.51e-01)\nINFO 2025-01-24 20:35:42,722 train_utils.py: 271: Train Epoch: [0][  280/16621] | Batch Time: 1.10 (1.18) | Data Time: 0.00 (0.05) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 1.06e+00 (9.38e-01)\nINFO 2025-01-24 20:35:53,932 train_utils.py: 271: Train Epoch: [0][  290/16621] | Batch Time: 1.13 (1.18) | Data Time: 0.00 (0.05) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 3.34e-01 (9.34e-01)\nINFO 2025-01-24 20:36:05,214 train_utils.py: 271: Train Epoch: [0][  300/16621] | Batch Time: 1.11 (1.18) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 6.85e-01 (9.32e-01)\nINFO 2025-01-24 20:36:16,403 train_utils.py: 271: Train Epoch: [0][  310/16621] | Batch Time: 1.15 (1.18) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 2.27e-01 (9.36e-01)\nINFO 2025-01-24 20:36:27,620 train_utils.py: 271: Train Epoch: [0][  320/16621] | Batch Time: 1.12 (1.18) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 2.88e-01 (9.45e-01)\nINFO 2025-01-24 20:36:39,013 train_utils.py: 271: Train Epoch: [0][  330/16621] | Batch Time: 1.11 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.03e+00 (9.45e-01)\nINFO 2025-01-24 20:36:50,149 train_utils.py: 271: Train Epoch: [0][  340/16621] | Batch Time: 1.13 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.03e+00 (9.47e-01)\nINFO 2025-01-24 20:37:01,348 train_utils.py: 271: Train Epoch: [0][  350/16621] | Batch Time: 1.11 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 1.17e+00 (9.36e-01)\nINFO 2025-01-24 20:37:12,485 train_utils.py: 271: Train Epoch: [0][  360/16621] | Batch Time: 1.10 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.07/10.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 4.72e-01 (9.27e-01)\nINFO 2025-01-24 20:37:24,110 train_utils.py: 271: Train Epoch: [0][  370/16621] | Batch Time: 1.29 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 9.00 (8.08/10.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 8.14e-01 (9.33e-01)\nINFO 2025-01-24 20:37:35,447 train_utils.py: 271: Train Epoch: [0][  380/16621] | Batch Time: 1.13 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 1.76e-01 (9.31e-01)\nINFO 2025-01-24 20:37:47,109 train_utils.py: 271: Train Epoch: [0][  390/16621] | Batch Time: 1.11 (1.17) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.18e-01 (9.22e-01)\nINFO 2025-01-24 20:37:58,268 train_utils.py: 271: Train Epoch: [0][  400/16621] | Batch Time: 1.14 (1.17) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 7.91e-01 (9.18e-01)\nINFO 2025-01-24 20:38:09,634 train_utils.py: 271: Train Epoch: [0][  410/16621] | Batch Time: 1.10 (1.17) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.01e+00 (9.29e-01)\nINFO 2025-01-24 20:38:21,070 train_utils.py: 271: Train Epoch: [0][  420/16621] | Batch Time: 1.10 (1.17) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 3.94e-01 (9.21e-01)\nINFO 2025-01-24 20:38:32,500 train_utils.py: 271: Train Epoch: [0][  430/16621] | Batch Time: 1.14 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 5.77e-01 (9.21e-01)\nINFO 2025-01-24 20:38:44,036 train_utils.py: 271: Train Epoch: [0][  440/16621] | Batch Time: 1.17 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 4.22e-01 (9.27e-01)\nINFO 2025-01-24 20:38:55,324 train_utils.py: 271: Train Epoch: [0][  450/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 7.16e-01 (9.37e-01)\nINFO 2025-01-24 20:39:06,873 train_utils.py: 271: Train Epoch: [0][  460/16621] | Batch Time: 1.29 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 9.00 (8.09/10.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 2.31e+00 (9.38e-01)\nINFO 2025-01-24 20:39:18,448 train_utils.py: 271: Train Epoch: [0][  470/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 8.96e-01 (9.35e-01)\nINFO 2025-01-24 20:39:29,792 train_utils.py: 271: Train Epoch: [0][  480/16621] | Batch Time: 1.10 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 7.59e-01 (9.32e-01)\nINFO 2025-01-24 20:39:41,324 train_utils.py: 271: Train Epoch: [0][  490/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 4.72e-01 (9.23e-01)\nINFO 2025-01-24 20:39:52,821 train_utils.py: 271: Train Epoch: [0][  500/16621] | Batch Time: 1.14 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 2.67e-01 (9.22e-01)\nINFO 2025-01-24 20:40:04,341 train_utils.py: 271: Train Epoch: [0][  510/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.48e+00 (9.25e-01)\nINFO 2025-01-24 20:40:15,601 train_utils.py: 271: Train Epoch: [0][  520/16621] | Batch Time: 1.14 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 7.02e-01 (9.16e-01)\nINFO 2025-01-24 20:40:26,805 train_utils.py: 271: Train Epoch: [0][  530/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 3.98e-01 (9.19e-01)\nINFO 2025-01-24 20:40:37,960 train_utils.py: 271: Train Epoch: [0][  540/16621] | Batch Time: 1.10 (1.16) | Data Time: 0.00 (0.03) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 4.06e-01 (9.13e-01)\nINFO 2025-01-24 20:40:49,122 train_utils.py: 271: Train Epoch: [0][  550/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 1.68e+00 (9.15e-01)\nINFO 2025-01-24 20:41:00,369 train_utils.py: 271: Train Epoch: [0][  560/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 1.07e+00 (9.10e-01)\nINFO 2025-01-24 20:41:11,603 train_utils.py: 271: Train Epoch: [0][  570/16621] | Batch Time: 1.15 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 9.46e+00 (9.20e-01)\nINFO 2025-01-24 20:41:23,069 train_utils.py: 271: Train Epoch: [0][  580/16621] | Batch Time: 1.16 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.81e-01 (9.27e-01)\nINFO 2025-01-24 20:41:34,437 train_utils.py: 271: Train Epoch: [0][  590/16621] | Batch Time: 1.16 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 6.54e-01 (9.28e-01)\nINFO 2025-01-24 20:41:45,784 train_utils.py: 271: Train Epoch: [0][  600/16621] | Batch Time: 1.15 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.26e+00 (9.37e-01)\nINFO 2025-01-24 20:41:57,398 train_utils.py: 271: Train Epoch: [0][  610/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 3.18e-01 (9.39e-01)\nINFO 2025-01-24 20:42:08,762 train_utils.py: 271: Train Epoch: [0][  620/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 4.26e-01 (9.37e-01)\nINFO 2025-01-24 20:42:20,038 train_utils.py: 271: Train Epoch: [0][  630/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.34e+00 (9.34e-01)\nINFO 2025-01-24 20:42:31,521 train_utils.py: 271: Train Epoch: [0][  640/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.54e-01 (9.42e-01)\nINFO 2025-01-24 20:42:42,971 train_utils.py: 271: Train Epoch: [0][  650/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.80e+00 (9.54e-01)\nINFO 2025-01-24 20:42:54,465 train_utils.py: 271: Train Epoch: [0][  660/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 1.20e-01 (9.53e-01)\nINFO 2025-01-24 20:43:06,030 train_utils.py: 271: Train Epoch: [0][  670/16621] | Batch Time: 1.16 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 5.43e-01 (9.51e-01)\nINFO 2025-01-24 20:43:17,303 train_utils.py: 271: Train Epoch: [0][  680/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 3.47e-01 (9.46e-01)\nINFO 2025-01-24 20:43:28,776 train_utils.py: 271: Train Epoch: [0][  690/16621] | Batch Time: 1.14 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 3.49e-01 (9.42e-01)\nINFO 2025-01-24 20:43:40,315 train_utils.py: 271: Train Epoch: [0][  700/16621] | Batch Time: 1.15 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 4.11e-01 (9.41e-01)\nINFO 2025-01-24 20:43:51,760 train_utils.py: 271: Train Epoch: [0][  710/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.08/10.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 2.34e-01 (9.37e-01)\nINFO 2025-01-24 20:44:03,671 train_utils.py: 271: Train Epoch: [0][  720/16621] | Batch Time: 1.14 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 1.31e+00 (9.35e-01)\nINFO 2025-01-24 20:44:15,162 train_utils.py: 271: Train Epoch: [0][  730/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 6.71e-01 (9.56e-01)\nINFO 2025-01-24 20:44:26,961 train_utils.py: 271: Train Epoch: [0][  740/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 8.35e-01 (9.54e-01)\nINFO 2025-01-24 20:44:38,343 train_utils.py: 271: Train Epoch: [0][  750/16621] | Batch Time: 1.16 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 3.64e-01 (9.51e-01)\nINFO 2025-01-24 20:44:49,782 train_utils.py: 271: Train Epoch: [0][  760/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 2.79e-01 (9.52e-01)\nINFO 2025-01-24 20:45:01,406 train_utils.py: 271: Train Epoch: [0][  770/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 2.61e-01 (9.48e-01)\nINFO 2025-01-24 20:45:12,990 train_utils.py: 271: Train Epoch: [0][  780/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 2.84e-01 (9.42e-01)\nINFO 2025-01-24 20:45:24,591 train_utils.py: 271: Train Epoch: [0][  790/16621] | Batch Time: 1.12 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 7.78e-01 (9.38e-01)\nINFO 2025-01-24 20:45:35,987 train_utils.py: 271: Train Epoch: [0][  800/16621] | Batch Time: 1.20 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 2.52e-01 (9.37e-01)\nINFO 2025-01-24 20:45:47,472 train_utils.py: 271: Train Epoch: [0][  810/16621] | Batch Time: 1.16 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 3.61e-01 (9.30e-01)\nINFO 2025-01-24 20:45:58,790 train_utils.py: 271: Train Epoch: [0][  820/16621] | Batch Time: 1.14 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 3.85e-01 (9.33e-01)\nINFO 2025-01-24 20:46:10,403 train_utils.py: 271: Train Epoch: [0][  830/16621] | Batch Time: 1.11 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 5.75e-01 (9.32e-01)\nINFO 2025-01-24 20:46:21,789 train_utils.py: 271: Train Epoch: [0][  840/16621] | Batch Time: 1.13 (1.16) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.79e-01 (9.28e-01)\nINFO 2025-01-24 20:46:33,025 train_utils.py: 271: Train Epoch: [0][  850/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.20e+00 (9.29e-01)\nINFO 2025-01-24 20:46:44,426 train_utils.py: 271: Train Epoch: [0][  860/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.39e-01 (9.26e-01)\nINFO 2025-01-24 20:46:56,004 train_utils.py: 271: Train Epoch: [0][  870/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 6.17e-01 (9.23e-01)\nINFO 2025-01-24 20:47:07,261 train_utils.py: 271: Train Epoch: [0][  880/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 1.76e-01 (9.19e-01)\nINFO 2025-01-24 20:47:18,535 train_utils.py: 271: Train Epoch: [0][  890/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.14e+00 (9.16e-01)\nINFO 2025-01-24 20:47:29,933 train_utils.py: 271: Train Epoch: [0][  900/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 9.00 (8.09/10.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.00e+00 (9.19e-01)\nINFO 2025-01-24 20:47:41,341 train_utils.py: 271: Train Epoch: [0][  910/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.64e-01 (9.14e-01)\nINFO 2025-01-24 20:47:52,922 train_utils.py: 271: Train Epoch: [0][  920/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 2.30e+00 (9.13e-01)\nINFO 2025-01-24 20:48:04,232 train_utils.py: 271: Train Epoch: [0][  930/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.02) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 3.07e-01 (9.16e-01)\nINFO 2025-01-24 20:48:15,687 train_utils.py: 271: Train Epoch: [0][  940/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 3.25e-01 (9.15e-01)\nINFO 2025-01-24 20:48:26,933 train_utils.py: 271: Train Epoch: [0][  950/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 1.36e+00 (9.11e-01)\nINFO 2025-01-24 20:48:38,593 train_utils.py: 271: Train Epoch: [0][  960/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 6.78e-01 (9.07e-01)\nINFO 2025-01-24 20:48:49,977 train_utils.py: 271: Train Epoch: [0][  970/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 5.40e-01 (9.06e-01)\nINFO 2025-01-24 20:49:01,351 train_utils.py: 271: Train Epoch: [0][  980/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 6.76e-01 (9.03e-01)\nINFO 2025-01-24 20:49:12,863 train_utils.py: 271: Train Epoch: [0][  990/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.09/10.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 7.63e-01 (9.04e-01)\nINFO 2025-01-24 20:49:24,331 train_utils.py: 271: Train Epoch: [0][ 1000/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 9.17e-01 (9.04e-01)\nINFO 2025-01-24 20:49:35,928 train_utils.py: 271: Train Epoch: [0][ 1010/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.09/10.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 4.52e-01 (9.05e-01)\nINFO 2025-01-24 20:49:47,413 train_utils.py: 271: Train Epoch: [0][ 1020/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 4.57e-01 (9.07e-01)\nINFO 2025-01-24 20:49:58,970 train_utils.py: 271: Train Epoch: [0][ 1030/16621] | Batch Time: 1.37 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 1.02e+00 (9.07e-01)\nINFO 2025-01-24 20:50:10,393 train_utils.py: 271: Train Epoch: [0][ 1040/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.94e-01 (9.10e-01)\nINFO 2025-01-24 20:50:21,851 train_utils.py: 271: Train Epoch: [0][ 1050/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.17e+00 (9.07e-01)\nINFO 2025-01-24 20:50:33,375 train_utils.py: 271: Train Epoch: [0][ 1060/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 4.50e-01 (9.04e-01)\nINFO 2025-01-24 20:50:44,865 train_utils.py: 271: Train Epoch: [0][ 1070/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.57e-01 (9.13e-01)\nINFO 2025-01-24 20:50:56,789 train_utils.py: 271: Train Epoch: [0][ 1080/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 7.58e-01 (9.10e-01)\nINFO 2025-01-24 20:51:08,234 train_utils.py: 271: Train Epoch: [0][ 1090/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 1.06e+00 (9.08e-01)\nINFO 2025-01-24 20:51:19,707 train_utils.py: 271: Train Epoch: [0][ 1100/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 2.49e-01 (9.05e-01)\nINFO 2025-01-24 20:51:31,403 train_utils.py: 271: Train Epoch: [0][ 1110/16621] | Batch Time: 1.46 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 10.00 (8.09/10.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 3.89e-01 (9.01e-01)\nINFO 2025-01-24 20:51:43,042 train_utils.py: 271: Train Epoch: [0][ 1120/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 6.53e-01 (9.01e-01)\nINFO 2025-01-24 20:51:54,403 train_utils.py: 271: Train Epoch: [0][ 1130/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.07e+00 (9.04e-01)\nINFO 2025-01-24 20:52:05,684 train_utils.py: 271: Train Epoch: [0][ 1140/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 1.03e+00 (9.02e-01)\nINFO 2025-01-24 20:52:17,001 train_utils.py: 271: Train Epoch: [0][ 1150/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 3.34e-01 (9.01e-01)\nINFO 2025-01-24 20:52:28,349 train_utils.py: 271: Train Epoch: [0][ 1160/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 4.66e-01 (8.97e-01)\nINFO 2025-01-24 20:52:39,918 train_utils.py: 271: Train Epoch: [0][ 1170/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 5.84e+00 (9.00e-01)\nINFO 2025-01-24 20:52:51,373 train_utils.py: 271: Train Epoch: [0][ 1180/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 1.07e+00 (9.02e-01)\nINFO 2025-01-24 20:53:03,053 train_utils.py: 271: Train Epoch: [0][ 1190/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 5.29e-01 (8.99e-01)\nINFO 2025-01-24 20:53:14,491 train_utils.py: 271: Train Epoch: [0][ 1200/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 7.44e-01 (8.97e-01)\nINFO 2025-01-24 20:53:25,777 train_utils.py: 271: Train Epoch: [0][ 1210/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 9.08e-01 (8.95e-01)\nINFO 2025-01-24 20:53:37,147 train_utils.py: 271: Train Epoch: [0][ 1220/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.91e-01 (8.91e-01)\nINFO 2025-01-24 20:53:48,787 train_utils.py: 271: Train Epoch: [0][ 1230/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 1.16e+00 (8.89e-01)\nINFO 2025-01-24 20:54:00,368 train_utils.py: 271: Train Epoch: [0][ 1240/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.92e-01 (8.87e-01)\nINFO 2025-01-24 20:54:11,810 train_utils.py: 271: Train Epoch: [0][ 1250/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.09/10.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 5.07e-01 (8.87e-01)\nINFO 2025-01-24 20:54:23,580 train_utils.py: 271: Train Epoch: [0][ 1260/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.09/10.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 9.11e-01 (8.86e-01)\nINFO 2025-01-24 20:54:34,849 train_utils.py: 271: Train Epoch: [0][ 1270/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 9.52e-01 (8.83e-01)\nINFO 2025-01-24 20:54:46,249 train_utils.py: 271: Train Epoch: [0][ 1280/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.40e+00 (8.83e-01)\nINFO 2025-01-24 20:54:57,457 train_utils.py: 271: Train Epoch: [0][ 1290/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 7.70e+00 (8.86e-01)\nINFO 2025-01-24 20:55:08,732 train_utils.py: 271: Train Epoch: [0][ 1300/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 4.15e-01 (8.83e-01)\nINFO 2025-01-24 20:55:19,823 train_utils.py: 271: Train Epoch: [0][ 1310/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 4.56e-01 (8.90e-01)\nINFO 2025-01-24 20:55:31,371 train_utils.py: 271: Train Epoch: [0][ 1320/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 1.65e+00 (8.89e-01)\nINFO 2025-01-24 20:55:42,901 train_utils.py: 271: Train Epoch: [0][ 1330/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 1.03e+00 (8.87e-01)\nINFO 2025-01-24 20:55:54,451 train_utils.py: 271: Train Epoch: [0][ 1340/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 1.76e+00 (8.87e-01)\nINFO 2025-01-24 20:56:05,710 train_utils.py: 271: Train Epoch: [0][ 1350/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 7.06e-01 (8.84e-01)\nINFO 2025-01-24 20:56:17,097 train_utils.py: 271: Train Epoch: [0][ 1360/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.99e-01 (8.89e-01)\nINFO 2025-01-24 20:56:28,520 train_utils.py: 271: Train Epoch: [0][ 1370/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 9.82e-01 (8.86e-01)\nINFO 2025-01-24 20:56:40,050 train_utils.py: 271: Train Epoch: [0][ 1380/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 3.05e-01 (8.83e-01)\nINFO 2025-01-24 20:56:51,739 train_utils.py: 271: Train Epoch: [0][ 1390/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 8.11e-01 (8.81e-01)\nINFO 2025-01-24 20:57:03,040 train_utils.py: 271: Train Epoch: [0][ 1400/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 6.67e-01 (8.90e-01)\nINFO 2025-01-24 20:57:14,589 train_utils.py: 271: Train Epoch: [0][ 1410/16621] | Batch Time: 1.18 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 2.45e-01 (8.87e-01)\nINFO 2025-01-24 20:57:26,202 train_utils.py: 271: Train Epoch: [0][ 1420/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.55e-01 (8.84e-01)\nINFO 2025-01-24 20:57:37,680 train_utils.py: 271: Train Epoch: [0][ 1430/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 2.62e-01 (8.83e-01)\nINFO 2025-01-24 20:57:49,514 train_utils.py: 271: Train Epoch: [0][ 1440/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 5.51e-01 (8.85e-01)\nINFO 2025-01-24 20:58:00,997 train_utils.py: 271: Train Epoch: [0][ 1450/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.09/10.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 4.40e-01 (8.84e-01)\nINFO 2025-01-24 20:58:12,653 train_utils.py: 271: Train Epoch: [0][ 1460/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 1.03e+00 (8.82e-01)\nINFO 2025-01-24 20:58:24,177 train_utils.py: 271: Train Epoch: [0][ 1470/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 9.35e-01 (8.79e-01)\nINFO 2025-01-24 20:58:35,890 train_utils.py: 271: Train Epoch: [0][ 1480/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 3.72e-01 (8.77e-01)\nINFO 2025-01-24 20:58:47,437 train_utils.py: 271: Train Epoch: [0][ 1490/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 3.67e-01 (8.76e-01)\nINFO 2025-01-24 20:58:58,922 train_utils.py: 271: Train Epoch: [0][ 1500/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 5.14e-01 (8.77e-01)\nINFO 2025-01-24 20:59:10,843 train_utils.py: 271: Train Epoch: [0][ 1510/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 4.60e-01 (8.76e-01)\nINFO 2025-01-24 20:59:22,744 train_utils.py: 271: Train Epoch: [0][ 1520/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 2.46e-01 (8.78e-01)\nINFO 2025-01-24 20:59:34,123 train_utils.py: 271: Train Epoch: [0][ 1530/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 1.69e+00 (8.79e-01)\nINFO 2025-01-24 20:59:45,441 train_utils.py: 271: Train Epoch: [0][ 1540/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 1.37e+00 (8.79e-01)\nINFO 2025-01-24 20:59:56,992 train_utils.py: 271: Train Epoch: [0][ 1550/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 3.03e-01 (8.85e-01)\nINFO 2025-01-24 21:00:08,291 train_utils.py: 271: Train Epoch: [0][ 1560/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 1.13e+00 (8.91e-01)\nINFO 2025-01-24 21:00:19,877 train_utils.py: 271: Train Epoch: [0][ 1570/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 2.15e-01 (8.91e-01)\nINFO 2025-01-24 21:00:31,431 train_utils.py: 271: Train Epoch: [0][ 1580/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 2.91e-01 (8.90e-01)\nINFO 2025-01-24 21:00:42,898 train_utils.py: 271: Train Epoch: [0][ 1590/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 3.41e-01 (8.88e-01)\nINFO 2025-01-24 21:00:54,316 train_utils.py: 271: Train Epoch: [0][ 1600/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 2.37e+00 (8.91e-01)\nINFO 2025-01-24 21:01:05,826 train_utils.py: 271: Train Epoch: [0][ 1610/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 1.20e+00 (8.92e-01)\nINFO 2025-01-24 21:01:17,298 train_utils.py: 271: Train Epoch: [0][ 1620/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 8.35e-01 (8.91e-01)\nINFO 2025-01-24 21:01:28,731 train_utils.py: 271: Train Epoch: [0][ 1630/16621] | Batch Time: 1.17 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 2.23e-01 (8.89e-01)\nINFO 2025-01-24 21:01:40,097 train_utils.py: 271: Train Epoch: [0][ 1640/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 3.45e-01 (8.90e-01)\nINFO 2025-01-24 21:01:51,545 train_utils.py: 271: Train Epoch: [0][ 1650/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 2.73e-01 (8.90e-01)\nINFO 2025-01-24 21:02:02,883 train_utils.py: 271: Train Epoch: [0][ 1660/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 3.12e-01 (8.88e-01)\nINFO 2025-01-24 21:02:14,560 train_utils.py: 271: Train Epoch: [0][ 1670/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 5.93e-01 (8.85e-01)\nINFO 2025-01-24 21:02:25,852 train_utils.py: 271: Train Epoch: [0][ 1680/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 2.20e-01 (8.84e-01)\nINFO 2025-01-24 21:02:37,162 train_utils.py: 271: Train Epoch: [0][ 1690/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 6.33e-01 (8.82e-01)\nINFO 2025-01-24 21:02:49,058 train_utils.py: 271: Train Epoch: [0][ 1700/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 6.52e-01 (8.81e-01)\nINFO 2025-01-24 21:03:00,618 train_utils.py: 271: Train Epoch: [0][ 1710/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 2.79e-01 (8.81e-01)\nINFO 2025-01-24 21:03:11,994 train_utils.py: 271: Train Epoch: [0][ 1720/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 9.02e-01 (8.80e-01)\nINFO 2025-01-24 21:03:23,802 train_utils.py: 271: Train Epoch: [0][ 1730/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 3.74e-01 (8.78e-01)\nINFO 2025-01-24 21:03:35,740 train_utils.py: 271: Train Epoch: [0][ 1740/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 5.89e-01 (8.76e-01)\nINFO 2025-01-24 21:03:47,484 train_utils.py: 271: Train Epoch: [0][ 1750/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 5.58e-01 (8.75e-01)\nINFO 2025-01-24 21:03:59,157 train_utils.py: 271: Train Epoch: [0][ 1760/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 6.52e-01 (8.73e-01)\nINFO 2025-01-24 21:04:10,647 train_utils.py: 271: Train Epoch: [0][ 1770/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 1.31e+00 (8.73e-01)\nINFO 2025-01-24 21:04:22,481 train_utils.py: 271: Train Epoch: [0][ 1780/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 9.51e-01 (8.75e-01)\nINFO 2025-01-24 21:04:33,743 train_utils.py: 271: Train Epoch: [0][ 1790/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 6.83e-01 (8.73e-01)\nINFO 2025-01-24 21:04:45,071 train_utils.py: 271: Train Epoch: [0][ 1800/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 1.07e+00 (8.72e-01)\nINFO 2025-01-24 21:04:56,332 train_utils.py: 271: Train Epoch: [0][ 1810/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 5.00e-01 (8.73e-01)\nINFO 2025-01-24 21:05:07,640 train_utils.py: 271: Train Epoch: [0][ 1820/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 1.99e+00 (8.72e-01)\nINFO 2025-01-24 21:05:18,939 train_utils.py: 271: Train Epoch: [0][ 1830/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 4.82e-01 (8.74e-01)\nINFO 2025-01-24 21:05:30,567 train_utils.py: 271: Train Epoch: [0][ 1840/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 3.23e-01 (8.72e-01)\nINFO 2025-01-24 21:05:42,234 train_utils.py: 271: Train Epoch: [0][ 1850/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 2.39e-01 (8.70e-01)\nINFO 2025-01-24 21:05:53,631 train_utils.py: 271: Train Epoch: [0][ 1860/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 2.56e+00 (8.70e-01)\nINFO 2025-01-24 21:06:04,949 train_utils.py: 271: Train Epoch: [0][ 1870/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 3.26e-01 (8.70e-01)\nINFO 2025-01-24 21:06:16,575 train_utils.py: 271: Train Epoch: [0][ 1880/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 6.06e-01 (8.69e-01)\nINFO 2025-01-24 21:06:28,117 train_utils.py: 271: Train Epoch: [0][ 1890/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 7.79e-01 (8.69e-01)\nINFO 2025-01-24 21:06:39,417 train_utils.py: 271: Train Epoch: [0][ 1900/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 1.66e+00 (8.68e-01)\nINFO 2025-01-24 21:06:51,058 train_utils.py: 271: Train Epoch: [0][ 1910/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.86e-01 (8.68e-01)\nINFO 2025-01-24 21:07:02,349 train_utils.py: 271: Train Epoch: [0][ 1920/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.07e-01 (8.68e-01)\nINFO 2025-01-24 21:07:13,653 train_utils.py: 271: Train Epoch: [0][ 1930/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 8.14e-01 (8.66e-01)\nINFO 2025-01-24 21:07:24,918 train_utils.py: 271: Train Epoch: [0][ 1940/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 4.90e-01 (8.67e-01)\nINFO 2025-01-24 21:07:36,488 train_utils.py: 271: Train Epoch: [0][ 1950/16621] | Batch Time: 1.18 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 3.20e-01 (8.70e-01)\nINFO 2025-01-24 21:07:47,804 train_utils.py: 271: Train Epoch: [0][ 1960/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 3.74e-01 (8.69e-01)\nINFO 2025-01-24 21:07:59,115 train_utils.py: 271: Train Epoch: [0][ 1970/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 2.66e+00 (8.70e-01)\nINFO 2025-01-24 21:08:10,605 train_utils.py: 271: Train Epoch: [0][ 1980/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 3.43e-01 (8.70e-01)\nINFO 2025-01-24 21:08:22,098 train_utils.py: 271: Train Epoch: [0][ 1990/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 2.47e-01 (8.68e-01)\nINFO 2025-01-24 21:08:33,341 train_utils.py: 271: Train Epoch: [0][ 2000/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 6.90e-01 (8.66e-01)\nINFO 2025-01-24 21:08:44,610 train_utils.py: 271: Train Epoch: [0][ 2010/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 3.60e-01 (8.65e-01)\nINFO 2025-01-24 21:08:55,991 train_utils.py: 271: Train Epoch: [0][ 2020/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 1.89e-01 (8.65e-01)\nINFO 2025-01-24 21:09:07,487 train_utils.py: 271: Train Epoch: [0][ 2030/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 9.15e-01 (8.65e-01)\nINFO 2025-01-24 21:09:18,763 train_utils.py: 271: Train Epoch: [0][ 2040/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 2.36e+00 (8.65e-01)\nINFO 2025-01-24 21:09:30,048 train_utils.py: 271: Train Epoch: [0][ 2050/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 1.16e+00 (8.66e-01)\nINFO 2025-01-24 21:09:41,794 train_utils.py: 271: Train Epoch: [0][ 2060/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 5.06e-01 (8.64e-01)\nINFO 2025-01-24 21:09:53,578 train_utils.py: 271: Train Epoch: [0][ 2070/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 2.58e-01 (8.63e-01)\nINFO 2025-01-24 21:10:05,267 train_utils.py: 271: Train Epoch: [0][ 2080/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 3.71e-01 (8.63e-01)\nINFO 2025-01-24 21:10:16,629 train_utils.py: 271: Train Epoch: [0][ 2090/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 5.78e-01 (8.63e-01)\nINFO 2025-01-24 21:10:28,437 train_utils.py: 271: Train Epoch: [0][ 2100/16621] | Batch Time: 1.21 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 6.60e-01 (8.65e-01)\nINFO 2025-01-24 21:10:39,974 train_utils.py: 271: Train Epoch: [0][ 2110/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 3.18e-01 (8.64e-01)\nINFO 2025-01-24 21:10:52,128 train_utils.py: 271: Train Epoch: [0][ 2120/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 7.53e-01 (8.69e-01)\nINFO 2025-01-24 21:11:03,914 train_utils.py: 271: Train Epoch: [0][ 2130/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 1.69e+00 (8.69e-01)\nINFO 2025-01-24 21:11:14,994 train_utils.py: 271: Train Epoch: [0][ 2140/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 2.45e+00 (8.68e-01)\nINFO 2025-01-24 21:11:26,263 train_utils.py: 271: Train Epoch: [0][ 2150/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 1.27e+00 (8.68e-01)\nINFO 2025-01-24 21:11:37,585 train_utils.py: 271: Train Epoch: [0][ 2160/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 9.81e-01 (8.71e-01)\nINFO 2025-01-24 21:11:48,988 train_utils.py: 271: Train Epoch: [0][ 2170/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 1.08e+00 (8.69e-01)\nINFO 2025-01-24 21:12:00,653 train_utils.py: 271: Train Epoch: [0][ 2180/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 4.82e-01 (8.68e-01)\nINFO 2025-01-24 21:12:12,136 train_utils.py: 271: Train Epoch: [0][ 2190/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 2.92e-01 (8.66e-01)\nINFO 2025-01-24 21:12:23,803 train_utils.py: 271: Train Epoch: [0][ 2200/16621] | Batch Time: 1.30 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 8.99e-01 (8.65e-01)\nINFO 2025-01-24 21:12:35,467 train_utils.py: 271: Train Epoch: [0][ 2210/16621] | Batch Time: 1.31 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 6.25e-01 (8.65e-01)\nINFO 2025-01-24 21:12:47,267 train_utils.py: 271: Train Epoch: [0][ 2220/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 4.90e-01 (8.65e-01)\nINFO 2025-01-24 21:12:58,917 train_utils.py: 271: Train Epoch: [0][ 2230/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 5.50e-01 (8.65e-01)\nINFO 2025-01-24 21:13:10,294 train_utils.py: 271: Train Epoch: [0][ 2240/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 7.72e-01 (8.66e-01)\nINFO 2025-01-24 21:13:21,478 train_utils.py: 271: Train Epoch: [0][ 2250/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.46e-01 (8.63e-01)\nINFO 2025-01-24 21:13:32,742 train_utils.py: 271: Train Epoch: [0][ 2260/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 6.98e-01 (8.64e-01)\nINFO 2025-01-24 21:13:44,174 train_utils.py: 271: Train Epoch: [0][ 2270/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 1.44e+00 (8.63e-01)\nINFO 2025-01-24 21:13:55,781 train_utils.py: 271: Train Epoch: [0][ 2280/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 3.70e-01 (8.63e-01)\nINFO 2025-01-24 21:14:07,019 train_utils.py: 271: Train Epoch: [0][ 2290/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 5.63e-01 (8.61e-01)\nINFO 2025-01-24 21:14:18,320 train_utils.py: 271: Train Epoch: [0][ 2300/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 4.89e-01 (8.60e-01)\nINFO 2025-01-24 21:14:29,819 train_utils.py: 271: Train Epoch: [0][ 2310/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 5.70e-01 (8.58e-01)\nINFO 2025-01-24 21:14:41,582 train_utils.py: 271: Train Epoch: [0][ 2320/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 5.18e-01 (8.58e-01)\nINFO 2025-01-24 21:14:53,057 train_utils.py: 271: Train Epoch: [0][ 2330/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 8.61e-01 (8.58e-01)\nINFO 2025-01-24 21:15:04,375 train_utils.py: 271: Train Epoch: [0][ 2340/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 3.87e-01 (8.57e-01)\nINFO 2025-01-24 21:15:15,636 train_utils.py: 271: Train Epoch: [0][ 2350/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.40e-01 (8.56e-01)\nINFO 2025-01-24 21:15:26,915 train_utils.py: 271: Train Epoch: [0][ 2360/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 1.78e-01 (8.55e-01)\nINFO 2025-01-24 21:15:38,368 train_utils.py: 271: Train Epoch: [0][ 2370/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 9.50e-01 (8.55e-01)\nINFO 2025-01-24 21:15:49,724 train_utils.py: 271: Train Epoch: [0][ 2380/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 4.11e-01 (8.54e-01)\nINFO 2025-01-24 21:16:00,968 train_utils.py: 271: Train Epoch: [0][ 2390/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.57e+00 (8.55e-01)\nINFO 2025-01-24 21:16:12,466 train_utils.py: 271: Train Epoch: [0][ 2400/16621] | Batch Time: 1.16 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.45e-01 (8.54e-01)\nINFO 2025-01-24 21:16:24,014 train_utils.py: 271: Train Epoch: [0][ 2410/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 1.30e-01 (8.53e-01)\nINFO 2025-01-24 21:16:35,356 train_utils.py: 271: Train Epoch: [0][ 2420/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 1.45e-01 (8.52e-01)\nINFO 2025-01-24 21:16:46,980 train_utils.py: 271: Train Epoch: [0][ 2430/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 7.37e-01 (8.53e-01)\nINFO 2025-01-24 21:16:58,300 train_utils.py: 271: Train Epoch: [0][ 2440/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 4.60e-01 (8.53e-01)\nINFO 2025-01-24 21:17:09,818 train_utils.py: 271: Train Epoch: [0][ 2450/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 3.00e-01 (8.52e-01)\nINFO 2025-01-24 21:17:21,460 train_utils.py: 271: Train Epoch: [0][ 2460/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 1.36e-01 (8.51e-01)\nINFO 2025-01-24 21:17:32,882 train_utils.py: 271: Train Epoch: [0][ 2470/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 3.09e-01 (8.49e-01)\nINFO 2025-01-24 21:17:44,398 train_utils.py: 271: Train Epoch: [0][ 2480/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 8.04e-01 (8.51e-01)\nINFO 2025-01-24 21:17:56,028 train_utils.py: 271: Train Epoch: [0][ 2490/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 1.51e+00 (8.52e-01)\nINFO 2025-01-24 21:18:07,466 train_utils.py: 271: Train Epoch: [0][ 2500/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 1.91e-01 (8.51e-01)\nINFO 2025-01-24 21:18:18,849 train_utils.py: 271: Train Epoch: [0][ 2510/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 5.34e-01 (8.50e-01)\nINFO 2025-01-24 21:18:30,530 train_utils.py: 271: Train Epoch: [0][ 2520/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 1.83e-01 (8.49e-01)\nINFO 2025-01-24 21:18:41,771 train_utils.py: 271: Train Epoch: [0][ 2530/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 6.20e-01 (8.49e-01)\nINFO 2025-01-24 21:18:53,162 train_utils.py: 271: Train Epoch: [0][ 2540/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 1.54e-01 (8.48e-01)\nINFO 2025-01-24 21:19:04,571 train_utils.py: 271: Train Epoch: [0][ 2550/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 7.00e-01 (8.49e-01)\nINFO 2025-01-24 21:19:15,835 train_utils.py: 271: Train Epoch: [0][ 2560/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.33e-01 (8.48e-01)\nINFO 2025-01-24 21:19:27,244 train_utils.py: 271: Train Epoch: [0][ 2570/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 7.78e-01 (8.47e-01)\nINFO 2025-01-24 21:19:38,654 train_utils.py: 271: Train Epoch: [0][ 2580/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 4.98e-01 (8.50e-01)\nINFO 2025-01-24 21:19:50,706 train_utils.py: 271: Train Epoch: [0][ 2590/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 1.39e-01 (8.48e-01)\nINFO 2025-01-24 21:20:01,892 train_utils.py: 271: Train Epoch: [0][ 2600/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 3.86e-01 (8.47e-01)\nINFO 2025-01-24 21:20:13,239 train_utils.py: 271: Train Epoch: [0][ 2610/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 2.21e+00 (8.47e-01)\nINFO 2025-01-24 21:20:24,583 train_utils.py: 271: Train Epoch: [0][ 2620/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 6.41e-01 (8.46e-01)\nINFO 2025-01-24 21:20:35,813 train_utils.py: 271: Train Epoch: [0][ 2630/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 7.22e-01 (8.45e-01)\nINFO 2025-01-24 21:20:47,514 train_utils.py: 271: Train Epoch: [0][ 2640/16621] | Batch Time: 1.45 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 10.00 (8.10/10.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 1.61e+00 (8.45e-01)\nINFO 2025-01-24 21:20:58,831 train_utils.py: 271: Train Epoch: [0][ 2650/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 2.56e-01 (8.47e-01)\nINFO 2025-01-24 21:21:10,063 train_utils.py: 271: Train Epoch: [0][ 2660/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 2.53e-01 (8.47e-01)\nINFO 2025-01-24 21:21:21,601 train_utils.py: 271: Train Epoch: [0][ 2670/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 2.24e-01 (8.46e-01)\nINFO 2025-01-24 21:21:32,811 train_utils.py: 271: Train Epoch: [0][ 2680/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 4.41e+00 (8.46e-01)\nINFO 2025-01-24 21:21:44,028 train_utils.py: 271: Train Epoch: [0][ 2690/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 8.32e-01 (8.44e-01)\nINFO 2025-01-24 21:21:55,234 train_utils.py: 271: Train Epoch: [0][ 2700/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 3.84e-01 (8.44e-01)\nINFO 2025-01-24 21:22:06,638 train_utils.py: 271: Train Epoch: [0][ 2710/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 5.00e-01 (8.43e-01)\nINFO 2025-01-24 21:22:18,013 train_utils.py: 271: Train Epoch: [0][ 2720/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 1.65e-01 (8.42e-01)\nINFO 2025-01-24 21:22:29,306 train_utils.py: 271: Train Epoch: [0][ 2730/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 1.22e+00 (8.46e-01)\nINFO 2025-01-24 21:22:40,557 train_utils.py: 271: Train Epoch: [0][ 2740/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 1.07e+00 (8.45e-01)\nINFO 2025-01-24 21:22:51,838 train_utils.py: 271: Train Epoch: [0][ 2750/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 7.56e-01 (8.46e-01)\nINFO 2025-01-24 21:23:03,521 train_utils.py: 271: Train Epoch: [0][ 2760/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 2.53e-01 (8.45e-01)\nINFO 2025-01-24 21:23:14,741 train_utils.py: 271: Train Epoch: [0][ 2770/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 5.32e-01 (8.45e-01)\nINFO 2025-01-24 21:23:26,403 train_utils.py: 271: Train Epoch: [0][ 2780/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 9.09e-01 (8.45e-01)\nINFO 2025-01-24 21:23:37,732 train_utils.py: 271: Train Epoch: [0][ 2790/16621] | Batch Time: 1.28 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 1.18e+00 (8.44e-01)\nINFO 2025-01-24 21:23:49,163 train_utils.py: 271: Train Epoch: [0][ 2800/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 1.08e+00 (8.45e-01)\nINFO 2025-01-24 21:24:00,700 train_utils.py: 271: Train Epoch: [0][ 2810/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 8.18e-01 (8.44e-01)\nINFO 2025-01-24 21:24:12,069 train_utils.py: 271: Train Epoch: [0][ 2820/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 2.88e-01 (8.44e-01)\nINFO 2025-01-24 21:24:23,414 train_utils.py: 271: Train Epoch: [0][ 2830/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 1.52e+00 (8.45e-01)\nINFO 2025-01-24 21:24:34,604 train_utils.py: 271: Train Epoch: [0][ 2840/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 3.38e-01 (8.45e-01)\nINFO 2025-01-24 21:24:45,880 train_utils.py: 271: Train Epoch: [0][ 2850/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 3.94e-01 (8.45e-01)\nINFO 2025-01-24 21:24:57,202 train_utils.py: 271: Train Epoch: [0][ 2860/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 3.97e-01 (8.43e-01)\nINFO 2025-01-24 21:25:08,324 train_utils.py: 271: Train Epoch: [0][ 2870/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 7.04e-01 (8.42e-01)\nINFO 2025-01-24 21:25:19,647 train_utils.py: 271: Train Epoch: [0][ 2880/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 1.95e-01 (8.41e-01)\nINFO 2025-01-24 21:25:31,233 train_utils.py: 271: Train Epoch: [0][ 2890/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.05e-01 (8.40e-01)\nINFO 2025-01-24 21:25:42,368 train_utils.py: 271: Train Epoch: [0][ 2900/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 6.97e-01 (8.40e-01)\nINFO 2025-01-24 21:25:53,713 train_utils.py: 271: Train Epoch: [0][ 2910/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.49e-01 (8.39e-01)\nINFO 2025-01-24 21:26:04,893 train_utils.py: 271: Train Epoch: [0][ 2920/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 5.74e-01 (8.39e-01)\nINFO 2025-01-24 21:26:16,298 train_utils.py: 271: Train Epoch: [0][ 2930/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 2.42e-01 (8.38e-01)\nINFO 2025-01-24 21:26:27,704 train_utils.py: 271: Train Epoch: [0][ 2940/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.42e-01 (8.38e-01)\nINFO 2025-01-24 21:26:39,349 train_utils.py: 271: Train Epoch: [0][ 2950/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 3.39e-01 (8.37e-01)\nINFO 2025-01-24 21:26:50,825 train_utils.py: 271: Train Epoch: [0][ 2960/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 2.90e+00 (8.37e-01)\nINFO 2025-01-24 21:27:01,995 train_utils.py: 271: Train Epoch: [0][ 2970/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 2.79e-01 (8.37e-01)\nINFO 2025-01-24 21:27:13,159 train_utils.py: 271: Train Epoch: [0][ 2980/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 2.92e-01 (8.35e-01)\nINFO 2025-01-24 21:27:24,535 train_utils.py: 271: Train Epoch: [0][ 2990/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 1.89e+00 (8.35e-01)\nINFO 2025-01-24 21:27:35,932 train_utils.py: 271: Train Epoch: [0][ 3000/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 7.54e-01 (8.35e-01)\nINFO 2025-01-24 21:27:47,463 train_utils.py: 271: Train Epoch: [0][ 3010/16621] | Batch Time: 1.28 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 4.80e-01 (8.34e-01)\nINFO 2025-01-24 21:27:58,734 train_utils.py: 271: Train Epoch: [0][ 3020/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 1.45e+00 (8.33e-01)\nINFO 2025-01-24 21:28:10,119 train_utils.py: 271: Train Epoch: [0][ 3030/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 1.38e-01 (8.32e-01)\nINFO 2025-01-24 21:28:21,845 train_utils.py: 271: Train Epoch: [0][ 3040/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 8.15e-01 (8.31e-01)\nINFO 2025-01-24 21:28:33,742 train_utils.py: 271: Train Epoch: [0][ 3050/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 9.05e-01 (8.32e-01)\nINFO 2025-01-24 21:28:45,050 train_utils.py: 271: Train Epoch: [0][ 3060/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 9.14e-01 (8.32e-01)\nINFO 2025-01-24 21:28:56,476 train_utils.py: 271: Train Epoch: [0][ 3070/16621] | Batch Time: 1.15 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 2.52e-01 (8.31e-01)\nINFO 2025-01-24 21:29:08,131 train_utils.py: 271: Train Epoch: [0][ 3080/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 2.87e-01 (8.31e-01)\nINFO 2025-01-24 21:29:19,815 train_utils.py: 271: Train Epoch: [0][ 3090/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 3.47e-01 (8.30e-01)\nINFO 2025-01-24 21:29:31,017 train_utils.py: 271: Train Epoch: [0][ 3100/16621] | Batch Time: 1.16 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.99e-01 (8.29e-01)\nINFO 2025-01-24 21:29:42,213 train_utils.py: 271: Train Epoch: [0][ 3110/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 1.63e-01 (8.28e-01)\nINFO 2025-01-24 21:29:53,385 train_utils.py: 271: Train Epoch: [0][ 3120/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 6.41e-01 (8.29e-01)\nINFO 2025-01-24 21:30:05,241 train_utils.py: 271: Train Epoch: [0][ 3130/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 6.73e-01 (8.29e-01)\nINFO 2025-01-24 21:30:16,507 train_utils.py: 271: Train Epoch: [0][ 3140/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 4.03e-01 (8.30e-01)\nINFO 2025-01-24 21:30:27,961 train_utils.py: 271: Train Epoch: [0][ 3150/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 5.54e-01 (8.29e-01)\nINFO 2025-01-24 21:30:39,235 train_utils.py: 271: Train Epoch: [0][ 3160/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 5.23e-01 (8.29e-01)\nINFO 2025-01-24 21:30:51,011 train_utils.py: 271: Train Epoch: [0][ 3170/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 2.76e-01 (8.28e-01)\nINFO 2025-01-24 21:31:02,234 train_utils.py: 271: Train Epoch: [0][ 3180/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 3.08e-01 (8.29e-01)\nINFO 2025-01-24 21:31:13,521 train_utils.py: 271: Train Epoch: [0][ 3190/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.01) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 2.63e-01 (8.28e-01)\nINFO 2025-01-24 21:31:25,071 train_utils.py: 271: Train Epoch: [0][ 3200/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.71e-01 (8.29e-01)\nINFO 2025-01-24 21:31:36,636 train_utils.py: 271: Train Epoch: [0][ 3210/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.38e-01 (8.28e-01)\nINFO 2025-01-24 21:31:47,953 train_utils.py: 271: Train Epoch: [0][ 3220/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 4.36e-01 (8.27e-01)\nINFO 2025-01-24 21:31:59,105 train_utils.py: 271: Train Epoch: [0][ 3230/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 1.52e-01 (8.26e-01)\nINFO 2025-01-24 21:32:10,346 train_utils.py: 271: Train Epoch: [0][ 3240/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.11e+00 (8.27e-01)\nINFO 2025-01-24 21:32:21,659 train_utils.py: 271: Train Epoch: [0][ 3250/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.42e-01 (8.27e-01)\nINFO 2025-01-24 21:32:32,995 train_utils.py: 271: Train Epoch: [0][ 3260/16621] | Batch Time: 1.28 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 2.97e-01 (8.28e-01)\nINFO 2025-01-24 21:32:44,342 train_utils.py: 271: Train Epoch: [0][ 3270/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.87e-01 (8.27e-01)\nINFO 2025-01-24 21:32:55,670 train_utils.py: 271: Train Epoch: [0][ 3280/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 1.40e+00 (8.27e-01)\nINFO 2025-01-24 21:33:06,995 train_utils.py: 271: Train Epoch: [0][ 3290/16621] | Batch Time: 1.28 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 7.60e-01 (8.26e-01)\nINFO 2025-01-24 21:33:18,348 train_utils.py: 271: Train Epoch: [0][ 3300/16621] | Batch Time: 1.12 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 1.06e+00 (8.26e-01)\nINFO 2025-01-24 21:33:30,000 train_utils.py: 271: Train Epoch: [0][ 3310/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 6.29e-01 (8.25e-01)\nINFO 2025-01-24 21:33:41,195 train_utils.py: 271: Train Epoch: [0][ 3320/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 7.07e-01 (8.25e-01)\nINFO 2025-01-24 21:33:52,408 train_utils.py: 271: Train Epoch: [0][ 3330/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 2.98e-01 (8.25e-01)\nINFO 2025-01-24 21:34:03,907 train_utils.py: 271: Train Epoch: [0][ 3340/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 1.35e+00 (8.26e-01)\nINFO 2025-01-24 21:34:15,206 train_utils.py: 271: Train Epoch: [0][ 3350/16621] | Batch Time: 1.14 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 7.75e-01 (8.25e-01)\nINFO 2025-01-24 21:34:26,892 train_utils.py: 271: Train Epoch: [0][ 3360/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 3.11e-01 (8.24e-01)\nINFO 2025-01-24 21:34:38,276 train_utils.py: 271: Train Epoch: [0][ 3370/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 1.06e+00 (8.24e-01)\nINFO 2025-01-24 21:34:49,446 train_utils.py: 271: Train Epoch: [0][ 3380/16621] | Batch Time: 1.10 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 3.38e-01 (8.23e-01)\nINFO 2025-01-24 21:35:00,954 train_utils.py: 271: Train Epoch: [0][ 3390/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 2.26e-01 (8.23e-01)\nINFO 2025-01-24 21:35:12,519 train_utils.py: 271: Train Epoch: [0][ 3400/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 1.77e-01 (8.23e-01)\nINFO 2025-01-24 21:35:23,743 train_utils.py: 271: Train Epoch: [0][ 3410/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 6.18e-01 (8.23e-01)\nINFO 2025-01-24 21:35:35,496 train_utils.py: 271: Train Epoch: [0][ 3420/16621] | Batch Time: 1.29 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 9.00 (8.10/10.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 9.01e-01 (8.23e-01)\nINFO 2025-01-24 21:35:46,826 train_utils.py: 271: Train Epoch: [0][ 3430/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 2.82e-01 (8.22e-01)\nINFO 2025-01-24 21:35:58,367 train_utils.py: 271: Train Epoch: [0][ 3440/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 1.74e+00 (8.23e-01)\nINFO 2025-01-24 21:36:09,557 train_utils.py: 271: Train Epoch: [0][ 3450/16621] | Batch Time: 1.11 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 1.18e+00 (8.23e-01)\nINFO 2025-01-24 21:36:21,122 train_utils.py: 271: Train Epoch: [0][ 3460/16621] | Batch Time: 1.13 (1.15) | Data Time: 0.00 (0.00) | Mem (GB): 8.00 (8.10/10.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 4.39e-01 (8.22e-01)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}