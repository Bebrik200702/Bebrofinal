{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10514767,"sourceType":"datasetVersion","datasetId":6508435}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:14.925902Z","iopub.execute_input":"2025-01-19T14:16:14.926200Z","iopub.status.idle":"2025-01-19T14:16:15.075686Z","shell.execute_reply.started":"2025-01-19T14:16:14.926173Z","shell.execute_reply":"2025-01-19T14:16:15.074909Z"}},"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘src’: File exists\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile ./src/__init__.py\nfrom .classification_modeling import CustomClassifierEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.076570Z","iopub.execute_input":"2025-01-19T14:16:15.076797Z","iopub.status.idle":"2025-01-19T14:16:15.082078Z","shell.execute_reply.started":"2025-01-19T14:16:15.076766Z","shell.execute_reply":"2025-01-19T14:16:15.081316Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Overwriting ./src/__init__.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile ./src/poolings.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContextPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.pooler_hidden_size, config.pooler_hidden_size)\n        self.dropout = StableDropout(config.pooler_dropout)\n        self.config = config\n\n    def forward(self, hidden_states, mask=None):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n\n        context_token = hidden_states[:, 0]\n        context_token = self.dropout(context_token)\n        pooled_output = self.dense(context_token)\n        pooled_output = ACT2FN[self.config.pooler_hidden_act](pooled_output)\n        return pooled_output\n\n    @property\n    def output_dim(self):\n        return self.config.hidden_size\n\n\nclass XSoftmax(torch.autograd.Function):\n    \"\"\"\n    Masked Softmax which is optimized for saving memory\n\n    Args:\n      input (:obj:`torch.tensor`): The input tensor that will apply softmax.\n      mask (:obj:`torch.IntTensor`): The mask matrix where 0 indicate that element will be ignored in the softmax calculation.\n      dim (int): The dimension that will apply softmax\n\n    Example::\n      import torch\n      from transformers.models.deberta import XSoftmax\n      # Make a tensor\n      x = torch.randn([4,20,100])\n      # Create a mask\n      mask = (x>0).int()\n      y = XSoftmax.apply(x, mask, dim=-1)\n    \"\"\"\n\n    @staticmethod\n    def forward(self, input, mask, dim):\n        self.dim = dim\n        if version.Version(torch.__version__) >= version.Version(\"1.2.0a\"):\n            rmask = ~(mask.bool())\n        else:\n            rmask = (1 - mask).byte()  # This line is not supported by Onnx tracing.\n\n        output = input.masked_fill(rmask, float(\"-inf\"))\n        output = torch.softmax(output, self.dim)\n        output.masked_fill_(rmask, 0)\n        self.save_for_backward(output)\n        return output\n\n    @staticmethod\n    def backward(self, grad_output):\n        (output,) = self.saved_tensors\n        inputGrad = _softmax_backward_data(grad_output, output, self.dim, output)\n        return inputGrad, None, None\n\n\nclass DropoutContext(object):\n    def __init__(self):\n        self.dropout = 0\n        self.mask = None\n        self.scale = 1\n        self.reuse_mask = True\n\n\ndef get_mask(input, local_context):\n    if not isinstance(local_context, DropoutContext):\n        dropout = local_context\n        mask = None\n    else:\n        dropout = local_context.dropout\n        dropout *= local_context.scale\n        mask = local_context.mask if local_context.reuse_mask else None\n\n    if dropout > 0 and mask is None:\n        if version.Version(torch.__version__) >= version.Version(\"1.2.0a\"):\n            mask = (1 - torch.empty_like(input).bernoulli_(1 - dropout)).bool()\n        else:\n            mask = (1 - torch.empty_like(input).bernoulli_(1 - dropout)).byte()\n\n    if isinstance(local_context, DropoutContext):\n        if local_context.mask is None:\n            local_context.mask = mask\n\n    return mask, dropout\n\n\nclass XDropout(torch.autograd.Function):\n    \"\"\"Optimized dropout function to save computation and memory by using mask operation instead of multiplication.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input, local_ctx):\n        mask, dropout = get_mask(input, local_ctx)\n        ctx.scale = 1.0 / (1 - dropout)\n        if dropout > 0:\n            ctx.save_for_backward(mask)\n            return input.masked_fill(mask, 0) * ctx.scale\n        else:\n            return input\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if ctx.scale > 1:\n            (mask,) = ctx.saved_tensors\n            return grad_output.masked_fill(mask, 0) * ctx.scale, None\n        else:\n            return grad_output, None\n\n\nclass StableDropout(torch.nn.Module):\n    \"\"\"\n    Optimized dropout module for stabilizing the training\n\n    Args:\n\n        drop_prob (float): the dropout probabilities\n\n    \"\"\"\n\n    def __init__(self, drop_prob):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.count = 0\n        self.context_stack = None\n\n    def forward(self, x):\n        \"\"\"\n        Call the module\n\n        Args:\n            x (:obj:`torch.tensor`): The input tensor to apply dropout\n\n\n        \"\"\"\n        if self.training and self.drop_prob > 0:\n            return XDropout.apply(x, self.get_context())\n        return x\n\n    def clear_context(self):\n        self.count = 0\n        self.context_stack = None\n\n    def init_context(self, reuse_mask=True, scale=1):\n        if self.context_stack is None:\n            self.context_stack = []\n        self.count = 0\n        for c in self.context_stack:\n            c.reuse_mask = reuse_mask\n            c.scale = scale\n\n    def get_context(self):\n        if self.context_stack is not None:\n            if self.count >= len(self.context_stack):\n                self.context_stack.append(DropoutContext())\n            ctx = self.context_stack[self.count]\n            ctx.dropout = self.drop_prob\n            self.count += 1\n            return ctx\n        else:\n            return self.drop_prob\n\nclass MeanPooling(nn.Module):\n    def __init__(self, clamp_min=1e-9):\n        super(MeanPooling, self).__init__()\n        self.clamp_min = clamp_min\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=self.clamp_min)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim=1)\n        return max_embeddings\n\nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e4\n        min_embeddings, _ = torch.min(embeddings, dim=1)\n        return min_embeddings\n\nclass Multisample_Dropout(nn.Module):\n    def __init__(self,drop_range=5,predrop=0.0,):\n        super(Multisample_Dropout, self).__init__()\n        self.dropout = nn.Dropout(predrop)\n        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(drop_range)])\n\n    def forward(self, x, module):\n        x = self.dropout(x)\n        return torch.mean(torch.stack([module(dropout(x)) for dropout in self.dropouts],dim=0),dim=0)\n\nclass Multisample_StableDropout(nn.Module):\n    def __init__(self,drop_range=5,predrop=0.0,):\n        super(Multisample_Dropout, self).__init__()\n        self.dropout = StableDropout(predrop)\n        self.dropouts = nn.ModuleList([StableDropout((i+1)*.1) for i in range(drop_range)])\n\n    def forward(self, x, module):\n        x = self.dropout(x)\n        return torch.mean(torch.stack([module(dropout(x)) for dropout in self.dropouts],dim=0),dim=0)\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, layers = 12):\n        super(WeightedLayerPooling, self).__init__()\n        self.layers = layers\n        self.layer_weights = nn.Parameter(\n                torch.tensor([1] * layers, dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_hidden_states = torch.stack(all_hidden_states, dim=0)\n        all_layer_embedding = all_hidden_states[-self.layers:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average\n\nclass Weighted_Linear(nn.Module):\n    def __init__(self, hidden_size, n_layers=12):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*3\n\n        self.layer_pooler = WeightedLayerPooling(n_layers)\n        self.sequence_pooler = MeanPooling()\n\n    def forward(self, x, mask):\n        x = self.layer_pooler(x.hidden_states)\n\n        x = self.sequence_pooler(x, mask)\n\n        return x\n\nclass Cat_LSTM(nn.Module):\n    def __init__(self, hidden_size, n_layers):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*n_layers\n        self.n_layers = n_layers\n\n        self.sequence_pooler = MeanPooling(1e-9)\n        self.rnn = Bi_RNN_FOUT(self.cat_size, self.cat_size//2)\n\n    def forward(self, x, mask):\n        \n        x = torch.cat(x.hidden_states[-self.n_layers:], dim=-1)\n\n        hidden_mask = mask.unsqueeze(-1).expand(x.size()).float()\n        x = (x * hidden_mask)\n\n        x = self.rnn(x)\n        x = self.sequence_pooler(x, mask)\n\n        return x\n\nclass LayerBaseLSTM(nn.Module):\n    def __init__(self, hidden_size,n_layers,extra_head_instances):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*n_layers\n        self.n_layers = n_layers\n        self.pooler = LSTM_Layer_Pooling(hidden_size, num_hidden_layers=self.n_layers)\n\n    def forward(self, x, mask):\n\n        x = self.pooler(x.hidden_states, mask)\n\n        return x\n\nclass LSTM_Layer_Pooling(nn.Module):\n    def __init__(self, hidden_size, num_hidden_layers=12, is_lstm=True,bidirectional=True):\n        super().__init__()\n        self.num_hidden_layers = num_hidden_layers\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n\n        self.is_lstm = is_lstm\n\n        if self.is_lstm:\n            self.lstm = nn.LSTM(\n                self.hidden_size,\n                self.hidden_size,\n                bidirectional=self.bidirectional,\n                batch_first=True\n            )\n        else:\n            self.lstm = nn.GRU(\n                self.hidden_size,\n                self.hidden_size,\n                bidirectional=self.bidirectional,\n                batch_first=True\n            )\n\n\n        self.pooling = MeanPooling(.0)\n\n    def forward(self, all_hidden_states, mask):\n\n        hidden_states = torch.stack([self.pooling(layer_i, mask)\n                                     for layer_i in all_hidden_states[-self.num_hidden_layers:]], dim=1)\n        out, _ = self.lstm(hidden_states)\n        out = out[:, -1, :]\n        return out\n\nclass Bi_RNN(nn.Module):\n    def __init__(self, size, hidden_size, layers=1):\n        super().__init__()\n        self.layers = layers\n        self.hidden_size = hidden_size\n        self.rnn = nn.LSTM(size, hidden_size, num_layers=layers, bidirectional=True, bias=False, batch_first=True)\n\n    def forward(self, x):\n        x, hidden = self.rnn(x)\n        return torch.cat((x[:,-1,:self.hidden_size], x[:,0,self.hidden_size:]), dim=-1)\n\nclass Bi_RNN_FOUT(nn.Module):\n    def __init__(self, size, hidden_size, layers=1):\n        super().__init__()\n        self.layers = layers\n        self.hidden_size = hidden_size\n        self.rnn = nn.LSTM(size, hidden_size, num_layers=layers, bidirectional=True, bias=False, batch_first=True)\n        self.initialize_lstm(self.rnn)\n    \n    def initialize_lstm(self, lstm_layer):\n        for name, param in lstm_layer.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n\n    def forward(self, x):\n        x, hidden = self.rnn(x)\n        return x\n\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self._init_weights(self.attention)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, x, mask):\n        last_hidden_states = x[0]\n        weights = self.attention(last_hidden_states)\n        feature = torch.sum(weights * last_hidden_states, dim=1)\n        return feature\n\nclass LSTMPooling(nn.Module):\n    def __init__(self, hidden_size,num_layers=1,drop=0.0):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            hidden_size,\n            hidden_size//2,\n            num_layers=1,\n            dropout=drop,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.pool = MeanPooling()\n        self.initialize_lstm(self.lstm)\n\n    def initialize_lstm(self, lstm_layer):\n        for name, param in lstm_layer.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n\n    def forward(self, x, mask):\n        last_hidden_states = x[0]\n        feature, hc = self.lstm(last_hidden_states)\n        feature = self.pool(feature, mask)\n        return feature\n    \nclass LSTMAttnPooling(nn.Module):\n    def __init__(self, hidden_size,num_layers=1,drop=0.0):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            hidden_size,\n            hidden_size//2,\n            num_layers=1,\n            dropout=drop,\n            batch_first=True,\n            bidirectional=True\n        )\n        self.pool = AttentionPooling(hidden_size)\n        self.initialize_lstm(self.lstm)\n\n    def initialize_lstm(self, lstm_layer):\n        for name, param in lstm_layer.named_parameters():\n            if 'weight_ih' in name:\n                torch.nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                torch.nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                param.data.fill_(0)\n\n    def forward(self, x, mask):\n        last_hidden_states = x[0]\n        feature, hc = self.lstm(last_hidden_states)\n        feature = self.pool([feature], mask)\n        return feature\n\nclass Weighted_Linear_Attn(nn.Module):\n    def __init__(self, hidden_size, n_layers=12):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*3\n\n        self.layer_pooler = WeightedLayerPooling(n_layers)\n        self.sequence_pooler = AttentionPooling(hidden_size)\n\n    def forward(self, x, mask):\n        x = self.layer_pooler(x.hidden_states)\n\n        x = self.sequence_pooler([x],mask)\n\n        return x\n\nclass Weighted_Linear_LSTM(nn.Module):\n    def __init__(self, hidden_size, n_layers=12):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.cat_size = hidden_size*3\n\n        self.layer_pooler = WeightedLayerPooling(n_layers)\n        self.sequence_pooler = LSTMPooling(hidden_size)\n\n    def forward(self, x, mask):\n        x = self.layer_pooler(x.hidden_states)\n\n        x = self.sequence_pooler({'last_hidden_states':x},mask)\n\n        return x\n\nclass LastTokenPooling(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.config = config\n        \n    def forward(self,input_ids,hidden_states):\n        batch_size = input_ids.shape[0]\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(hidden_states.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]\n        return pooled_logits\n\ndef get_pooling(cfg,config=None):\n    if cfg.pool == 'last_token':\n        return LastTokenPooling(config)\n    elif cfg.pool == 'mean':\n        return MeanPooling()\n    elif cfg.pool == 'max':\n        return MaxPooling()\n    elif cfg.pool == 'min':\n        return MinPooling()\n    elif cfg.pool == 'attention':\n        return AttentionPooling(config.hidden_size)\n    elif cfg.pool == 'lstm_simple':\n        return LSTMPooling(config.hidden_size)\n    elif cfg.pool == 'lstm_attention':\n        return LSTMAttnPooling(config.hidden_size)\n    elif cfg.pool == 'lstm_cat':\n        return Cat_LSTM(config.hidden_size,config.num_hidden_layers)\n    elif cfg.pool == 'lstm_layer_base':\n        return LSTM_Layer_Pooling(config.hidden_size,config.num_hidden_layers)\n    elif cfg.pool == 'weighted_linear_mean':\n        return Weighted_Linear(config.hidden_size, config.num_hidden_layers)\n    elif cfg.pool == 'weighted_linear_attn':\n        return Weighted_Linear_Attn(config.hidden_size,config.num_hidden_layers)\n    elif cfg.pool == 'weighted_linear_lstm':\n        return Weighted_Linear_LSTM(config.hidden_size,config.num_hidden_layers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.084909Z","iopub.execute_input":"2025-01-19T14:16:15.085132Z","iopub.status.idle":"2025-01-19T14:16:15.096808Z","shell.execute_reply.started":"2025-01-19T14:16:15.085113Z","shell.execute_reply":"2025-01-19T14:16:15.095990Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Overwriting ./src/poolings.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile ./src/classification_modeling.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Union\n#from transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom .poolings import *\nfrom transformers import AutoConfig, AutoModel, T5EncoderModel\n\nclass CustomClassifierEncoder(nn.Module):\n    def __init__(self,cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.config = AutoConfig.from_pretrained(cfg.model)\n        if self.config.pad_token_id is None:\n            if type(self.config.eos_token_id) != list:\n                self.config.pad_token_id = self.config.eos_token_id\n            else:\n                self.config.pad_token_id = self.config.eos_token_id[0]\n        if cfg.turn_off_drop:\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        \n        if cfg.use_only_encoder:\n            self.model = T5EncoderModel.from_pretrained(\n                cfg.model,\n                config=self.config,\n                torch_dtype=cfg.torch_dtype\n            )\n\n        else:\n            self.model = AutoModel.from_pretrained(\n                cfg.model,\n                config=self.config,\n                torch_dtype=cfg.torch_dtype\n            )\n        if cfg.use_lora:\n            peft_config = LoraConfig(\n                r=cfg.lora.r,\n                lora_alpha=cfg.lora.lora_alpha,\n                lora_dropout=cfg.lora.lora_dropout,\n                bias=cfg.lora.bias,\n                #task_type='SEQ_CLS',\n                use_dora=cfg.lora.use_dora,\n                target_modules=cfg.lora.target_modules,\n                #layers_to_transform=cfg.lora.layers_to_transform\n            )\n            self.model = get_peft_model(self.model, peft_config)\n        \n        self.pool = get_pooling(cfg,config=self.config)\n        \n        if cfg.cls_drop_type == 'stable':\n            self.cls_drop = StableDropout(cfg.cls_drop)\n        elif cfg.cls_drop_type == 'multi':\n            self.cls_drop = Multisample_Dropout(cfg.multi_drop_range)\n        else:\n            self.cls_drop = nn.Dropout(cfg.cls_drop)\n            \n        if self.cfg.pool != 'lstm_cat':\n            self.fc = nn.Linear(self.config.hidden_size,self.cfg.num_labels)\n        #elif self.cfg.pool in ['lstm_attn','lstm_simple']:\n        #    self.fc = nn.Linear(self.config.hidden_size // 2,self.cfg.num_labels)\n        else:\n            self.fc = nn.Linear(self.config.hidden_size * self.config.num_hidden_layers,self.cfg.num_labels)\n        self._init_weights(self.fc)\n\n    def _init_weights(self, module):\n        if 'initializer_range' not in self.config.to_dict().keys():\n            self.config.initializer_range = 0.02\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutput]:\n\n        outputs = self.model(\n            input_ids,\n            position_ids=position_ids,# comment for t5\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            token_type_ids=token_type_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=True,\n            return_dict=return_dict,\n        )\n        \n        if self.cfg.pool == 'last_token':\n            pooled_output = self.pool(input_ids, outputs[0]) \n        elif self.cfg.pool == 'mean':\n            pooled_output = self.pool(outputs[0],attention_mask)\n        else:\n            pooled_output = self.pool(outputs,attention_mask)\n            \n        if self.cfg.cls_drop_type != 'multi':\n            pooled_output = self.cls_drop(pooled_output)\n            logits = self.fc(pooled_output)\n        else:\n            logits = self.cls_drop(pooled_output,self.fc)\n\n\n        loss = None\n        if labels is not None:\n            pass\n            #loss_fct = nn.CrossEntropyLoss(label_smoothing=self.cfg.label_smoothing)\n            #loss = loss_fct(logits, labels)\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.098105Z","iopub.execute_input":"2025-01-19T14:16:15.098428Z","iopub.status.idle":"2025-01-19T14:16:15.113857Z","shell.execute_reply.started":"2025-01-19T14:16:15.098399Z","shell.execute_reply":"2025-01-19T14:16:15.113164Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Overwriting ./src/classification_modeling.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport pytorch_lightning as pl\nfrom src import CustomClassifierEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.stats import rankdata\nfrom tqdm.auto import tqdm, trange\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup, AutoModel, AutoTokenizer, AutoModelForMultipleChoice, AutoConfig\nimport wandb\npl.seed_everything(56)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.114652Z","iopub.execute_input":"2025-01-19T14:16:15.114962Z","iopub.status.idle":"2025-01-19T14:16:15.132040Z","shell.execute_reply.started":"2025-01-19T14:16:15.114941Z","shell.execute_reply":"2025-01-19T14:16:15.131403Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class CFG:\n    class data:\n        train_path = '/kaggle/input/t1-genii-text/train-14.csv'\n        test_path = '/kaggle/input/t1-genii-text/test-11.csv'\n        tokenizer = \"deepvk/USER-bge-m3\"#'microsoft/mdeberta-v3-base'\n        num_workers = 4\n        nfolds = 5\n        batch_size = 16\n        use_prefix = False\n        max_length = 256 \n        seed = 56\n    class model:\n        model = \"deepvk/USER-bge-m3\"#'microsoft/mdeberta-v3-base'\n        optim = torch.optim.AdamW\n        use_only_encoder = False\n        grad_acum_steps = 1\n        torch_dtype = None\n        scheduler= 'cosine'\n        warmup_steps = 0.0 #0.25\n        num_labels = 3\n        label_smoothing = 0.0\n        lr = lr_fn = 1e-5\n        cls_drop_type = None\n        cls_drop = 0.0\n        pool = 'attention'\n        max_epoches = 10\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        turn_off_drop = True\n        num_cycles = 0.5\n        eps = 1e-7\n        weight_decay = 0.0\n        weight_decay_fn = 0.0\n        betas = (0.9, 0.999)\n        use_lora = False\n    seed = 56\n    fold_number = 0\n\ndef set_wandb_cfg():\n    config = {}\n    for k,v in CFG.model.__dict__.items():\n        if '__' not in k:\n            config[k] = v\n    for k,v in CFG.data.__dict__.items():\n        if '__' not in k:\n            config[k] = v\n    config['fold_number'] = CFG.fold_number\n    return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.132748Z","iopub.execute_input":"2025-01-19T14:16:15.133043Z","iopub.status.idle":"2025-01-19T14:16:15.149789Z","shell.execute_reply.started":"2025-01-19T14:16:15.132992Z","shell.execute_reply":"2025-01-19T14:16:15.148998Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def make_df(path,is_test=False):\n    data = pd.read_csv(path)\n    df = pd.DataFrame()\n    df['text'] = data['review']\n    if not is_test:\n        df['label'] = data['sentiment']\n    else:\n        df['label'] = 0\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.150713Z","iopub.execute_input":"2025-01-19T14:16:15.151005Z","iopub.status.idle":"2025-01-19T14:16:15.166563Z","shell.execute_reply.started":"2025-01-19T14:16:15.150978Z","shell.execute_reply":"2025-01-19T14:16:15.165887Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class PLDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super().__init__()\n        self.cfg = CFG.data\n        self.data = df\n        self.tokenizer = tokenizer\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        row = self.data.iloc[index]        \n        \n        encodes = self.tokenizer.encode_plus(\n            row['text'],\n            max_length=self.cfg.max_length,\n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encodes.input_ids.squeeze(0),\n            'attention_mask': encodes.attention_mask.squeeze(0),\n            #'token_type_ids': encodes.token_type_ids.squeeze(0),\n            'labels': torch.tensor(row['label'])\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.167320Z","iopub.execute_input":"2025-01-19T14:16:15.167503Z","iopub.status.idle":"2025-01-19T14:16:15.181337Z","shell.execute_reply.started":"2025-01-19T14:16:15.167487Z","shell.execute_reply":"2025-01-19T14:16:15.180514Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class PLDataModule(pl.LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        self.cfg = CFG.data\n        self.is_setup = False\n        self.is_prepared = False\n        \n    def prepare_data(self):\n        if self.is_prepared: return None\n        self.df = make_df(self.cfg.train_path)\n        self.test_df = make_df(self.cfg.test_path,is_test=True)\n        self.test_df['text'] = self.test_df['text'].fillna('')\n        self.tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer)\n        self.is_prepared = True\n        \n    def setup(self, stage: str):\n        if self.is_setup: return None\n        kf = StratifiedKFold(n_splits=self.cfg.nfolds, shuffle=True, random_state=self.cfg.seed)\n        splits = [(x,y) for x,y in  kf.split(self.df.values,np.stack(self.df['label'].values))][CFG.fold_number]\n        self.train_df, self.val_df = self.df.iloc[splits[0]], self.df.iloc[splits[1]]\n        self.train_dataset = PLDataset(self.train_df,self.tokenizer)\n        self.val_dataset = PLDataset(self.val_df,self.tokenizer)\n        self.predict_dataset = PLDataset(self.test_df,self.tokenizer)\n        self.is_setup = True\n    \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset,\n                         batch_size=self.cfg.batch_size,\n                         num_workers=self.cfg.num_workers,\n                         pin_memory=True,\n                         shuffle=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_dataset,\n                          batch_size=self.cfg.batch_size,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True,\n                          shuffle=False)\n    \n    def predict_dataloader(self):\n        return DataLoader(self.predict_dataset,\n                          batch_size=self.cfg.batch_size,\n                          num_workers=self.cfg.num_workers,\n                          pin_memory=True,\n                          shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.182192Z","iopub.execute_input":"2025-01-19T14:16:15.182452Z","iopub.status.idle":"2025-01-19T14:16:15.199594Z","shell.execute_reply.started":"2025-01-19T14:16:15.182427Z","shell.execute_reply":"2025-01-19T14:16:15.198861Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class AverageMeter():\n    def __init__(self):\n        self.preds = []\n        self.labels = []\n        self.history = []\n    \n    def update(self,y_t,y_p):\n        self.labels += y_t\n        self.preds += y_p\n        \n    def clean(self):\n        self.preds = []\n        self.labels = []\n\n    def calc_metrics(self):\n        metrics = {}\n        \n        metrics['accuracy'] = accuracy_score(self.labels, self.preds)\n        metrics['f1_macro'] = f1_score(self.labels, self.preds,average='macro')\n        metrics['f1_micro'] = f1_score(self.labels, self.preds,average='micro')\n        \n        self.history.append(metrics)\n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.200368Z","iopub.execute_input":"2025-01-19T14:16:15.200588Z","iopub.status.idle":"2025-01-19T14:16:15.217442Z","shell.execute_reply.started":"2025-01-19T14:16:15.200570Z","shell.execute_reply":"2025-01-19T14:16:15.216701Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class PLModule(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.cfg = CFG.model\n        self.model = CustomClassifierEncoder(self.cfg)\n        self.avg_meter = AverageMeter()\n        self.criterion = nn.CrossEntropyLoss()\n        \n    def forward(self, batch):\n        output = self.model(**batch)\n        return output\n\n    def training_step(self, batch, i):\n        logits = self(batch).logits\n        loss = self.criterion(logits, batch['labels'])\n        self.log('train_loss', loss.item())\n        return loss\n            \n    def validation_step(self, batch, i):\n        logits = self(batch).logits\n        loss = self.criterion(logits, batch['labels'])\n        self.log('val_loss',loss.item())\n        \n        preds = logits.cpu().argmax(dim=-1)\n        labels = batch['labels'].cpu().tolist()\n        \n        self.avg_meter.update(labels,preds)\n    \n    def predict_step(self, batch, i):\n        logits = self(batch).logits\n        return logits.argmax(dim=-1)\n                \n    def on_validation_epoch_end(self):\n        metrics = self.avg_meter.calc_metrics()\n        self.log_dict(metrics)\n        self.avg_meter.clean()\n            \n    def configure_optimizers(self):        \n        optimizer_parameters = [\n            {'params': [p for n, p in self.model.model.named_parameters() if not any(nd in n for nd in self.cfg.no_decay)],\n             'lr': self.cfg.lr, 'weight_decay': self.cfg.weight_decay},\n            {'params': [p for n, p in self.model.model.named_parameters() if any(nd in n for nd in self.cfg.no_decay)],\n             'lr': self.cfg.lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in self.model.named_parameters() if \"model\" not in n],\n             'lr': self.cfg.lr_fn, 'weight_decay': self.cfg.weight_decay_fn}\n        ]\n        \n        optim = self.cfg.optim(\n            optimizer_parameters,\n            lr=self.cfg.lr,\n            betas=self.cfg.betas,\n            weight_decay=self.cfg.weight_decay,\n            eps=self.cfg.eps\n        )\n        \n        if self.cfg.scheduler == 'cosine':\n            scheduler = get_cosine_schedule_with_warmup(optim,\n                                                        num_training_steps=self.cfg.num_training_steps,\n                                                        num_warmup_steps=self.cfg.num_training_steps * self.cfg.warmup_steps,\n                                                        num_cycles=self.cfg.num_cycles)\n        elif self.cfg.scheduler == 'linear':\n            scheduler = get_linear_schedule_with_warmup(optim,\n                                                        num_training_steps=self.cfg.num_training_steps,\n                                                        num_warmup_steps=self.cfg.num_training_steps * self.cfg.warmup_steps)\n        else:\n            return optim\n        \n        scheduler = {'scheduler': scheduler,'interval': 'step', 'frequency': 1}\n\n        return [optim], [scheduler]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.218297Z","iopub.execute_input":"2025-01-19T14:16:15.218510Z","iopub.status.idle":"2025-01-19T14:16:15.236386Z","shell.execute_reply.started":"2025-01-19T14:16:15.218493Z","shell.execute_reply":"2025-01-19T14:16:15.235715Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def train_fold(fold_n=0):\n    pl.seed_everything(56)\n    CFG.fold_number = fold_n\n    \n    dm = PLDataModule()\n    dm.prepare_data()\n    dm.setup(0)\n    dm.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    \n    CFG.model.num_training_steps = len(dm.train_dataloader()) * CFG.model.max_epoches\n    model = PLModule()\n    \n    wandb.login(key=\"31520b01739d418e5d77a11fd8a79a70b189b8bc\")\n    os.environ['WANDB_API_KEY'] = \"31520b01739d418e5d77a11fd8a79a70b189b8bc\"\n    wandb.init(project='T1',name='labse_en_ru',config=set_wandb_cfg())\n    \n    lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n    checkpoint_cb = pl.callbacks.ModelCheckpoint(\n        dirpath='./outputs/',\n        filename='model_{epoch:02d}-{accuracy:.4f}',\n        monitor='accuracy',\n        mode='max',\n        save_last=True\n    )\n\n    trainer = pl.Trainer(\n        accelerator=\"gpu\",\n        precision=32,\n        callbacks = [lr_monitor],#[lr_monitor,checkpoint_cb],\n        logger = pl.loggers.WandbLogger(save_code=True),\n        log_every_n_steps=1,\n        accumulate_grad_batches=CFG.model.grad_acum_steps,\n        enable_checkpointing=False,\n        min_epochs=1,\n        devices=1,\n        check_val_every_n_epoch=1,\n        max_epochs=CFG.model.max_epoches\n    )\n    trainer.fit(model, datamodule=dm)\n    val_preds = trainer.predict(model,dm.val_dataloader())\n    test_preds = trainer.predict(model,dm.predict_dataloader())\n    hist = model.avg_meter.history[-1]['accuracy']\n    model = model.cpu()\n    del model, trainer, dm, checkpoint_cb, lr_monitor\n    gc.collect()\n    torch.cuda.empty_cache()\n    return hist,val_preds,test_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.237090Z","iopub.execute_input":"2025-01-19T14:16:15.237284Z","iopub.status.idle":"2025-01-19T14:16:15.254927Z","shell.execute_reply.started":"2025-01-19T14:16:15.237267Z","shell.execute_reply":"2025-01-19T14:16:15.254311Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"pl.seed_everything(56)\nCFG.fold_number = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.255614Z","iopub.execute_input":"2025-01-19T14:16:15.255890Z","iopub.status.idle":"2025-01-19T14:16:15.273516Z","shell.execute_reply.started":"2025-01-19T14:16:15.255862Z","shell.execute_reply":"2025-01-19T14:16:15.272901Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"dm = PLDataModule()\ndm.prepare_data()\ndm.setup(0)\n#dm.tokenizer.add_special_tokens({'pad_token': '[PAD]'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:15.274300Z","iopub.execute_input":"2025-01-19T14:16:15.274561Z","iopub.status.idle":"2025-01-19T14:16:19.090967Z","shell.execute_reply.started":"2025-01-19T14:16:15.274535Z","shell.execute_reply":"2025-01-19T14:16:19.089916Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"906ecd89620945418566590de565a0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b5fbd1bfe3460283e0c3a9bd31dffb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.33M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf78401afc424c719c9822761c50cb41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/963 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719dd29ab8384d19ad228751263a3984"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"CFG.model.num_training_steps = len(dm.train_dataloader()) * CFG.model.max_epoches\nmodel = PLModule()\n    \nwandb.login(key=\"31520b01739d418e5d77a11fd8a79a70b189b8bc\")\nos.environ['WANDB_API_KEY'] = \"31520b01739d418e5d77a11fd8a79a70b189b8bc\"\nwandb.init(project='T1',name='labse_en_ru',config=set_wandb_cfg())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:16:19.091870Z","iopub.execute_input":"2025-01-19T14:16:19.092129Z","iopub.status.idle":"2025-01-19T14:17:07.761566Z","shell.execute_reply.started":"2025-01-19T14:16:19.092109Z","shell.execute_reply":"2025-01-19T14:17:07.760829Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/697 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c90a18391e44ca49cb9d6a8f81a53b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9cdbfbc22a3468e8b68591689f77405"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrewkhl\u001b[0m (\u001b[33mandlh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250119_141700-ckxkdszq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/andlh/T1/runs/ckxkdszq' target=\"_blank\">labse_en_ru</a></strong> to <a href='https://wandb.ai/andlh/T1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/andlh/T1' target=\"_blank\">https://wandb.ai/andlh/T1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/andlh/T1/runs/ckxkdszq' target=\"_blank\">https://wandb.ai/andlh/T1/runs/ckxkdszq</a>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/andlh/T1/runs/ckxkdszq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7e3470c3c400>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\ncheckpoint_cb = pl.callbacks.ModelCheckpoint(\n    dirpath='./outputs/',\n    filename='model_{epoch:02d}-{accuracy:.4f}',\n    monitor='accuracy',\n    mode='max',\n    save_last=True\n)\n\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    precision=32,\n    callbacks = [lr_monitor],#[lr_monitor,checkpoint_cb],\n    logger = pl.loggers.WandbLogger(save_code=True),\n    log_every_n_steps=1,\n    accumulate_grad_batches=CFG.model.grad_acum_steps,\n    enable_checkpointing=False,\n    min_epochs=1,\n    devices=1,\n    check_val_every_n_epoch=1,\n    max_epochs=CFG.model.max_epoches\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:17:07.762468Z","iopub.execute_input":"2025-01-19T14:17:07.762767Z","iopub.status.idle":"2025-01-19T14:17:07.834403Z","shell.execute_reply.started":"2025-01-19T14:17:07.762736Z","shell.execute_reply":"2025-01-19T14:17:07.833536Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trainer.fit(model, datamodule=dm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:17:07.835283Z","iopub.execute_input":"2025-01-19T14:17:07.835555Z","iopub.status.idle":"2025-01-19T14:44:50.548842Z","shell.execute_reply.started":"2025-01-19T14:17:07.835527Z","shell.execute_reply":"2025-01-19T14:44:50.547279Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f02d37e278ae4b618979fc86ca61c913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;31m# in automatic optimization, there can only be one optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# model hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         call._call_lightning_module_hook(\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \"\"\"\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mclosure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36m_wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mclosure_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/automatic.py\u001b[0m in \u001b[0;36mbackward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"backward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/plugins/precision/precision.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-f854c515c2ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"],"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error"}],"execution_count":25},{"cell_type":"code","source":"preds = trainer.predict(model, dm.predict_dataloader())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:45:47.594819Z","iopub.execute_input":"2025-01-19T14:45:47.595199Z","iopub.status.idle":"2025-01-19T14:48:19.750104Z","shell.execute_reply.started":"2025-01-19T14:45:47.595169Z","shell.execute_reply":"2025-01-19T14:48:19.748890Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a5f8ae3660e4af1886683adb7a40ee9"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['index'] = pd.read_csv(CFG.data.test_path)['index']\ndf['sentiment'] = torch.cat(preds).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:48:19.751320Z","iopub.execute_input":"2025-01-19T14:48:19.751608Z","iopub.status.idle":"2025-01-19T14:48:20.010288Z","shell.execute_reply.started":"2025-01-19T14:48:19.751584Z","shell.execute_reply":"2025-01-19T14:48:20.009410Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"df.to_csv('t1_preds_text4.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:48:20.011605Z","iopub.execute_input":"2025-01-19T14:48:20.011898Z","iopub.status.idle":"2025-01-19T14:48:20.031196Z","shell.execute_reply.started":"2025-01-19T14:48:20.011871Z","shell.execute_reply":"2025-01-19T14:48:20.030459Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"hist,val_preds,test_preds = train_fold(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}